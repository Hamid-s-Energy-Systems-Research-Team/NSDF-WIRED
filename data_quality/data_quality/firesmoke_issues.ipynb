{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72ac55c2-6665-45b5-ad8f-da750b4d1d14",
   "metadata": {},
   "source": [
    "# Identify missing hours between March 3, 2021 and February 10, 2024.\n",
    "#### We want to fill all gaps if possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42900781-b44f-42e5-ac70-4c8255709117",
   "metadata": {},
   "source": [
    "#### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a18e54-a3f5-46c7-a6ad-35daff3d89df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading netcdf files, for metadata\n",
    "import xarray as xr\n",
    "from backend_v3 import *\n",
    "\n",
    "# Used for processing netCDF time data\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Used for numerical work\n",
    "import numpy as np\n",
    "\n",
    "# Used for loading data from pickle data\n",
    "import pickle\n",
    "\n",
    "# For working with pandas timestamps\n",
    "import pandas as pd\n",
    "\n",
    "# For downloading from the internet, retrying netCDF files\n",
    "import wget\n",
    "\n",
    "# Accessory for generating progress bar to see progress of loops\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1196ff-4767-4c54-90f7-1f3831a4b528",
   "metadata": {},
   "source": [
    "#### Set relevant directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36932f8e-4993-4c29-898e-8994fe566b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******* THIS IS WHEN RUNNING FROM ATLANTIS.SCI **************\n",
    "firesmoke_dir = \"/usr/sci/cedmav/data/firesmoke\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6708d12-9882-41c7-af65-91aca25b1349",
   "metadata": {},
   "source": [
    "### 1. Determine the hours that failed to visualize in video, even though those hours are available in netCDF files/idx..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb5c1ed-3e34-4017-b219-553c1efc68a1",
   "metadata": {},
   "source": [
    "#### Import the issues found while generating visualizations of each timestep from IDX conversion and original netCDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440f50fc-aa9a-4307-bc64-a41cc9a659a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"new_idx_issues.pkl\", \"rb\") as input_file:\n",
    "    idx_issues = pickle.load(input_file)\n",
    "\n",
    "with open(\"new_netcdf_issues.pkl\", \"rb\") as input_file:\n",
    "    netcdf_issues = pickle.load(input_file)\n",
    "\n",
    "# print how many issues there are\n",
    "print(f'Number of IDX issues: {len(idx_issues)}')\n",
    "print(f'Number of netCDF issues: {len(netcdf_issues)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d4c3d2-585e-40a3-a2f3-210d5daf6e60",
   "metadata": {},
   "source": [
    "#### Let's see where the IDX conversion and netCDF files agree on issues. \n",
    "If it's an issue in netCDF original file, it'll be an issue in IDX too. However converse is not necessarily true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b165557-1f3a-4e7b-96e1-c7d5bfebf3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make set of common datetime keys betwix idx and netcdf issues\n",
    "common_dates = set(netcdf_issues.keys()).intersection(idx_issues.keys())\n",
    "\n",
    "# get issues unique to each version of firesmoke data\n",
    "idx_only_issues = set(idx_issues.keys()).difference(netcdf_issues.keys())\n",
    "netcdf_only_issues = set(netcdf_issues.keys()).difference(idx_issues.keys())\n",
    "\n",
    "# see how many such issues exist in these sets\n",
    "print(f'Num. of issues IDX and netCDF agree on is {len(common_dates)}')\n",
    "print(f'Num. of issues unique to IDX is {len(idx_only_issues)}')\n",
    "print(f'Num. of issues unique to netCDF is {len(netcdf_only_issues)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b107563b-1ab1-466c-83c4-7a9deae1a111",
   "metadata": {},
   "source": [
    "#### Now let's see what's wrong with the array at the issues found above...\n",
    "I suspect these arrays are all zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb0300a-a2be-4cfb-bd13-95220f72533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "netcdf_zeros = set()\n",
    "idx_zeros = set()\n",
    "\n",
    "for i in netcdf_issues:\n",
    "    if np.all(netcdf_issues[i] == 0):\n",
    "        netcdf_zeros.add(i)\n",
    "        \n",
    "for i in idx_issues:\n",
    "    if np.all(idx_issues[i] == 0):\n",
    "        idx_zeros.add(i)\n",
    "        \n",
    "print(f'Number of idx issues where array is all zeros is: {len(idx_zeros)}')\n",
    "print(f'Number of netcdf issues where array is all zeros is: {len(netcdf_zeros)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac4525e-c2d1-4868-9960-58adf2000585",
   "metadata": {},
   "source": [
    "#### Let's see what dates failed i.e. are just all zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89350be7-6cc0-4d8c-a390-5ee06a14d12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_hours = set()\n",
    "\n",
    "for k in idx_issues.keys():\n",
    "    zero_hours.add(pd.Timestamp(year=k.year, month=k.month, day=k.day, hour=k.hour))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81676299-b24e-4039-b3df-58b1a83b96e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'there are {len(zero_hours)} hours whose data are all zeros')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5a88d0-aeb8-42e0-9828-ab7be06bccc7",
   "metadata": {},
   "source": [
    "## 2. See what timesteps are unavailable due to netCDF files being missing\n",
    "#### This is the available conversion from our IDX conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae0a999-f1b9-4b7b-bab7-39980179d8e2",
   "metadata": {},
   "source": [
    "#### Import data available from URL in our metadata file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dcdec5-f8db-4da4-a323-e618d9317d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to tiny netcdf\n",
    "tiny_netcdf = \"firesmoke_metadata.nc\"\n",
    "\n",
    "# open tiny netcdf with xarray and OpenVisus backend\n",
    "ds = xr.open_dataset(tiny_netcdf, engine=OpenVisusBackendEntrypoint)\n",
    "ds['TFLAG'].values[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce69755d-272a-4b5e-b06f-71bc0ca91e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tflag(tflag):\n",
    "    \"\"\"\n",
    "    Return the tflag as a datetime object\n",
    "    :param list tflag: a list of two int32, the 1st representing date and 2nd representing time\n",
    "    \"\"\"\n",
    "    # obtain year and day of year from tflag[0] (date)\n",
    "    date = int(tflag[0])\n",
    "    year = date // 1000 # first 4 digits of tflag[0]\n",
    "    day_of_year = date % 1000 # last 3 digits of tflag[0]\n",
    "\n",
    "    # create datetime object representing date\n",
    "    final_date = datetime.datetime(year, 1, 1) + datetime.timedelta(days=day_of_year - 1)\n",
    "\n",
    "    # obtain hour, mins, and secs from tflag[1] (time)\n",
    "    time = int(tflag[1])\n",
    "    hours = time // 10000 # first 2 digits of tflag[1]\n",
    "    minutes = (time % 10000) // 100 # 3rd and 4th digits of tflag[1] \n",
    "    seconds = time % 100  # last 2 digits of tflag[1]\n",
    "\n",
    "    # create final datetime object\n",
    "    full_datetime = datetime.datetime(year, final_date.month, final_date.day, hours, minutes, seconds)\n",
    "    return full_datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0ec044-a7a7-4ac5-9a55-3d1a75800cde",
   "metadata": {},
   "source": [
    "#### Create the set of the available hours in this dataset\n",
    "Including the hours with all zeros..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5753ea50-1e3c-469e-839d-9cc2c0ea6027",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_tflag_set = set()\n",
    "\n",
    "# get TFLAGs as datetime objects, friendlier, make a set, wanna see what's missing\n",
    "for t in ds['TFLAG'].values:\n",
    "    actual_tflag_set.add(parse_tflag(t[0]))\n",
    "\n",
    "print(f'there are {len(actual_tflag_set)} hours available in firesmoke dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a977df15-6e93-448e-8ebf-b5f37b165291",
   "metadata": {},
   "source": [
    "#### Create the set of all hours we would like, between dates 3/3/2021 - 2/10/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d304fb9b-384c-416e-ba3e-efdcb6b96dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the start and end dates\n",
    "start_date = pd.Timestamp(datetime.datetime.strptime(\"20210303\", \"%Y%m%d\"))\n",
    "end_date = pd.Timestamp(datetime.datetime.strptime(\"20240210\", \"%Y%m%d\"))\n",
    "\n",
    "# Get all hours between the start and end dates\n",
    "desired_tflag_set = {start_date + pd.Timedelta(hours=x) for x in range(int((end_date - start_date).total_seconds() // 3600) + 1)}\n",
    "\n",
    "print(f'There are {len(desired_tflag_set)} hours between 3/3/21 and 2/10/24')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71f1c56-6b31-4015-b3e1-a241a98ed448",
   "metadata": {},
   "source": [
    "#### Determine what hours are apparently missing netCDF files using the sets above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da88445-465b-4643-a340-0f0a52e6422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_tflags = desired_tflag_set.difference(actual_tflag_set)\n",
    "\n",
    "print(f'there are {len(missing_tflags)} missing hours from our dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88161e0e-e965-4529-aa34-9c9962c1346a",
   "metadata": {},
   "source": [
    "## We must account for these missing hours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd043ad-ce28-4d6f-8653-64289b15e9b0",
   "metadata": {},
   "source": [
    "### Import all calls made during conversion. Determine holes in those calls and, again, confirm the holes really are because of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a60ae20-15a1-4ba7-b2e8-4657942a97a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"idx_calls.pkl\", \"rb\") as input_file:\n",
    "    idx_calls = pickle.load(input_file)\n",
    "\n",
    "print(f'total hours called during idx conversion is {len(idx_calls)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c89436-382d-46c7-ab58-9a40469c7c42",
   "metadata": {},
   "source": [
    "#### Ensure that all the missing_tflags were not called during IDX conversion in the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44d4ee2-0537-4302-830d-809ed2e3a2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_calls_set = set()\n",
    "\n",
    "for call in idx_calls:\n",
    "    idx_calls_set.add(pd.Timestamp(call[2]))\n",
    "\n",
    "desired_not_called = desired_tflag_set.difference(idx_calls_set)\n",
    "\n",
    "print(f'len(idx_calls_set) = {len(idx_calls_set)}')\n",
    "print(f'len(desired_not_called.difference(missing_tflags)) = {len(desired_not_called.difference(missing_tflags))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131dc008-1b2c-430e-b531-1e7bff2f37bd",
   "metadata": {},
   "source": [
    "#### Good, now let's see if the missing_tflags aren't available from firesmoke.ca...\n",
    "For each missing date, let's just grab that day and the 4 that **precede** it, at each dataset. Then we'll check if any of those downloaded files have the missing hours available in them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f04f2b-0bbd-492a-a2a1-7675aac098d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to hold the downloaded files\n",
    "garbage = '/usr/sci/scratch_nvme/arleth/total_garbage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61abcee-197e-4183-b007-48cf25e69d46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from all the failed hours, get the DDMMYYYY dates, this is what we use to query firesmoke.ca\n",
    "missing_dates = set()\n",
    "\n",
    "for t in missing_tflags:\n",
    "    missing_dates.add(t.normalize())\n",
    "\n",
    "missing_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6fbfee-23cd-4d5b-8eae-2ae2da14f9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sequence of queries to download, using missing_tflags and dataset metadata\n",
    "ids = [\"BSC18CA12-01\", \"BSC00CA12-01\", \"BSC06CA12-01\", \"BSC12CA12-01\"]\n",
    "init_times = [\"02\", \"08\", \"14\", \"20\"]\n",
    "to_download = set()\n",
    "\n",
    "# for all missing dates\n",
    "for date in missing_dates:\n",
    "    # for 4 days\n",
    "    for i in range(4):\n",
    "        curr_date = date + pd.Timedelta(days=-i)\n",
    "        \n",
    "        # build date string for query\n",
    "        date_str = curr_date.strftime('%Y%m%d')\n",
    "        \n",
    "        # for each dataset\n",
    "        for id_, init_time in zip(ids, init_times):\n",
    "            # build URL string to download from and directory & filename to download to\n",
    "            url = f'https://firesmoke.ca/forecasts/{id_}/{date_str}{init_time}/dispersion.nc'\n",
    "            directory = f'{garbage}/{id_}/dispersion_{date_str}.nc'\n",
    "            to_download.add((url, directory))\n",
    "\n",
    "print(f'there are {len(to_download)} files to try downloading...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4669be13-d3e1-424f-ab25-718910f60500",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download files, do this as a python script by doing `nbconvert --to script...`, \n",
    "# it's faster somehow\n",
    "for q in tqdm(to_download):\n",
    "    print(f'downloading {q[0]} to {q[1]}')\n",
    "    wget.download(q[0], out=q[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdba0ea2-6114-448f-a6ef-d7f67870b9fc",
   "metadata": {},
   "source": [
    "#### See all hours available from files we downloaded. Check intersection with our missing tflags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648113af-ce80-4037-bbd7-ecb6a5b8e114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Track files that successfully opened\n",
    "# successful_files = {id_: [] for id_ in ids}\n",
    "\n",
    "# # Try opening all downloaded files\n",
    "# for q in to_download:\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
