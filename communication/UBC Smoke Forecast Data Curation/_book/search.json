[
  {
    "objectID": "sys_specs.html",
    "href": "sys_specs.html",
    "title": "3  System and Environment",
    "section": "",
    "text": "3.1 Machine Specification\nThe data we curate is over 300 gigabytes large. Therefore we used the SCI institute’s in-house machine ‘atlantis’ for data staging and processing. See Table 3.1.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>System and Environment</span>"
    ]
  },
  {
    "objectID": "sys_specs.html#machine-specification",
    "href": "sys_specs.html#machine-specification",
    "title": "3  System and Environment",
    "section": "",
    "text": "Table 3.1: ‘atlantis’ System Specifications\n\n\n\n\n\nProperty\nValue\n\n\n\n\nArchitecture\nx86_64\n\n\nCPU op-mode(s)\n32-bit, 64-bit\n\n\nByte Order\nLittle Endian\n\n\nAddress sizes\n44 bits physical, 48 bits virtual\n\n\nCPU(s)\n48\n\n\nOn-line CPU(s) list\n0-47\n\n\nThread(s) per core\n2\n\n\nCore(s) per socket\n6\n\n\nSocket(s)\n4",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>System and Environment</span>"
    ]
  },
  {
    "objectID": "sys_specs.html#environment",
    "href": "sys_specs.html#environment",
    "title": "3  System and Environment",
    "section": "3.2 Environment",
    "text": "3.2 Environment\nWe aim to work in an environment that can be most easily reproduced and documented. Therefore we used Python 3.9.19 via conda. We did all our work within the Project Jupyter environment.\nTo find our the yml file containing our exported conda environment please see the sidebar.\nTo work on the ‘atlantis’ machine, we used SSH to connect to the machine.\nIn the proceeding chapters we will specify which tools and libraries were used and why.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>System and Environment</span>"
    ]
  },
  {
    "objectID": "demo.html#this-notebook-provide-the-instructions-on-how-to-read-ubc-firesmoke-data-from-firsmoke_metadata_current.nc-using-xarray-and-the-openvisus-xarray-backend.",
    "href": "demo.html#this-notebook-provide-the-instructions-on-how-to-read-ubc-firesmoke-data-from-firsmoke_metadata_current.nc-using-xarray-and-the-openvisus-xarray-backend.",
    "title": "UBC Smoke Forecast Data Curation",
    "section": "0.1 This notebook provide the instructions on how to read UBC firesmoke data from firsmoke_metadata_current.nc using xarray and the OpenVisus xarray backend.",
    "text": "0.1 This notebook provide the instructions on how to read UBC firesmoke data from firsmoke_metadata_current.nc using xarray and the OpenVisus xarray backend.\nDashboard visible here: http://chpc3.nationalsciencedatafabric.org:9988/dashboards",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>demo.html</span>"
    ]
  },
  {
    "objectID": "demo.html#step-1-importing-the-libraries",
    "href": "demo.html#step-1-importing-the-libraries",
    "title": "UBC Smoke Forecast Data Curation",
    "section": "0.2 Step 1: Importing the libraries",
    "text": "0.2 Step 1: Importing the libraries\n\n0.2.1 Please be sure to have libraries installed\n\n\nCode\n# for numerical work\nimport numpy as np\n\n# for accessing file system\nimport os\n\n# for loading netcdf files, for metadata\nimport xarray as xr\n# for connecting OpenVisus framework to xarray\n# from https://github.com/sci-visus/openvisuspy, \nfrom openvisuspy.xarray_backend import OpenVisusBackendEntrypoint\n# from backend_v3 import *\n\n# Used for processing netCDF time data\nimport time\nimport datetime\nimport requests\n# Used for indexing via metadata\nimport pandas as pd\n\n# for plotting\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\n\n\n#Stores the OpenVisus cache in the local direcrtory \nimport os\nos.environ[\"VISUS_CACHE\"]=\"./visus_cache_can_be_erased\"\nos.environ['CURL_CA_BUNDLE'] = ''",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>demo.html</span>"
    ]
  },
  {
    "objectID": "demo.html#step-2-reading-the-data-metadata-from-file",
    "href": "demo.html#step-2-reading-the-data-metadata-from-file",
    "title": "UBC Smoke Forecast Data Curation",
    "section": "0.3 Step 2: Reading the data & metadata from file",
    "text": "0.3 Step 2: Reading the data & metadata from file\n\n0.3.1 In this section, we load our data using xr.open_dataset.\n\n\nCode\n# path to tiny NetCDF\nurl = 'https://github.com/sci-visus/NSDF-WIRED/raw/main/data/firesmoke_metadata_current.nc'\n\n# Download the file using requests\nresponse = requests.get(url)\nlocal_netcdf = 'firesmoke_metadata.nc'\nwith open(local_netcdf, 'wb') as f:\n    f.write(response.content)\n    \n# open tiny netcdf with xarray and OpenVisus backend\nds = xr.open_dataset(local_netcdf, engine=OpenVisusBackendEntrypoint)\n\n\nov.LoadDataset(http://atlantis.sci.utah.edu/mod_visus?dataset=UBC_fire_smoke_BSC&cached=1)\nPM25\nAdding field  PM25 shape  [27357, 381, 1081, 21] dtype  float32 labels  ['time', 'ROW', 'COL', 'resolution'] Max Resolution  20\n\n\n\n\nCode\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:            (time: 27357, ROW: 381, COL: 1081, resolution: 21,\n                        VAR: 1, DATE-TIME: 2)\nDimensions without coordinates: time, ROW, COL, resolution, VAR, DATE-TIME\nData variables:\n    PM25               (time, ROW, COL, resolution) float32 ...\n    TFLAG              (time, VAR, DATE-TIME) int32 ...\n    wrf_arw_init_time  (time, VAR, DATE-TIME) int32 ...\n    resampled          (time) bool ...\n    CDATE              (time) int32 ...\n    CTIME              (time) int32 ...\n    WDATE              (time) int32 ...\n    WTIME              (time) int32 ...\n    SDATE              (time) int32 ...\n    STIME              (time) int32 ...\nAttributes: (12/28)\n    IOAPI_VERSION:  $Id: @(#) ioapi library version 3.0 $                    ...\n    EXEC_ID:        ????????????????                                         ...\n    FTYPE:          1\n    TSTEP:          10000\n    NTHIK:          1\n    NCOLS:          1081\n    ...             ...\n    GDNAM:          HYSPLIT CONC    \n    UPNAM:          hysplit2netCDF  \n    VAR-LIST:       PM25            \n    FILEDESC:       Hysplit Concentration Model Output                       ...\n    HISTORY:        \n    idx_url:        http://atlantis.sci.utah.edu/mod_visus?dataset=UBC_fire_s...xarray.DatasetDimensions:time: 27357ROW: 381COL: 1081resolution: 21VAR: 1DATE-TIME: 2Coordinates: (0)Data variables: (10)PM25(time, ROW, COL, resolution)float32...long_name :PM25            units :ug/m^3          var_desc :PM25                                                                            [236612908917 values with dtype=float32]TFLAG(time, VAR, DATE-TIME)int32...[54714 values with dtype=int32]wrf_arw_init_time(time, VAR, DATE-TIME)int32...[54714 values with dtype=int32]resampled(time)bool...[27357 values with dtype=bool]CDATE(time)int32...[27357 values with dtype=int32]CTIME(time)int32...[27357 values with dtype=int32]WDATE(time)int32...[27357 values with dtype=int32]WTIME(time)int32...[27357 values with dtype=int32]SDATE(time)int32...[27357 values with dtype=int32]STIME(time)int32...[27357 values with dtype=int32]Indexes: (0)Attributes: (28)IOAPI_VERSION :$Id: @(#) ioapi library version 3.0 $                                           EXEC_ID :????????????????                                                                FTYPE :1TSTEP :10000NTHIK :1NCOLS :1081NROWS :381NLAYS :1NVARS :1GDTYP :1P_ALP :0.0P_BET :0.0P_GAM :0.0XCENT :-106.0YCENT :51.0XORIG :-160.0YORIG :32.0XCELL :0.10000000149011612YCELL :0.10000000149011612VGTYP :5VGTOP :-9999.0VGLVLS :[10.  0.]GDNAM :HYSPLIT CONC    UPNAM :hysplit2netCDF  VAR-LIST :PM25            FILEDESC :Hysplit Concentration Model Output                                              lat-lon coordinate system                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       HISTORY :idx_url :http://atlantis.sci.utah.edu/mod_visus?dataset=UBC_fire_smoke_BSC&cached=1",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>demo.html</span>"
    ]
  },
  {
    "objectID": "demo.html#step-2.5-calculate-derived-metadata-using-original-metadata-above-to-create-coordinates",
    "href": "demo.html#step-2.5-calculate-derived-metadata-using-original-metadata-above-to-create-coordinates",
    "title": "UBC Smoke Forecast Data Curation",
    "section": "0.4 Step 2.5, Calculate derived metadata using original metadata above to create coordinates",
    "text": "0.4 Step 2.5, Calculate derived metadata using original metadata above to create coordinates\n\n0.4.1 This is required to allow for indexing of data via metadata\n\n0.4.1.1 Calculate latitude and longitude grid\n\n\nCode\n# Get metadata to compute lon and lat\nxorig = ds.XORIG\nyorig = ds.YORIG\nxcell = ds.XCELL\nycell = ds.YCELL\nncols = ds.NCOLS\nnrows = ds.NROWS\n\nlongitude = np.linspace(xorig, xorig + xcell * (ncols - 1), ncols)\nlatitude = np.linspace(yorig, yorig + ycell * (nrows - 1), nrows)\n\nprint(\"Size of longitude & latitude arrays:\")\nprint(f'np.size(longitude) = {np.size(longitude)}')\nprint(f'np.size(latitude) = {np.size(latitude)}\\n')\nprint(\"Min & Max of longitude and latitude arrays:\")\nprint(f'longitude: min = {np.min(longitude)}, max = {np.max(longitude)}')\nprint(f'latitude: min = {np.min(latitude)}, max = {np.max(latitude)}')\n\n\nSize of longitude & latitude arrays:\nnp.size(longitude) = 1081\nnp.size(latitude) = 381\n\nMin & Max of longitude and latitude arrays:\nlongitude: min = -160.0, max = -51.99999839067459\nlatitude: min = 32.0, max = 70.00000056624413\n\n\n\n\n0.4.1.2 Using calculated latitude and longitude, create coordinates allowing for indexing data using lat/lon\n\n\nCode\n# Create coordinates for lat and lon (credit: Aashish Panta)\nds.coords['lat'] = ('ROW', latitude)\nds.coords['lon'] = ('COL', longitude)\n\n# Replace col and row dimensions with newly calculated lon and lat arrays (credit: Aashish Panta)\nds = ds.swap_dims({'COL': 'lon', 'ROW': 'lat'})\n\n\n\n\n0.4.1.3 Create coordinates allowing for indexing data using timestamp\n\n0.4.1.3.1 First, convert tflags to timestamps that are compatible with xarray\n\n\nCode\ndef parse_tflag(tflag):\n    \"\"\"\n    Return the tflag as a datetime object\n    :param list tflag: a list of two int32, the 1st representing date and 2nd representing time\n    \"\"\"\n    # obtain year and day of year from tflag[0] (date)\n    date = int(tflag[0])\n    year = date // 1000 # first 4 digits of tflag[0]\n    day_of_year = date % 1000 # last 3 digits of tflag[0]\n\n    # create datetime object representing date\n    final_date = datetime.datetime(year, 1, 1) + datetime.timedelta(days=day_of_year - 1)\n\n    # obtain hour, mins, and secs from tflag[1] (time)\n    time = int(tflag[1])\n    hours = time // 10000 # first 2 digits of tflag[1]\n    minutes = (time % 10000) // 100 # 3rd and 4th digits of tflag[1] \n    seconds = time % 100  # last 2 digits of tflag[1]\n\n    # create final datetime object\n    full_datetime = datetime.datetime(year, final_date.month, final_date.day, hours, minutes, seconds)\n    return full_datetime\n\n\n\n\n0.4.1.3.2 Return an array of the tflags as pandas timestamps\n\n\nCode\n# get all tflags\ntflag_values = ds['TFLAG'].values\n\n# to store pandas timestamps\ntimestamps = []\n\n# convert all tflags to pandas timestamps, store in timestamps list\nfor tflag in tflag_values:\n    timestamps.append(pd.Timestamp(parse_tflag(tflag[0])))\n\n# check out the first 3 timestamps\ntimestamps[0:3]\n\n\n[Timestamp('2021-03-04 00:00:00'),\n Timestamp('2021-03-04 01:00:00'),\n Timestamp('2021-03-04 02:00:00')]\n\n\n\n\nCode\n# set coordinates to each timestep with these pandas timestamps\nds.coords['time'] = ('time', timestamps)\n\n\n\n\n\n0.4.1.4 The timestamps may not be intuitive. The following utility function returns the desired pandas timestamp based on your date and time of interest.\n\n0.4.1.4.1 When you index the data at a desired time, use this function to get the timestamp you need to index.\n\n\nCode\ndef get_timestamp(year, month, day, hour):\n    \"\"\"\n    return a pandas timestamp using the given date-time arguments\n    :param int year: year\n    :param int month: month\n    :param int day: day\n    :param int hour: hour\n    \"\"\"\n    # Convert year, month, day, and hour to a datetime object\n    full_datetime = datetime.datetime(year, month, day, hour)\n    \n    # Extract components from the datetime object\n    year = full_datetime.year\n    day_of_year = full_datetime.timetuple().tm_yday\n    hours = full_datetime.hour\n    minutes = full_datetime.minute\n    seconds = full_datetime.second\n\n    # Compute tflag[0] and tflag[1]\n    tflag0 = year * 1000 + day_of_year\n    tflag1 = hours * 10000 + minutes * 100 + seconds\n\n    # Return the Pandas Timestamp object\n    return pd.Timestamp(full_datetime)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>demo.html</span>"
    ]
  },
  {
    "objectID": "demo.html#step-3-select-a-data_slice",
    "href": "demo.html#step-3-select-a-data_slice",
    "title": "UBC Smoke Forecast Data Curation",
    "section": "0.5 Step 3: Select a data_slice",
    "text": "0.5 Step 3: Select a data_slice\n\n0.5.1 This section shows you how to load the data you want.\n\n0.5.1.1 You can index the data using indices, timestamps*, latitude & longitude, and by desired resolution**.\n*Not setting any time means the first timestep available is selected. **Not setting quality means full data resolution is selected.\n\n\n0.5.1.1.1 In this case, let’s get all available firesmoke data for March 5, 2021 00:00:00.\n\n\nCode\n# select timestamp\nmy_timestamp = get_timestamp(2021, 3, 5, 0)\n\n# select resolution, let's use full resolution since data isn't too big at one time slice\n# data resolution can be -19 for lowest res and 0 for highest res\ndata_resolution = 0\n\n# get PM25 values and provide 4 values, the colons mean select all lat and lon indices\ndata_array_at_time = ds['PM25'].loc[my_timestamp, :, :, data_resolution]\n\n# notice, to access the data, you must append \".values\" to the data array we got above\nprint(f'timestamp: {my_timestamp}')\nprint(f'shape of data_array_at_time.values = {np.shape(data_array_at_time.values)}')\n\n\ntimestamp: 2021-03-05 00:00:00\nUsing Max Resolution:  20\nTime: 24, max_resolution: 20, logic_box=(0, 1081, 0, 381), field: PM25\nshape of data_array_at_time.values = (381, 1081)\n\n\n\n\n0.5.1.1.2 Perhaps we want to slice a specific latitude longitude range from our data_array_at_time, for example, latitude range [35, 50] and longitude range [-140, -80]. Let’s do that below.\n\n\nCode\n# # define range for latitude and longitude to use\nmin_lat = 35\nmax_lat = 50\nmin_lon = -140\nmax_lon = -80\n\n# min_lon = np.min(longitude)\n# max_lon = np.max(longitude)\n# min_lat = np.min(latitude)\n# max_lat = np.max(latitude)\n\n# get PM25 values and provide 4 values, but this time at our desired ranges\ndata_array_at_latlon = ds['PM25'].loc[my_timestamp, min_lat:max_lat, min_lon:max_lon, data_resolution]\n\n# notice, to access the data, you must append \".values\" to the data array we got above\nprint(f'timestamp: {my_timestamp}')\nprint(f'shape of data_array_at_time.values = {np.shape(data_array_at_latlon.values)}')\n\n\ntimestamp: 2021-03-05 00:00:00\nUsing Max Resolution:  20\nTime: 24, max_resolution: 20, logic_box=(200, 800, 30, 180), field: PM25\nshape of data_array_at_time.values = (150, 600)\n\n\n\n\n\n0.5.1.2 The following are the max and min timestamps, lon/lat values, and data resolutions you can index by\n\n0.5.1.2.1 Be sure you index within the data range, otherwise you may get errors since no data exists outside these ranges!\n\n\nCode\n# NOTE: there is one dummy date, ignore ds['time'].values[-1]\nprint(f\"earliest valid timestamp is: {ds['time'].values[0]}\")\nprint(f\"latest valid timestamp is: {ds['time'].values[-2]}\\n\")\n\nprint(f\"valid longitude range is: {ds['lon'].values[0]}, {ds['lon'].values[-1]}\")\nprint(f\"valid latitude range is: {ds['lat'].values[0]}, {ds['lat'].values[-1]}\\n\")\n\nprint(f\"valid data resolutions range is: [-19, 0]\")\n\n\nearliest valid timestamp is: 2021-03-04T00:00:00.000000000\nlatest valid timestamp is: 2024-06-27T22:00:00.000000000\n\nvalid longitude range is: -160.0, -51.99999839067459\nvalid latitude range is: 32.0, 70.00000056624413\n\nvalid data resolutions range is: [-19, 0]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>demo.html</span>"
    ]
  },
  {
    "objectID": "demo.html#step-4-visualize-data_slice",
    "href": "demo.html#step-4-visualize-data_slice",
    "title": "UBC Smoke Forecast Data Curation",
    "section": "0.6 Step 4: Visualize data_slice",
    "text": "0.6 Step 4: Visualize data_slice\n\n0.6.1 One can visualize the data either by:\n\n\n0.6.2 1. Get the values from your data_array_at_time and plot using your favorite python visualization library. We’ll use matplotlib.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>demo.html</span>"
    ]
  },
  {
    "objectID": "demo.html#use-xarrays-built-in-plotting-function-not-recommended-as-it-is-not-robust",
    "href": "demo.html#use-xarrays-built-in-plotting-function-not-recommended-as-it-is-not-robust",
    "title": "UBC Smoke Forecast Data Curation",
    "section": "0.7 #### 2. Use xarray’s built in plotting function (not recommended, as it is not robust)",
    "text": "0.7 #### 2. Use xarray’s built in plotting function (not recommended, as it is not robust)\nHere we plot data_array_at_time with matplotlib and its basemap extenstion to add geographic context.\n\n\nCode\n# Let's use matplotlib's imshow, since our data is on a grid\n# ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n\n# Initialize a figure and plot, so we can customize figure and plot of data\n# ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html\n# ref: https://scitools.org.uk/cartopy/docs/latest/getting_started/index.html\nmy_fig, my_plt = plt.subplots(figsize=(15, 6), subplot_kw=dict(projection=ccrs.PlateCarree()))\n\n# Let's set some parameters to get the visualization we want\n# ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n\n# color PM25 values on a log scale, since values are small\nmy_norm = \"log\" \n# this will number our x and y axes based on the longitude latitude range\nmy_extent = [np.min(longitude), np.max(longitude), np.min(latitude), np.max(latitude)]\n# ensure the aspect ratio of our plot fits all data, matplotlib can does this automatically\nmy_aspect = 'auto'\n# tell matplotlib, our origin is the lower-left corner\nmy_origin = 'lower'\n# select a colormap for our plot and the color bar on the right\nmy_cmap = 'Oranges'\n\n# create our plot using imshow\nplot = my_plt.imshow(data_array_at_time.values, norm=my_norm, extent=my_extent, \n          aspect=my_aspect, origin=my_origin, cmap=my_cmap)\n\n# draw coastlines\nmy_plt.coastlines()\n\n# draw latitude longitude lines\n# ref: https://scitools.org.uk/cartopy/docs/latest/gallery/gridlines_and_labels/gridliner.html\nmy_plt.gridlines(draw_labels=True)\n\n# add a colorbar to our figure, based on the plot we just made above\nmy_fig.colorbar(plot,location='right', label='ug/m^3')\n\n# # Set x and y axis labels on our ax\n# my_plt.set_xlabel('Longitude')\n# my_plt.set_ylabel('Latitude')\n\n# Set title of our figure\nmy_fig.suptitle('Ground level concentration of PM2.5 microns and smaller')\n\n# Set title of our plot as the timestamp of our data\nmy_plt.set_title(f'{my_timestamp}')\n\n# Show the resulting visualization\nplt.show()\n\n\n\n\n\n\n\n\n\nHere we plot with xarray’s built-in matplotlib powered plotter.\n\n\nCode\ndata_array_at_time.plot(vmin=0, vmax=30)\n\n\n\n\n\n\n\n\n\n\nHere we plot data_array_at_latlon. We use the exact same code, but define my_extent accordingly.\n\n\nCode\n# Let's use matplotlib's imshow, since our data is on a grid\n# ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n\n# Initialize a figure and plot, so we can customize figure and plot of data\n# ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html\nmy_fig, my_plt = plt.subplots(figsize=(15, 6), subplot_kw=dict(projection=ccrs.PlateCarree()))\n\n# Let's set some parameters to get the visualization we want\n# ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n\n# color PM25 values on a log scale, since values are small\nmy_norm = \"log\" \n# ***this will number our x and y axes based on the longitude latitude range***\nmy_extent = [min_lon, max_lon, min_lat, max_lat]\n# ensure the aspect ratio of our plot fits all data, matplotlib can does this automatically\nmy_aspect = 'auto'\n# tell matplotlib, our origin is the lower-left corner\nmy_origin = 'lower'\n# select a colormap for our plot and the color bar on the right\nmy_cmap = 'Oranges'\n\n# create our plot using imshow\nplot = plt.imshow(data_array_at_latlon.values, norm=my_norm, extent=my_extent, \n          aspect=my_aspect, origin=my_origin, cmap=my_cmap)\n\n# draw coastlines\nmy_plt.coastlines()\n\n# draw latitude longitude lines\n# ref: https://scitools.org.uk/cartopy/docs/latest/gallery/gridlines_and_labels/gridliner.html\nmy_plt.gridlines(draw_labels=True)\n\n# add a colorbar to our figure, based on the plot we just made above\nmy_fig.colorbar(plot,location='right', label='ug/m^3')\n\n# Set x and y axis labels on our ax\nmy_plt.set_xlabel('Longitude')\nmy_plt.set_ylabel('Latitude')\n\n# Set title of our figure\nmy_fig.suptitle('Ground level concentration of PM2.5 microns and smaller')\n\n# Set title of our plot as the timestamp of our data\nmy_plt.set_title(f'{my_timestamp}')\n\n# Show the resulting visualization\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndata_array_at_latlon.plot(vmin=0, vmax=30)\n\n\n\n\n\n\n\n\n\n\n0.7.1 Please reach out to Arleth Salinas or Valerio Pascucci for any concerns about the notebook. Thank you!\n\nArleth Salinas (arleth.salinas@utah.edu)\nValerio Pascucci (pascucci.valerio@gmail.com)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>demo.html</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2  Introduction",
    "section": "",
    "text": "Wildfires in North America have significantly impacted ecosystems and human society [1]. Climate change affects the frequency, duration, and severity of wildfires thus necessitating the use of wildfire prediction systems to effectively mitigate wildfire impact. However, data for understanding the impact of climate change on wildfires is limited, only available for a few regions and for only a few decades [2]. Furthermore, wildfire prediction systems in North America prioritize decision making and fire management on short timescales, from minutes to months. Therefore, long term wildfire prediction systems have limited access to aggregate short term data, due to resource constraints from fire management entities to share the data they collect and curate.\nThe Weather Forecast Research Team at the University of British Columbia (UBC) generates a short term dataset of PM2.5 smoke particulate presence in North America. Over the past 3 years, each day four times a day, UBC has created forecasts of PM2.5 smoke particulate on the ground for Canada and the continental United States. This is done using their The BlueSky Western Canada Wildfire Smoke Forecasting System. UBC provides access to this data to paying customers and for free on a daily basis via a web-based visualization and file download.\nThese smoke predictions are useful for those who must make decisions on how to deal with smoke as it comes. However, these years of forecasts are not available in a non-trivial fashion for long term forecasting. The data only exists among the hundreds of netCDF files that UBC has generated.\nOur task is to obtain a single long term dataset from the smoke forecast files that are available from UBC.\n\n3 References\n[1] https://dl.acm.org/doi/abs/10.1145/583890.583893\n[2] climate change and wildfire article that was in spanish",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data_source.html",
    "href": "data_source.html",
    "title": "4  The Data Source",
    "section": "",
    "text": "4.1 Downloading UBC Smoke Forecast Files",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Data Source</span>"
    ]
  },
  {
    "objectID": "data_source.html#downloading-ubc-smoke-forecast-files",
    "href": "data_source.html#downloading-ubc-smoke-forecast-files",
    "title": "4  The Data Source",
    "section": "",
    "text": "4.1.1 Available Time Range\nTODO CREDIT UBC The time ranges of available data for each forecast ID is shown in Table 4.1. Please note, there are occassional failed forecasts or otherwise unavailable files within the date ranges specified Table 4.1.\n\n\n\nTable 4.1: Dates for which all Forecast ID Datasets are Publicly Available. All times are in UTC and the grid size is 12 km.\n\n\n\n\n\n\n\n\n\n\n\n\nForecast ID\nMeteorology Forecast Initialization (UTC)\nSmoke Forecast Initialization (UTC)\nStart Date\nEnd Date\n\n\n\n\nBSC00CA12-01\n00Z\n08Z\nMarch 4, 2021\nPresent Day\n\n\nBSC06CA12-01\n06Z\n14Z\nMarch 4, 2021\nPresent Day\n\n\nBSC12CA12-01\n12Z\n20Z\nMarch 3, 2021\nPresent Day\n\n\nBSC18CA12-01\n18Z\n02Z\nMarch 4, 2021\nPresent Day\n\n\n\n\n\n\nTODO what do grid sizes mean, i.e. Meteorology: 00 UTC, nested 12 km + 36 km grids Smoke: 08 UTC\nThe smoke forecasts are updated daily, including the present day, so there is no fixed end date. Therefore, the latest data must be downloaded on a regular basis. We have not implemented this process yet, so the latest forecast files we use are up to June 27, 2024.\nThere is no official source stating the earliest available date for each forecast. So, knowing the project began in 2021, we inferenced that the earliest available date would be in 2021. Via trial and error we found the earliest available dates.\n\n\n4.1.2 Download Instructions\nTo download the 2-day forecast for the forecast initialization date of one’s choice, one follows the instructions below. The downloaded file can be a NetCDF or KMZ file. We use NetCDF files. Later in Chapter 5 we detail our automation of the download process described below.\n\nTODO CREDIT UBC\nGo to the URL: https://firesmoke.ca/forecasts/{Forecast ID}/{YYYYMMDD}{InitTime}/{File Type}\nWhere:\n\nYYYYMMDD is the date of choice.\nForecastID and InitTime are the chosen values as described in Table 4.2.\nFile Type is either dispersion.nc or dispersion.kmz for either the NetCDF file or KMZ file, respectively.\n\n\n\n\nTable 4.2: UBC Smoke Forecast Data Download Parameters. All times are in UTC and the grid size is 12 km.\n\n\n\n\n\nForecast ID\nSmoke Forecast Initialization (UTC)\n\n\n\n\nBSC00CA12-01\n08Z\n\n\nBSC06CA12-01\n14Z\n\n\nBSC12CA12-01\n20Z\n\n\nBSC18CA12-01\n02Z\n\n\n\n\n\n\n\n4.1.2.1 Download Example\nLet’s try downloading the forecast for January 1, 2024 where the weather forecast is initiated at 00:00:00 UTC and the smoke forecast is initialized at 08:00:00 UTC by navigating to the corresponding URL.\n\n\nCode\nforecast_id = \"BSC00CA12-01\"\nyyyymmdd = \"20210304\"\ninit_time = \"08\"\n\nurl = (\n    f\"https://firesmoke.ca/forecasts/{forecast_id}/{yyyymmdd}{init_time}/dispersion.nc\"\n)\n\nprint(f\"Navigate to this URL in your browser: {url}\")\n\n\nNavigate to this URL in your browser: https://firesmoke.ca/forecasts/BSC00CA12-01/2021030408/dispersion.nc",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Data Source</span>"
    ]
  },
  {
    "objectID": "data_source.html#the-netcdf-file",
    "href": "data_source.html#the-netcdf-file",
    "title": "4  The Data Source",
    "section": "4.2 The NetCDF File",
    "text": "4.2 The NetCDF File\nNext, let’s look at what is within the NetCDF file located at the URL in our previous example.\n\n4.2.1 File Preview\n\nWe load dispersion.nc using xarray, which provides a handy preview of the file.\n\n\nCode\nimport xarray as xr\n\nds = xr.open_dataset(\"data_notebooks/data_source/dispersion.nc\")\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (TSTEP: 51, VAR: 1, DATE-TIME: 2, LAY: 1, ROW: 381, COL: 1041)\nDimensions without coordinates: TSTEP, VAR, DATE-TIME, LAY, ROW, COL\nData variables:\n    TFLAG    (TSTEP, VAR, DATE-TIME) int32 ...\n    PM25     (TSTEP, LAY, ROW, COL) float32 ...\nAttributes: (12/33)\n    IOAPI_VERSION:  $Id: @(#) ioapi library version 3.0 $                    ...\n    EXEC_ID:        ????????????????                                         ...\n    FTYPE:          1\n    CDATE:          2021063\n    CTIME:          101914\n    WDATE:          2021063\n    ...             ...\n    VGLVLS:         [10.  0.]\n    GDNAM:          HYSPLIT CONC    \n    UPNAM:          hysplit2netCDF  \n    VAR-LIST:       PM25            \n    FILEDESC:       Hysplit Concentration Model Output                       ...\n    HISTORY:        xarray.DatasetDimensions:TSTEP: 51VAR: 1DATE-TIME: 2LAY: 1ROW: 381COL: 1041Coordinates: (0)Data variables: (2)TFLAG(TSTEP, VAR, DATE-TIME)int32...units :&lt;YYYYDDD,HHMMSS&gt;long_name :TFLAG           var_desc :Timestep-valid flags:  (1) YYYYDDD or (2) HHMMSS                                [102 values with dtype=int32]PM25(TSTEP, LAY, ROW, COL)float32...long_name :PM25            units :ug/m^3          var_desc :PM25                                                                            [20227671 values with dtype=float32]Indexes: (0)Attributes: (33)IOAPI_VERSION :$Id: @(#) ioapi library version 3.0 $                                           EXEC_ID :????????????????                                                                FTYPE :1CDATE :2021063CTIME :101914WDATE :2021063WTIME :101914SDATE :2021063STIME :90000TSTEP :10000NTHIK :1NCOLS :1041NROWS :381NLAYS :1NVARS :1GDTYP :1P_ALP :0.0P_BET :0.0P_GAM :0.0XCENT :-104.0YCENT :51.0XORIG :-156.0YORIG :32.0XCELL :0.10000000149011612YCELL :0.10000000149011612VGTYP :5VGTOP :-9999.0VGLVLS :[10.  0.]GDNAM :HYSPLIT CONC    UPNAM :hysplit2netCDF  VAR-LIST :PM25            FILEDESC :Hysplit Concentration Model Output                                              lat-lon coordinate system                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       HISTORY :\n\n\n\n\n4.2.2 File Attributes\ndispersion.nc contains the following attributes. Note that for all files across forecast IDs, they have the same dimension and variable names:\n\n4.2.2.1 Dimensions:\nThe dimensions described in Table 4.3 determine on which indicies we may index our variables.\n\n\n\nTable 4.3: Description of Dimensions for Indexing Data in NetCDF Files\n\n\n\n\n\n\n\n\n\n\nDimension\nSize\nDescription\n\n\n\n\nTSTEP\n51\nThis dimension represents the number of time steps in the file. Each file has 51 hours represented.\n\n\nVAR\n1\nThis dimension is a placeholder for the variables in the file.\n\n\nDATE-TIME\n2\nThis dimension stores the date and time information for each time step.\n\n\nLAY\n1\nThis dimension represents the number of layers in the file, which is 1 in this case.\n\n\nROW\n381\nThis dimension represents the number of rows in the spatial grid.\n\n\nCOL\n1041\nThis dimension represents the number of columns in the spatial grid.\n\n\n\n\n\n\n\n\n4.2.2.2 Variables:\nThe variables described in Table 4.4 contain the data in question that we would like to extract.\n\n\n\nTable 4.4: Description of Variables in NetCDF Files\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDimensions\nData Type\nDescription\n\n\n\n\nTFLAG\nTSTEP, VAR, DATE-TIME\nint32\nThis variable stores the date and time of each time step.\n\n\nPM25\nTSTEP, LAY, ROW, COL\nfloat32\nThis variable contains the concentration of particulate matter (PM2.5) for each time step, layer, row, and column in the spatial grid.\n\n\n\n\n\n\n\n\n4.2.2.3 Attributes\nOf the 33 available attributes we use the ones shown in Table 4.5: TODO NEED TO CONFIRM IF THIS IS RIGHT\n\n\n\nTable 4.5: Description of Attributes in NetCDF Files\n\n\n\n\n\n\n\n\n\n\nAttribute\nValue\nDescription\n\n\n\n\nCDATE\n2021063\nThe creation date of the dataset, in YYYYDDD format.\n\n\nCTIME\n101914\nThe creation time of the dataset, in HHMMSS format.\n\n\nWDATE\n2021063\nThe date for which the weather forecast is initiated, in YYYYDDD format.\n\n\nWTIME\n101914\nThe time for which the weather forecast is initiated, in HHMMSS format.\n\n\nSDATE\n2021063\nThe date for which the smoke forecast is initiated,in YYYYDDD format.\n\n\nSTIME\n90000\nThe time for which the weather forecast is initiated, in HHMMSS format.\n\n\nNCOLS\n1041\nThe number of columns in the spatial grid.\n\n\nNROWS\n381\nThe number of rows in the spatial grid.\n\n\nXORIG\n-156.0\nThe origin (starting point) of the grid in the x-direction.\n\n\nYORIG\n32.0\nThe origin (starting point) of the grid in the y-direction.\n\n\nXCELL\n0.10000000149011612\nThe cell size in the x-direction.\n\n\nYCELL\n0.10000000149011612\nThe cell size in the y-direction.\n\n\n\n\n\n\nNow that we understand what exactly is within one NetCDF file, let’s establish the information we have when we download all available NetCDF files.\n\n\n\n4.2.3 Using the Data\nWe demonstrate a simple example of querying data at a certain timestep within this file and visualizing that data.\n\n\nCode\n1ds = xr.open_dataset(\"data_notebooks/data_source/dispersion.nc\")\n\n\n\n1\n\nOpen the NetCDF file.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Data Source</span>"
    ]
  },
  {
    "objectID": "data_source.html#metadata-of-collected-netcdf-files",
    "href": "data_source.html#metadata-of-collected-netcdf-files",
    "title": "4  The Data Source",
    "section": "4.3 Metadata of Collected NetCDF Files",
    "text": "4.3 Metadata of Collected NetCDF Files\nHere we describe metadata about the collection of NetCDF files we downloaded.\n\n4.3.1 Disk Size\nFor the time ranges we cover, Table 6.1 shows how large the set of files per forecast ID are.\n\n\n\nTable 4.6: File Sizes and Counts for Each Forecast ID within the Specified Date Range\n\n\n\n\n\n\n\n\n\n\n\nForecast ID\nDate Range\nSize\nFile Count\n\n\n\n\nBSC00CA12-01\nMarch 4, 2021 - June 27, 2024\n84G\n1077\n\n\nBSC06CA12-01\nMarch 4, 2021 - June 27, 2024\n78G\n1022\n\n\nBSC12CA12-01\nMarch 3, 2021 - June 27, 2024\n79G\n1022\n\n\nBSC18CA12-01\nMarch 4, 2021 - June 27, 2024\n79G\n1023\n\n\nTotal\n\n320G\n4144\n\n\n\n\n\n\n\n\n4.3.2 Data Uniformity\nTo create our dataset, we used the following understanding about the attributes across all files:\n\nAll files across all four forecast IDs have the same NROWS, XORIG, YORIG, XCELL, YCELL values.\nSome files have either NCOLS = 1041 or NCOLS = 1081.\n{C,W,S}_DATE and {C,W,S}_TIME values are unique to each file.\n\nNow that we know what data is available and how it can be obtained, we describe next how to extract data from these files to form one dataset.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Data Source</span>"
    ]
  },
  {
    "objectID": "data_loading.html",
    "href": "data_loading.html",
    "title": "5  Data Loading",
    "section": "",
    "text": "5.1 Downloading Data Locally\nWe decided to download all the available files from the data source onto our data staging machine to process from there.\nWe created 4 directories for each forecast ID that UBC provides at the following directory on our machine:\nThe following shows our approaches to doing this and discusses how our approach evolved. Note the scripts below refer to varying directories, but through simple copying operations we stored the final downloaded files to the directories listed above.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Loading</span>"
    ]
  },
  {
    "objectID": "data_loading.html#downloading-data-locally",
    "href": "data_loading.html#downloading-data-locally",
    "title": "5  Data Loading",
    "section": "",
    "text": "/usr/sci/cedmav/data/firesmoke\n├── BSC00CA12-01\n├── BSC06CA12-01\n├── BSC12CA12-01\n├── BSC18CA12-01\n\n\n5.1.1 First Approach\nWe delineate our first approach by detailing our download script, which is available in it’s entirety in the side bar.\n\n\n\nCode\nimport wget\nimport pandas as pd\n\n1ids = [\"BSC18CA12-01\", \"BSC00CA12-01\", \"BSC06CA12-01\", \"BSC12CA12-01\"]\nstart_dates = [\"20210304\", \"20210304\", \"20210304\", \"20210303\"]\nend_dates = [\"20231016\", \"20240210\", \"20231016\", \"20231015\"]\ninit_times = [\"02\", \"08\", \"14\", \"20\"]\n\n2for i in zip(start_dates, end_dates, ids, init_times):\n    start_date = i[0]\n    end_date = i[1]\n    forecast_id = i[2]\n    init_time = i[3]\n\n    dates = pd.date_range(start=start_date, end=end_date)\n    dates = dates.strftime(\"%Y%m%d\").tolist()\n\n3    for date in dates:\n      url = (\n          \"https://firesmoke.ca/forecasts/\"\n          + forecast_id\n          + \"/\"\n          + date\n          + init_time\n          + \"/dispersion.nc\"\n      )\n4      directory = \"/Users/arleth/Mount/firesmoke/\" + forecast_id + \"/dispersion_\" + date + \".nc\"\n      wget.download(url, out=directory)\n\n\n\n1\n\nFirst, create 4 lists containing forecast IDs, the start and end dates we wish to index on, and the smoke forecast initiation times. We will loop through the 4 sets of parameters.\n\n2\n\nIn a for loop, we use pandas to create a list of every date from the start date and end date of the current iteration. We will loop through these dates next.\n\n3\n\nFor each date in the list, we create the url to download the file.\n\n4\n\nFinally, we use wget to download the contents at urlto directory. We append date to the file name so each file downloaded is identifiable by date.\n\n\n\n\nWe assumed that for all URLs, there was an available NetCDF file for download\nHowever, we realized that we downloaded either a NetCDF file or an HTML webpage. Using wget forcibly saved the contents at the URL into a NetCDF file.\nThis issue was not identified until after we visualized each hour of the data, and we noticed gaps and errors in our scripts to create visualizations. See Chapter 7 for further details on identifying these issues. For now, we show our modified approach to downloading the NetCDF files.\n\n\n5.1.2 Second Approach\nOur second approach is similar to the first, except we use requests, an HTTP client library that allows us to see the headers returned from the URL we query. The script is available in the side bar.\n\n\n\nCode\nimport requests\nimport pandas as pd\nfrom datetime import datetime\n\n1ids = [\"BSC18CA12-01\", \"BSC00CA12-01\", \"BSC06CA12-01\", \"BSC12CA12-01\"]\nstart_dates = [\"20210304\", \"20210304\", \"20210304\", \"20210303\"]\ntoday = datetime.now().strftime(\"%Y%m%d\")\ninit_times = [\"02\", \"08\", \"14\", \"20\"]\n\n2for i in zip(start_dates, ids, init_times):\n    start_date = i[0]\n    forecast_id = i[1]\n    init_time = i[2]\n\n    dates = pd.date_range(start=start_date, end=today)\n    dates = dates.strftime(\"%Y%m%d\").tolist()\n\n3    for date in dates:\n        url = (\n            \"https://firesmoke.ca/forecasts/\"\n            + forecast_id\n            + \"/\"\n            + date\n            + init_time\n            + \"/dispersion.nc\"\n        )\n4        directory = (\n            \"/usr/sci/scratch_nvme/arleth/basura_total/\"\n            + forecast_id\n            + \"/dispersion_\"\n            + date\n            + \".nc\"\n        )\n\n5        response = requests.get(url, stream=True)\n        header = response.headers\n        if (\n            \"Content-Type\" in header\n            and header[\"Content-Type\"] == \"application/octet-stream\"\n        ):\n6            with open(directory, mode=\"wb\") as file:\n                file.write(response.content)\n                print(f\"Downloaded file {directory}\")\n        else:\n            print(header[\"Content-Type\"])\n\n\n\n1\n\nFirst, create 3 lists containing forecast IDs, the start dates we wish to index on, and the smoke forecast initiation times . Notice we define a variable today, this allows us to run this script and query all URLs up to today’s date. Note we ran up to June 27, 2024 for now. We will loop through these sets of parameters.\n\n2\n\nIn a for loop, we use pandas to create a list of every date from the start date and end date of the current iteration. We will loop through these dates next.\n\n3\n\nFor each date in the list, we create the url to download the file.\n\n4\n\nDefine directory, the directory and file name to save the file to.\n\n5\n\nWe use requests to get the HTTP header at url. We inspect the Content-Type and if it is application/octet-stream, we download the file. We confirmed that a URL with a NetCDF file had this content type header.\n\n6\n\nWe write the content to directory, else we print the content header out to check what it is.\n\n\n\n\nThis approach yielded the results we expected, we downloaded only NetCDF files. We had failed downloads which appeared during conversion as described in Chapter 6. We assumed those files were unavailable from UBC which we later confirmed as described in Chapter 7.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Loading</span>"
    ]
  },
  {
    "objectID": "data_conversion.html",
    "href": "data_conversion.html",
    "title": "6  Data Conversion",
    "section": "",
    "text": "6.1 On Data Validation\nWe decided to perform data validation after conversion to the IDX file format. However, we realized that performing data validation both before and after conversion would be best. This is explored further in Chapter 7.\nFor now, the reader should understand that data validation of the NetCDF files is different from data validation of the IDX file. In this chapter, we use the assumption that the NetCDF files we can open have complete and uncorrupted data for conversion to IDX.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Conversion</span>"
    ]
  },
  {
    "objectID": "data_conversion.html#overview",
    "href": "data_conversion.html#overview",
    "title": "6  Data Conversion",
    "section": "6.2 Overview",
    "text": "6.2 Overview\nFor all our conversion attempts, the same general process is followed:\n\nCheck which NetCDF files were successfully downloaded from the data source by attempting to open each downloaded file with xarray.\nStep though all files in a chronological sequence, hour by hour, and save data at each hour to an IDX file using the OpenVisus framework.\n\nWe will describe the latest version of our conversion, version 4. Throughout, we will explain how previous attempts differed. To see previous attemps in their entirety, refer to the side bar. Please note that the previous scripts were working scripts, therefore they may be incomplete.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Conversion</span>"
    ]
  },
  {
    "objectID": "data_conversion.html#setting-system-directories",
    "href": "data_conversion.html#setting-system-directories",
    "title": "6  Data Conversion",
    "section": "6.3 Setting System Directories",
    "text": "6.3 Setting System Directories\nFirst we set the directory paths we want to use during the conversion process, which is to our 4 directories of NetCDF files for each forecast ID.\n\n\nCode\n1firesmoke_dir = \"/usr/sci/cedmav/data/firesmoke\"\nidx_dir = \"/usr/sci/scratch_nvme/arleth/idx/firesmoke\"\n\n2ids = [\"BSC18CA12-01\", \"BSC00CA12-01\", \"BSC06CA12-01\", \"BSC12CA12-01\"]\nstart_dates = [\"20210304\", \"20210304\", \"20210304\", \"20210303\"]\nend_dates = [\"20240627\", \"20240627\", \"20240627\", \"20240627\"]\n\n\n\n1\n\nEstablish the directory where all forecast ID NetCDFs are stored and where to save our IDX file on the ‘atlantis’ machine.\n\n2\n\nDefine the forecast IDs and dates we will loop over.\n\n\n\n\n\n6.3.1 Rationale and Future Improvements\nIn versions 1 and 2 of our conversion attempt, we did not use all four sets of forecast ID files. We only used BSC12CA12-01 files to compile a single data set. We learned that by not using all four sets of data, the data set we created was less accurate. See Chapter 7 for further details.\nTherefore we decide to use all four datasets. We elect to use dates up to June 26, 2024 as this was the last time we ran our scripts. We have yet to address the issue of how to keep the IDX file constantly up to date with data available up to the present day.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Conversion</span>"
    ]
  },
  {
    "objectID": "data_conversion.html#checking-the-netcdf-files",
    "href": "data_conversion.html#checking-the-netcdf-files",
    "title": "6  Data Conversion",
    "section": "6.4 Checking the NetCDF Files",
    "text": "6.4 Checking the NetCDF Files\nRecall we downloaded all NetCDF files available from UBC onto our machine in their respective directories as follows:\n/usr/sci/cedmav/data/firesmoke\n├── BSC00CA12-01\n├── BSC06CA12-01\n├── BSC12CA12-01\n├── BSC18CA12-01\nHere, we identify which NetCDF files for each forecast ID successfully open with xarray and store them in a dictionary.\nWe also confirm the following conditions we established in Chapter 4 by using dictionaries to track the max values and unique values of these attributes across all files:\n\nAll files across all four forecast IDs have the same NROWS, XORIG, YORIG, XCELL, YCELL values.\nSome files have either NCOLS = 1041 or NCOLS = 1081, but always NROWS = 381.\n\n\n\nCode\nimport os\nimport xarray as xr\nimport numpy as np\nimport tqdm\n\n1successful_files = {id_: [] for id_ in ids}\n\n2max_ncols = {id_: 0 for id_ in ids}\nmax_nrows = {id_: 0 for id_ in ids}\n3ncols = {id_: set() for id_ in ids}\nnrows = {id_: set() for id_ in ids}\n\n4max_grid_x = {id_: {\"xorig\": 0.0, \"xcell\": 0.0} for id_ in ids}\nmax_grid_y = {id_: {\"yorig\": 0.0, \"ycell\": 0.0} for id_ in ids}\n5xorigs = {id_: set() for id_ in ids}\nxcells = {id_: set() for id_ in ids}\nyorigs = {id_: set() for id_ in ids}\nycells = {id_: set() for id_ in ids}\n\n6for id_ in ids:\n7    file_names = os.listdir(f\"{firesmoke_dir}/{id_}/\")\n\n8    for file in tqdm(file_names):\n9        path = f\"{firesmoke_dir}/{id_}/{file}\"\n\n10        try:\n            ds = xr.open_dataset(path)\n\n11            successful_files[id_].append(file)\n\n12            max_ncols[id_] = max(max_ncols[id_], ds.NCOLS)\n            max_nrows[id_] = max(max_nrows[id_], ds.NROWS)\n            max_grid_x[id_][\"xorig\"] = max(max_grid_x[id_][\"xorig\"], ds.XORIG, key=abs)\n            max_grid_y[id_][\"yorig\"] = max(max_grid_y[id_][\"yorig\"], ds.YORIG, key=abs)\n            max_grid_x[id_][\"xcell\"] = max(max_grid_x[id_][\"xcell\"], ds.XCELL, key=abs)\n            max_grid_y[id_][\"ycell\"] = max(\n                max_grid_y[id_][\"ycell\"], ds.YCELL, key=abs\n            )\n\n13            ncols[id_].add(ds.NCOLS)\n            nrows[id_].add(ds.NROWS)\n            xorigs[id_].add(ds.XORIG)\n            yorigs[id_].add(ds.YORIG)\n            xcells[id_].add(ds.XCELL)\n            ycells[id_].add(ds.YCELL)\n\n14        except:\n            continue\n\n15for id_ in successful_files:\n    successful_files[id_] = np.sort(successful_files[id_]).tolist()\n\n\n\n1\n\nInitialize a dictionary to hold an empty list for each forecast ID. We update it with the file names that successfully open under the forecast ID directory.\n\n2\n\nInitialize dictionaries to hold an integer for each forecast ID. We update it to hold the maximum NCOLS/NROWS value available within forecast ID’s set of NetCDF files.\n\n3\n\nInitialize dictionaries to hold a set for each forecast ID. We update the set to hold all the unique NCOLS/NROWS values available within the forecast ID’s set of NetCDF files.\n\n4\n\nInitialize dictionaries to hold a dictionary of xorig/yorig and xcell/ycell values for each forecast ID. We update it to hold the maximum xorig/yorig and xcell/ycell pairs available within the forecast ID’s set of NetCDF files.\n\n5\n\nInitialize dictionaries to track unique xorig/yorig and xcell/ycell values.\n\n6\n\nFor each forecast ID, we populate the dictionaries above.\n\n7\n\nObtain a list of file names under the directory for id_. We loop through each file next.\n\n8\n\nBegin loop over each file. Note tqdm is just an accessory for generating a visible status bar in our Jupyter Notebook.\n\n9\n\nObtain absolute path name to current file.\n\n10\n\nHere we use a try statement since opening the file with xarray may lead to an error. except allows us to catch the exception accordingly and continue trying to open each file.\n\n11\n\nAt this line, the file opened without issue in xarray, so append this file name to the id_ list in the successful_files dictionary.\n\n12\n\nUse max to save the largest values in our dictionaries accordingly.\n\n13\n\nUpdate the dictionaries of sets with the file’s attributes, to ensure we catch all unique values.\n\n14\n\nIf the file did not open during the try continue to the next file.\n\n15\n\nSort the lists of successfully opened files by name, so they are in chronological order.\n\n\n\n\nThe following shows the information gathered:\n\nBSC18CA12-01BSC00CA12-01BSC06CA12-01BSC12CA12-01\n\n\ndataset: BSC18CA12-01\nNumber of successful files: 1010\nMax cell sizes: max_ncols = 1081 and max_nrows = 381\nMax xorig & xcell: {'xorig': -160.0, 'xcell': 0.10000000149011612}\nMax yorig & ycell: {'yorig': 32.0, 'ycell': 0.10000000149011612}\nncols: {1081, 1041}\nnrows: {381}\nxorigs: {-160.0, -156.0}\nyorigs: {32.0}\nxcells: {0.10000000149011612}\nycells: {0.10000000149011612}\n\n\ndataset: BSC00CA12-01\nNumber of successful files: 1067\nMax cell sizes: max_ncols = 1081 and max_nrows = 381\nMax xorig & xcell: {'xorig': -160.0, 'xcell': 0.10000000149011612}\nMax yorig & ycell: {'yorig': 32.0, 'ycell': 0.10000000149011612}\nncols: {1081, 1041}\nnrows: {381}\nxorigs: {-160.0, -156.0}\nyorigs: {32.0}\nxcells: {0.10000000149011612}\nycells: {0.10000000149011612}\n\n\ndataset: BSC06CA12-01\nNumber of successful files: 997\nMax cell sizes: max_ncols = 1081 and max_nrows = 381\nMax xorig & xcell: {'xorig': -160.0, 'xcell': 0.10000000149011612}\nMax yorig & ycell: {'yorig': 32.0, 'ycell': 0.10000000149011612}\nncols: {1081, 1041}\nnrows: {381}\nxorigs: {-160.0, -156.0}\nyorigs: {32.0}\nxcells: {0.10000000149011612}\nycells: {0.10000000149011612}\n\n\ndataset: BSC12CA12-01\nNumber of successful files: 1003\nMax cell sizes: max_ncols = 1081 and max_nrows = 381\nMax xorig & xcell: {'xorig': -160.0, 'xcell': 0.10000000149011612}\nMax yorig & ycell: {'yorig': 32.0, 'ycell': 0.10000000149011612}\nncols: {1081, 1041}\nnrows: {381}\nxorigs: {-160.0, -156.0}\nyorigs: {32.0}\nxcells: {0.10000000149011612}\nycells: {0.10000000149011612}\n\n\n\n\n6.4.1 Rationale and Future Improvements\nWe have used the successful_files dictionary to test which files are usable by testing whether they open with xarray or not.\nIn version 2 of our conversion, we discovered that some files had differing values for NCOLS. The IDX file format expects all arrays across all time steps to have the same number of rows and columns. We realized that our assumption that all attributes were the same across all files was unfounded. Therefore, we collected information about the values for all attributes in the NetCDF files as shown above so that we could resample arrays to use 1081 columns.\nOne improvement to this step is to stop tracking maxes and unique values seperately. Instead, we could just track unique values then get maxes from there.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Conversion</span>"
    ]
  },
  {
    "objectID": "data_conversion.html#preparing-resampling-grids",
    "href": "data_conversion.html#preparing-resampling-grids",
    "title": "6  Data Conversion",
    "section": "6.5 Preparing Resampling Grids",
    "text": "6.5 Preparing Resampling Grids\nTo resample arrays of shape 1041x381 to 1081x381, we use the SciPy griddata function from the interpolate package. This function gives interpolated values on set of points xi from a set of points with corresponding values. We refer the reader to SciPy’s documentation for details.\nRecall we can generate a set of latitude and longitude coordinates on the 1041x381 and 1081x381 grid by using the attributes given in each NetCDF file, see Chapter 4 for an example. We can thus sample a set of corresponding PM25 values on a 1081x381 grid by using using a 1041x381 array of latitude and longitudes as points and 1041x381 array of PM25 values as values with the griddata function. We willgenerate these grids of lat/lon points using the attribute information we collected in the previous step.\n\n6.5.1 Generate Grids of Latitude and Longitude Points\n1max_xorig = max_grid_x[ids[0]]['xorig']\nmax_xcell = max_grid_x[ids[0]]['xcell']\nmax_yorig = max_grid_y[ids[0]]['yorig']\nmax_ycell = max_grid_y[ids[0]]['ycell']\n\n2big_lon = np.linspace(max_xorig, max_xorig + max_xcell * (max_ncols[ids[0]] - 1), max_ncols[ids[0]])\nbig_lat = np.linspace(max_yorig, max_yorig + max_ycell * (max_nrows[ids[0]] - 1), max_nrows[ids[0]])\n\n3big_lon_pts, big_lat_pts = np.meshgrid(big_lon, big_lat)\nbig_tups = np.array([tup for tup in zip(big_lon_pts.flatten(), big_lat_pts.flatten())])\n\n4sml_ds = xr.open_dataset(firesmoke_dir + \"/BSC00CA12-01/dispersion_20210304.nc\")\nsml_lon = np.linspace(sml_ds.XORIG, sml_ds.XORIG + sml_ds.XCELL * (sml_ds.NCOLS - 1), sml_ds.NCOLS)\nsml_lat = np.linspace(sml_ds.YORIG, sml_ds.YORIG + sml_ds.YCELL * (sml_ds.NROWS - 1), sml_ds.NROWS)\n\n5sml_lon_pts, sml_lat_pts = np.meshgrid(sml_lon, sml_lat)\nsml_tups = np.array([tup for tup in zip(sml_lon_pts.flatten(), sml_lat_pts.flatten())])\n\n1\n\nGet the x/y origin and cell size parameters for the big 1081x381 grid.\n\n2\n\nGenerate one two lists, defining a grid of latitudes and longitudes.\n\n3\n\nUsing big_lon and big_lat, use meshgrid to generate our 1081x381 set of longitudes and latitudes.\n\n4\n\nOpen a file that uses the small 1041x381 grid. Then, use the attributes in that file to generate two lists defining a grid of latitudes and longitudes.\n\n5\n\nUsing sml_lon and sml_lat, use meshgrid to generate our 1041x381 set of longitudes and latitudes.\n\n\n\n\n6.5.2 Experimenting with griddata\nNow that we have the two sets of latitude and longitude points, we show an example of how these are used to resample an array of data from a 1041x381 grid to a 1081x381 grid.\n\n\n6.5.3 Rationale and Previous Revelations\nIDX expects all arrays at each time step to be the same dimensionality. However, as noted above the data has either NCOLS = 1041 or NCOLS = 1081.\nWe considered various approaches to the question of: How do we normalize variance in data dimensionality?\n\n\n\nTable 6.1: The Weaknesses of Approaches to Handling Varying Array Sizes\n\n\n\n\n\n\n\n\n\nApproach\nWeakness\n\n\n\n\nExclude arrays with 1041 columns.\nThrowing away those data points would discard all the information they hold.\n\n\nForce data with 1041 columns into an array with 1081 columns without resampling.\nThis results in unused columns within the 1081-column array, leading to discontinuities and potential artifacts in the data representation.\n\n\nCrop arrays on 1081 columns to 1041 columns\nCropping the data would result in loss of information.\n\n\n\n\n\n\nThe approach we chose was to resample the data with 1041 columns to arrays with 1081 columns. This produced the most visually appealing result and preserved the most information possible.\n\ncreate lat/lon points for small grid using dictionaries, not by opening an arbitrary file",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Conversion</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "UBC Smoke Forecast Data Curation",
    "section": "",
    "text": "Preface\nWelcome to NSDF’s UBC Firesmoke Data Curation website. Here we describe the data curation process of UBC’s Smoke Forecast datasets. We inform readers of the challenges and solutions we found when repurposing these short term datasets into a long term dataset.\nIt is important to note that although we will present the data curation process in a linear fashion here, it was not a linear process. Rather, the process was cyclical, and at each iteration we introduced new improvements.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#navigation-and-webpage-information",
    "href": "index.html#navigation-and-webpage-information",
    "title": "UBC Smoke Forecast Data Curation",
    "section": "Navigation and Webpage Information",
    "text": "Navigation and Webpage Information\nThis webpage is produced using Quart.\nYou can find the source code for this page at TODO.\nAll files or directories in this website are hosted at our GitHub repository, unless otherwise specified.\nAll code blocks contain annotations, hover over the numbers on the right hand side to see the accompanying annotation.\n1Hover over me ---&gt;\n\n1\n\nI’m an annotation.",
    "crumbs": [
      "Preface"
    ]
  }
]