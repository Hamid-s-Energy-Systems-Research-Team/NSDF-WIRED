[
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2  Introduction",
    "section": "",
    "text": "Wildfires in North America have significantly impacted ecosystems and human society [1]. Climate change affects the frequency, duration, and severity of wildfires thus necessitating the use of wildfire prediction systems to effectively mitigate wildfire impact. However, data for understanding the impact of climate change on wildfires is limited, only available for a few regions and for only a few decades [2]. Furthermore, wildfire prediction systems in North America prioritize decision making and fire management on short timescales, from minutes to months. Therefore, long term wildfire prediction systems have limited access to aggregate short term data, due to resource constraints from fire management entities to share the data they collect and curate.\nThe Weather Forecast Research Team at the University of British Columbia (UBC) generates a short term dataset of PM2.5 smoke particulate presence in North America. Over the past 3 years, each day four times a day, UBC has created forecasts of PM2.5 smoke particulate on the ground for Canada and the continental United States. This is done using their The BlueSky Western Canada Wildfire Smoke Forecasting System. UBC provides access to this data to paying customers and for free on a daily basis via a web-based visualization and file download.\nThese smoke predictions are useful for those who must make decisions on how to deal with smoke as it comes. However, these years of forecasts are not available in a non-trivial fashion for long term forecasting. The data only exists among the hundreds of NetCDF files that UBC has generated.\nOur task is to obtain a single long term dataset from the smoke forecast files that are available from UBC.\n\n3 References\n[1] https://dl.acm.org/doi/abs/10.1145/583890.583893\n[2] climate change and wildfire article that was in spanish",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "demo.html#this-notebook-provide-the-instructions-on-how-to-read-ubc-firesmoke-data-from-firsmoke_metadata_current.nc-using-xarray-and-the-openvisus-xarray-backend.",
    "href": "demo.html#this-notebook-provide-the-instructions-on-how-to-read-ubc-firesmoke-data-from-firsmoke_metadata_current.nc-using-xarray-and-the-openvisus-xarray-backend.",
    "title": "1  IDX File Demo",
    "section": "1.1 This notebook provide the instructions on how to read UBC firesmoke data from firsmoke_metadata_current.nc using xarray and the OpenVisus xarray backend.",
    "text": "1.1 This notebook provide the instructions on how to read UBC firesmoke data from firsmoke_metadata_current.nc using xarray and the OpenVisus xarray backend.\nDashboard visible here: http://chpc3.nationalsciencedatafabric.org:9988/dashboards",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>IDX File Demo</span>"
    ]
  },
  {
    "objectID": "demo.html#step-1-importing-the-libraries",
    "href": "demo.html#step-1-importing-the-libraries",
    "title": "1  IDX File Demo",
    "section": "1.2 Step 1: Importing the libraries",
    "text": "1.2 Step 1: Importing the libraries\n\n1.2.1 Please be sure to have libraries installed\n\n\nCode\n# for numerical work\nimport numpy as np\n\n# for accessing file system\nimport os\n\n# for loading netcdf files, for metadata\nimport xarray as xr\n# for connecting OpenVisus framework to xarray\n# from https://github.com/sci-visus/openvisuspy, \nfrom openvisuspy.xarray_backend import OpenVisusBackendEntrypoint\n# from backend_v3 import *\n\n# Used for processing netCDF time data\nimport time\nimport datetime\nimport requests\n# Used for indexing via metadata\nimport pandas as pd\n\n# for plotting\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\n\n\n#Stores the OpenVisus cache in the local direcrtory \nimport os\nos.environ[\"VISUS_CACHE\"]=\"./visus_cache_can_be_erased\"\nos.environ['CURL_CA_BUNDLE'] = ''",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>IDX File Demo</span>"
    ]
  },
  {
    "objectID": "demo.html#step-2-reading-the-data-metadata-from-file",
    "href": "demo.html#step-2-reading-the-data-metadata-from-file",
    "title": "1  IDX File Demo",
    "section": "1.3 Step 2: Reading the data & metadata from file",
    "text": "1.3 Step 2: Reading the data & metadata from file\n\n1.3.1 In this section, we load our data using xr.open_dataset.\n\n\nCode\n# path to tiny NetCDF\nurl = 'https://github.com/sci-visus/NSDF-WIRED/raw/main/data/firesmoke_metadata_current.nc'\n\n# Download the file using requests\nresponse = requests.get(url)\nlocal_netcdf = 'firesmoke_metadata.nc'\nwith open(local_netcdf, 'wb') as f:\n    f.write(response.content)\n    \n# open tiny netcdf with xarray and OpenVisus backend\nds = xr.open_dataset(local_netcdf, engine=OpenVisusBackendEntrypoint)\n\n\nov.LoadDataset(http://atlantis.sci.utah.edu/mod_visus?dataset=UBC_fire_smoke_BSC&cached=1)\nPM25\nAdding field  PM25 shape  [27357, 381, 1081, 21] dtype  float32 labels  ['time', 'ROW', 'COL', 'resolution'] Max Resolution  20\n\n\n\n\nCode\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:            (time: 27357, ROW: 381, COL: 1081, resolution: 21,\n                        VAR: 1, DATE-TIME: 2)\nDimensions without coordinates: time, ROW, COL, resolution, VAR, DATE-TIME\nData variables:\n    PM25               (time, ROW, COL, resolution) float32 ...\n    TFLAG              (time, VAR, DATE-TIME) int32 ...\n    wrf_arw_init_time  (time, VAR, DATE-TIME) int32 ...\n    resampled          (time) bool ...\n    CDATE              (time) int32 ...\n    CTIME              (time) int32 ...\n    WDATE              (time) int32 ...\n    WTIME              (time) int32 ...\n    SDATE              (time) int32 ...\n    STIME              (time) int32 ...\nAttributes: (12/28)\n    IOAPI_VERSION:  $Id: @(#) ioapi library version 3.0 $                    ...\n    EXEC_ID:        ????????????????                                         ...\n    FTYPE:          1\n    TSTEP:          10000\n    NTHIK:          1\n    NCOLS:          1081\n    ...             ...\n    GDNAM:          HYSPLIT CONC    \n    UPNAM:          hysplit2netCDF  \n    VAR-LIST:       PM25            \n    FILEDESC:       Hysplit Concentration Model Output                       ...\n    HISTORY:        \n    idx_url:        http://atlantis.sci.utah.edu/mod_visus?dataset=UBC_fire_s...xarray.DatasetDimensions:time: 27357ROW: 381COL: 1081resolution: 21VAR: 1DATE-TIME: 2Coordinates: (0)Data variables: (10)PM25(time, ROW, COL, resolution)float32...long_name :PM25            units :ug/m^3          var_desc :PM25                                                                            [236612908917 values with dtype=float32]TFLAG(time, VAR, DATE-TIME)int32...[54714 values with dtype=int32]wrf_arw_init_time(time, VAR, DATE-TIME)int32...[54714 values with dtype=int32]resampled(time)bool...[27357 values with dtype=bool]CDATE(time)int32...[27357 values with dtype=int32]CTIME(time)int32...[27357 values with dtype=int32]WDATE(time)int32...[27357 values with dtype=int32]WTIME(time)int32...[27357 values with dtype=int32]SDATE(time)int32...[27357 values with dtype=int32]STIME(time)int32...[27357 values with dtype=int32]Indexes: (0)Attributes: (28)IOAPI_VERSION :$Id: @(#) ioapi library version 3.0 $                                           EXEC_ID :????????????????                                                                FTYPE :1TSTEP :10000NTHIK :1NCOLS :1081NROWS :381NLAYS :1NVARS :1GDTYP :1P_ALP :0.0P_BET :0.0P_GAM :0.0XCENT :-106.0YCENT :51.0XORIG :-160.0YORIG :32.0XCELL :0.10000000149011612YCELL :0.10000000149011612VGTYP :5VGTOP :-9999.0VGLVLS :[10.  0.]GDNAM :HYSPLIT CONC    UPNAM :hysplit2netCDF  VAR-LIST :PM25            FILEDESC :Hysplit Concentration Model Output                                              lat-lon coordinate system                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       HISTORY :idx_url :http://atlantis.sci.utah.edu/mod_visus?dataset=UBC_fire_smoke_BSC&cached=1",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>IDX File Demo</span>"
    ]
  },
  {
    "objectID": "demo.html#step-2.5-calculate-derived-metadata-using-original-metadata-above-to-create-coordinates",
    "href": "demo.html#step-2.5-calculate-derived-metadata-using-original-metadata-above-to-create-coordinates",
    "title": "1  IDX File Demo",
    "section": "1.4 Step 2.5, Calculate derived metadata using original metadata above to create coordinates",
    "text": "1.4 Step 2.5, Calculate derived metadata using original metadata above to create coordinates\n\n1.4.1 This is required to allow for indexing of data via metadata\n\n1.4.1.1 Calculate latitude and longitude grid\n\n\nCode\n# Get metadata to compute lon and lat\nxorig = ds.XORIG\nyorig = ds.YORIG\nxcell = ds.XCELL\nycell = ds.YCELL\nncols = ds.NCOLS\nnrows = ds.NROWS\n\nlongitude = np.linspace(xorig, xorig + xcell * (ncols - 1), ncols)\nlatitude = np.linspace(yorig, yorig + ycell * (nrows - 1), nrows)\n\nprint(\"Size of longitude & latitude arrays:\")\nprint(f'np.size(longitude) = {np.size(longitude)}')\nprint(f'np.size(latitude) = {np.size(latitude)}\\n')\nprint(\"Min & Max of longitude and latitude arrays:\")\nprint(f'longitude: min = {np.min(longitude)}, max = {np.max(longitude)}')\nprint(f'latitude: min = {np.min(latitude)}, max = {np.max(latitude)}')\n\n\nSize of longitude & latitude arrays:\nnp.size(longitude) = 1081\nnp.size(latitude) = 381\n\nMin & Max of longitude and latitude arrays:\nlongitude: min = -160.0, max = -51.99999839067459\nlatitude: min = 32.0, max = 70.00000056624413\n\n\n\n\n1.4.1.2 Using calculated latitude and longitude, create coordinates allowing for indexing data using lat/lon\n\n\nCode\n# Create coordinates for lat and lon (credit: Aashish Panta)\nds.coords['lat'] = ('ROW', latitude)\nds.coords['lon'] = ('COL', longitude)\n\n# Replace col and row dimensions with newly calculated lon and lat arrays (credit: Aashish Panta)\nds = ds.swap_dims({'COL': 'lon', 'ROW': 'lat'})\n\n\n\n\n1.4.1.3 Create coordinates allowing for indexing data using timestamp\n\n1.4.1.3.1 First, convert tflags to timestamps that are compatible with xarray\n\n\nCode\ndef parse_tflag(tflag):\n    \"\"\"\n    Return the tflag as a datetime object\n    :param list tflag: a list of two int32, the 1st representing date and 2nd representing time\n    \"\"\"\n    # obtain year and day of year from tflag[0] (date)\n    date = int(tflag[0])\n    year = date // 1000 # first 4 digits of tflag[0]\n    day_of_year = date % 1000 # last 3 digits of tflag[0]\n\n    # create datetime object representing date\n    final_date = datetime.datetime(year, 1, 1) + datetime.timedelta(days=day_of_year - 1)\n\n    # obtain hour, mins, and secs from tflag[1] (time)\n    time = int(tflag[1])\n    hours = time // 10000 # first 2 digits of tflag[1]\n    minutes = (time % 10000) // 100 # 3rd and 4th digits of tflag[1] \n    seconds = time % 100  # last 2 digits of tflag[1]\n\n    # create final datetime object\n    full_datetime = datetime.datetime(year, final_date.month, final_date.day, hours, minutes, seconds)\n    return full_datetime\n\n\n\n\n1.4.1.3.2 Return an array of the tflags as pandas timestamps\n\n\nCode\n# get all tflags\ntflag_values = ds['TFLAG'].values\n\n# to store pandas timestamps\ntimestamps = []\n\n# convert all tflags to pandas timestamps, store in timestamps list\nfor tflag in tflag_values:\n    timestamps.append(pd.Timestamp(parse_tflag(tflag[0])))\n\n# check out the first 3 timestamps\ntimestamps[0:3]\n\n\n[Timestamp('2021-03-04 00:00:00'),\n Timestamp('2021-03-04 01:00:00'),\n Timestamp('2021-03-04 02:00:00')]\n\n\n\n\nCode\n# set coordinates to each timestep with these pandas timestamps\nds.coords['time'] = ('time', timestamps)\n\n\n\n\n\n1.4.1.4 The timestamps may not be intuitive. The following utility function returns the desired pandas timestamp based on your date and time of interest.\n\n1.4.1.4.1 When you index the data at a desired time, use this function to get the timestamp you need to index.\n\n\nCode\ndef get_timestamp(year, month, day, hour):\n    \"\"\"\n    return a pandas timestamp using the given date-time arguments\n    :param int year: year\n    :param int month: month\n    :param int day: day\n    :param int hour: hour\n    \"\"\"\n    # Convert year, month, day, and hour to a datetime object\n    full_datetime = datetime.datetime(year, month, day, hour)\n    \n    # Extract components from the datetime object\n    year = full_datetime.year\n    day_of_year = full_datetime.timetuple().tm_yday\n    hours = full_datetime.hour\n    minutes = full_datetime.minute\n    seconds = full_datetime.second\n\n    # Compute tflag[0] and tflag[1]\n    tflag0 = year * 1000 + day_of_year\n    tflag1 = hours * 10000 + minutes * 100 + seconds\n\n    # Return the Pandas Timestamp object\n    return pd.Timestamp(full_datetime)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>IDX File Demo</span>"
    ]
  },
  {
    "objectID": "demo.html#step-3-select-a-data_slice",
    "href": "demo.html#step-3-select-a-data_slice",
    "title": "1  IDX File Demo",
    "section": "1.5 Step 3: Select a data_slice",
    "text": "1.5 Step 3: Select a data_slice\n\n1.5.1 This section shows you how to load the data you want.\n\n1.5.1.1 You can index the data using indices, timestamps*, latitude & longitude, and by desired resolution**.\n*Not setting any time means the first timestep available is selected. **Not setting quality means full data resolution is selected.\n\n\n1.5.1.1.1 In this case, let’s get all available firesmoke data for March 5, 2021 00:00:00.\n\n\nCode\n# select timestamp\nmy_timestamp = get_timestamp(2021, 3, 5, 0)\n\n# select resolution, let's use full resolution since data isn't too big at one time slice\n# data resolution can be -19 for lowest res and 0 for highest res\ndata_resolution = 0\n\n# get PM25 values and provide 4 values, the colons mean select all lat and lon indices\ndata_array_at_time = ds['PM25'].loc[my_timestamp, :, :, data_resolution]\n\n# notice, to access the data, you must append \".values\" to the data array we got above\nprint(f'timestamp: {my_timestamp}')\nprint(f'shape of data_array_at_time.values = {np.shape(data_array_at_time.values)}')\n\n\ntimestamp: 2021-03-05 00:00:00\nUsing Max Resolution:  20\nTime: 24, max_resolution: 20, logic_box=(0, 1081, 0, 381), field: PM25\nshape of data_array_at_time.values = (381, 1081)\n\n\n\n\n1.5.1.1.2 Perhaps we want to slice a specific latitude longitude range from our data_array_at_time, for example, latitude range [35, 50] and longitude range [-140, -80]. Let’s do that below.\n\n\nCode\n# # define range for latitude and longitude to use\nmin_lat = 35\nmax_lat = 50\nmin_lon = -140\nmax_lon = -80\n\n# min_lon = np.min(longitude)\n# max_lon = np.max(longitude)\n# min_lat = np.min(latitude)\n# max_lat = np.max(latitude)\n\n# get PM25 values and provide 4 values, but this time at our desired ranges\ndata_array_at_latlon = ds['PM25'].loc[my_timestamp, min_lat:max_lat, min_lon:max_lon, data_resolution]\n\n# notice, to access the data, you must append \".values\" to the data array we got above\nprint(f'timestamp: {my_timestamp}')\nprint(f'shape of data_array_at_time.values = {np.shape(data_array_at_latlon.values)}')\n\n\ntimestamp: 2021-03-05 00:00:00\nUsing Max Resolution:  20\nTime: 24, max_resolution: 20, logic_box=(200, 800, 30, 180), field: PM25\nshape of data_array_at_time.values = (150, 600)\n\n\n\n\n\n1.5.1.2 The following are the max and min timestamps, lon/lat values, and data resolutions you can index by\n\n1.5.1.2.1 Be sure you index within the data range, otherwise you may get errors since no data exists outside these ranges!\n\n\nCode\n# NOTE: there is one dummy date, ignore ds['time'].values[-1]\nprint(f\"earliest valid timestamp is: {ds['time'].values[0]}\")\nprint(f\"latest valid timestamp is: {ds['time'].values[-2]}\\n\")\n\nprint(f\"valid longitude range is: {ds['lon'].values[0]}, {ds['lon'].values[-1]}\")\nprint(f\"valid latitude range is: {ds['lat'].values[0]}, {ds['lat'].values[-1]}\\n\")\n\nprint(f\"valid data resolutions range is: [-19, 0]\")\n\n\nearliest valid timestamp is: 2021-03-04T00:00:00.000000000\nlatest valid timestamp is: 2024-06-27T22:00:00.000000000\n\nvalid longitude range is: -160.0, -51.99999839067459\nvalid latitude range is: 32.0, 70.00000056624413\n\nvalid data resolutions range is: [-19, 0]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>IDX File Demo</span>"
    ]
  },
  {
    "objectID": "demo.html#step-4-visualize-data_slice",
    "href": "demo.html#step-4-visualize-data_slice",
    "title": "1  IDX File Demo",
    "section": "1.6 Step 4: Visualize data_slice",
    "text": "1.6 Step 4: Visualize data_slice\n\n1.6.1 One can visualize the data either by:\n\n\n1.6.2 1. Get the values from your data_array_at_time and plot using your favorite python visualization library. We’ll use matplotlib.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>IDX File Demo</span>"
    ]
  },
  {
    "objectID": "demo.html#use-xarrays-built-in-plotting-function-not-recommended-as-it-is-not-robust",
    "href": "demo.html#use-xarrays-built-in-plotting-function-not-recommended-as-it-is-not-robust",
    "title": "1  IDX File Demo",
    "section": "1.7 #### 2. Use xarray’s built in plotting function (not recommended, as it is not robust)",
    "text": "1.7 #### 2. Use xarray’s built in plotting function (not recommended, as it is not robust)\nHere we plot data_array_at_time with matplotlib and its basemap extenstion to add geographic context.\n\n\nCode\n# Let's use matplotlib's imshow, since our data is on a grid\n# ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n\n# Initialize a figure and plot, so we can customize figure and plot of data\n# ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html\n# ref: https://scitools.org.uk/cartopy/docs/latest/getting_started/index.html\nmy_fig, my_plt = plt.subplots(figsize=(15, 6), subplot_kw=dict(projection=ccrs.PlateCarree()))\n\n# Let's set some parameters to get the visualization we want\n# ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n\n# color PM25 values on a log scale, since values are small\nmy_norm = \"log\" \n# this will number our x and y axes based on the longitude latitude range\nmy_extent = [np.min(longitude), np.max(longitude), np.min(latitude), np.max(latitude)]\n# ensure the aspect ratio of our plot fits all data, matplotlib can does this automatically\nmy_aspect = 'auto'\n# tell matplotlib, our origin is the lower-left corner\nmy_origin = 'lower'\n# select a colormap for our plot and the color bar on the right\nmy_cmap = 'Oranges'\n\n# create our plot using imshow\nplot = my_plt.imshow(data_array_at_time.values, norm=my_norm, extent=my_extent, \n          aspect=my_aspect, origin=my_origin, cmap=my_cmap)\n\n# draw coastlines\nmy_plt.coastlines()\n\n# draw latitude longitude lines\n# ref: https://scitools.org.uk/cartopy/docs/latest/gallery/gridlines_and_labels/gridliner.html\nmy_plt.gridlines(draw_labels=True)\n\n# add a colorbar to our figure, based on the plot we just made above\nmy_fig.colorbar(plot,location='right', label='ug/m^3')\n\n# # Set x and y axis labels on our ax\n# my_plt.set_xlabel('Longitude')\n# my_plt.set_ylabel('Latitude')\n\n# Set title of our figure\nmy_fig.suptitle('Ground level concentration of PM2.5 microns and smaller')\n\n# Set title of our plot as the timestamp of our data\nmy_plt.set_title(f'{my_timestamp}')\n\n# Show the resulting visualization\nplt.show()\n\n\n\n\n\n\n\n\n\nHere we plot with xarray’s built-in matplotlib powered plotter.\n\n\nCode\ndata_array_at_time.plot(vmin=0, vmax=30)\n\n\n\n\n\n\n\n\n\n\nHere we plot data_array_at_latlon. We use the exact same code, but define my_extent accordingly.\n\n\nCode\n# Let's use matplotlib's imshow, since our data is on a grid\n# ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n\n# Initialize a figure and plot, so we can customize figure and plot of data\n# ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html\nmy_fig, my_plt = plt.subplots(figsize=(15, 6), subplot_kw=dict(projection=ccrs.PlateCarree()))\n\n# Let's set some parameters to get the visualization we want\n# ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n\n# color PM25 values on a log scale, since values are small\nmy_norm = \"log\" \n# ***this will number our x and y axes based on the longitude latitude range***\nmy_extent = [min_lon, max_lon, min_lat, max_lat]\n# ensure the aspect ratio of our plot fits all data, matplotlib can does this automatically\nmy_aspect = 'auto'\n# tell matplotlib, our origin is the lower-left corner\nmy_origin = 'lower'\n# select a colormap for our plot and the color bar on the right\nmy_cmap = 'Oranges'\n\n# create our plot using imshow\nplot = plt.imshow(data_array_at_latlon.values, norm=my_norm, extent=my_extent, \n          aspect=my_aspect, origin=my_origin, cmap=my_cmap)\n\n# draw coastlines\nmy_plt.coastlines()\n\n# draw latitude longitude lines\n# ref: https://scitools.org.uk/cartopy/docs/latest/gallery/gridlines_and_labels/gridliner.html\nmy_plt.gridlines(draw_labels=True)\n\n# add a colorbar to our figure, based on the plot we just made above\nmy_fig.colorbar(plot,location='right', label='ug/m^3')\n\n# Set x and y axis labels on our ax\nmy_plt.set_xlabel('Longitude')\nmy_plt.set_ylabel('Latitude')\n\n# Set title of our figure\nmy_fig.suptitle('Ground level concentration of PM2.5 microns and smaller')\n\n# Set title of our plot as the timestamp of our data\nmy_plt.set_title(f'{my_timestamp}')\n\n# Show the resulting visualization\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndata_array_at_latlon.plot(vmin=0, vmax=30)\n\n\n\n\n\n\n\n\n\n\n1.7.1 Please reach out to Arleth Salinas or Valerio Pascucci for any concerns about the notebook. Thank you!\n\nArleth Salinas (arleth.salinas@utah.edu)\nValerio Pascucci (pascucci.valerio@gmail.com)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>IDX File Demo</span>"
    ]
  },
  {
    "objectID": "sys_specs.html",
    "href": "sys_specs.html",
    "title": "3  System and Environment",
    "section": "",
    "text": "3.1 Machine Specification\nThe data we curate is over 300 gigabytes large. Therefore we used the SCI institute’s in-house machine ‘atlantis’ for data staging and processing. See Table 3.1.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>System and Environment</span>"
    ]
  },
  {
    "objectID": "sys_specs.html#machine-specification",
    "href": "sys_specs.html#machine-specification",
    "title": "3  System and Environment",
    "section": "",
    "text": "Table 3.1: ‘atlantis’ System Specifications\n\n\n\n\n\nProperty\nValue\n\n\n\n\nArchitecture\nx86_64\n\n\nCPU op-mode(s)\n32-bit, 64-bit\n\n\nByte Order\nLittle Endian\n\n\nAddress sizes\n44 bits physical, 48 bits virtual\n\n\nCPU(s)\n48\n\n\nOn-line CPU(s) list\n0-47\n\n\nThread(s) per core\n2\n\n\nCore(s) per socket\n6\n\n\nSocket(s)\n4",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>System and Environment</span>"
    ]
  },
  {
    "objectID": "sys_specs.html#environment",
    "href": "sys_specs.html#environment",
    "title": "3  System and Environment",
    "section": "3.2 Environment",
    "text": "3.2 Environment\nWe aim to work in an environment that can be most easily reproduced and documented. Therefore we used Python 3.9.19 via conda. We did all our work within the Project Jupyter environment.\nTo find our the yml file containing our exported conda environment please see the sidebar.\nTo work on the ‘atlantis’ machine, we used SSH to connect to the machine.\nIn the proceeding chapters we will specify which tools and libraries were used and why.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>System and Environment</span>"
    ]
  },
  {
    "objectID": "data_source.html",
    "href": "data_source.html",
    "title": "4  The Data Source",
    "section": "",
    "text": "4.1 Overview\nThe Weather Forecast Research Team at the University of British Columbia (UBC) generates a short term dataset of PM2.5 smoke particulate presence in North America. Over the past 3 years, each day four times a day, UBC creates 2-day forecasts of PM2.5 smoke particulate on the ground for Canada and the continental United States. This is done using their The BlueSky Western Canada Wildfire Smoke Forecasting System. UBC provides access to these predictions for free on a daily basis at their website firesmoke.ca.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Data Source</span>"
    ]
  },
  {
    "objectID": "data_source.html#downloading-ubc-smoke-forecast-files",
    "href": "data_source.html#downloading-ubc-smoke-forecast-files",
    "title": "4  The Data Source",
    "section": "4.2 Downloading UBC Smoke Forecast Files",
    "text": "4.2 Downloading UBC Smoke Forecast Files\n\n4.2.1 Available Time Range\nTODO CREDIT UBC The time ranges of available data for each forecast ID is shown in Table 4.1. Please note, there are occassional failed forecasts or otherwise unavailable files within the date ranges specified Table 4.1.\n\n\n\nTable 4.1: Dates for which all Forecast ID Datasets are Publicly Available. All times are in UTC and the grid size is 12 km.\n\n\n\n\n\n\n\n\n\n\n\n\nForecast ID\nMeteorology Forecast Initialization (UTC)\nSmoke Forecast Initialization (UTC)\nStart Date\nEnd Date\n\n\n\n\nBSC00CA12-01\n00Z\n08Z\nMarch 4, 2021\nPresent Day\n\n\nBSC06CA12-01\n06Z\n14Z\nMarch 4, 2021\nPresent Day\n\n\nBSC12CA12-01\n12Z\n20Z\nMarch 3, 2021\nPresent Day\n\n\nBSC18CA12-01\n18Z\n02Z\nMarch 4, 2021\nPresent Day\n\n\n\n\n\n\nTODO what do grid sizes mean, i.e. Meteorology: 00 UTC, nested 12 km + 36 km grids Smoke: 08 UTC\nThe smoke forecasts are updated daily, including the present day, so there is no fixed end date. Therefore, the latest data must be downloaded on a regular basis. We have not implemented this process yet, so the latest forecast files we use are up to June 27, 2024.\nThere is no official source stating the earliest available date for each forecast. So, knowing the project began in 2021, we inferenced that the earliest available date would be in 2021. Via trial and error we found the earliest available dates.\n\n\n4.2.2 Download Instructions\nTo download the 2-day forecast for the forecast initialization date of one’s choice, one follows the instructions below. The downloaded file can be a NetCDF or KMZ file. We use NetCDF files. Later in Chapter 6 we detail our automation of the download process described below.\n\nTODO CREDIT UBC\nGo to the URL: https://firesmoke.ca/forecasts/{Forecast ID}/{YYYYMMDD}{InitTime}/{File Type}\nWhere:\n\nYYYYMMDD is the date of choice.\nForecastID and InitTime are the chosen values as described in Table 4.2.\nFile Type is either dispersion.nc or dispersion.kmz for either the NetCDF file or KMZ file, respectively.\n\n\n\n\nTable 4.2: UBC Smoke Forecast Data Download Parameters. All times are in UTC and the grid size is 12 km.\n\n\n\n\n\nForecast ID\nSmoke Forecast Initialization (UTC)\n\n\n\n\nBSC00CA12-01\n08Z\n\n\nBSC06CA12-01\n14Z\n\n\nBSC12CA12-01\n20Z\n\n\nBSC18CA12-01\n02Z\n\n\n\n\n\n\n\n4.2.2.1 Download Example\nLet’s try downloading the forecast for January 1, 2024 where the weather forecast is initiated at 00:00:00 UTC and the smoke forecast is initialized at 08:00:00 UTC by navigating to the corresponding URL.\n\n\nCode\nforecast_id = \"BSC00CA12-01\"\nyyyymmdd = \"20210304\"\ninit_time = \"08\"\n\nurl = (\n    f\"https://firesmoke.ca/forecasts/{forecast_id}/{yyyymmdd}{init_time}/dispersion.nc\"\n)\n\nprint(f\"Navigate to this URL in your browser: {url}\")\n\n\nNavigate to this URL in your browser: https://firesmoke.ca/forecasts/BSC00CA12-01/2021030408/dispersion.nc",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Data Source</span>"
    ]
  },
  {
    "objectID": "data_source.html#the-netcdf-file",
    "href": "data_source.html#the-netcdf-file",
    "title": "4  The Data Source",
    "section": "4.3 The NetCDF File",
    "text": "4.3 The NetCDF File\nNext, let’s look at what is within the NetCDF file located at the URL in our previous example.\n\n4.3.1 File Preview\n\nWe load dispersion.nc using xarray, which provides a handy preview of the file.\n\n\nCode\nimport xarray as xr\n\nds = xr.open_dataset(\"data_notebooks/data_source/dispersion.nc\")\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (TSTEP: 51, VAR: 1, DATE-TIME: 2, LAY: 1, ROW: 381, COL: 1041)\nDimensions without coordinates: TSTEP, VAR, DATE-TIME, LAY, ROW, COL\nData variables:\n    TFLAG    (TSTEP, VAR, DATE-TIME) int32 ...\n    PM25     (TSTEP, LAY, ROW, COL) float32 ...\nAttributes: (12/33)\n    IOAPI_VERSION:  $Id: @(#) ioapi library version 3.0 $                    ...\n    EXEC_ID:        ????????????????                                         ...\n    FTYPE:          1\n    CDATE:          2021063\n    CTIME:          101914\n    WDATE:          2021063\n    ...             ...\n    VGLVLS:         [10.  0.]\n    GDNAM:          HYSPLIT CONC    \n    UPNAM:          hysplit2netCDF  \n    VAR-LIST:       PM25            \n    FILEDESC:       Hysplit Concentration Model Output                       ...\n    HISTORY:        xarray.DatasetDimensions:TSTEP: 51VAR: 1DATE-TIME: 2LAY: 1ROW: 381COL: 1041Coordinates: (0)Data variables: (2)TFLAG(TSTEP, VAR, DATE-TIME)int32...units :&lt;YYYYDDD,HHMMSS&gt;long_name :TFLAG           var_desc :Timestep-valid flags:  (1) YYYYDDD or (2) HHMMSS                                [102 values with dtype=int32]PM25(TSTEP, LAY, ROW, COL)float32...long_name :PM25            units :ug/m^3          var_desc :PM25                                                                            [20227671 values with dtype=float32]Indexes: (0)Attributes: (33)IOAPI_VERSION :$Id: @(#) ioapi library version 3.0 $                                           EXEC_ID :????????????????                                                                FTYPE :1CDATE :2021063CTIME :101914WDATE :2021063WTIME :101914SDATE :2021063STIME :90000TSTEP :10000NTHIK :1NCOLS :1041NROWS :381NLAYS :1NVARS :1GDTYP :1P_ALP :0.0P_BET :0.0P_GAM :0.0XCENT :-104.0YCENT :51.0XORIG :-156.0YORIG :32.0XCELL :0.10000000149011612YCELL :0.10000000149011612VGTYP :5VGTOP :-9999.0VGLVLS :[10.  0.]GDNAM :HYSPLIT CONC    UPNAM :hysplit2netCDF  VAR-LIST :PM25            FILEDESC :Hysplit Concentration Model Output                                              lat-lon coordinate system                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       HISTORY :\n\n\n\n\n4.3.2 File Attributes\ndispersion.nc contains the following attributes. Note that for all files across forecast IDs, they have the same dimension and variable names:\n\n4.3.2.1 Dimensions:\nThe dimensions described in Table 4.3 determine on which indicies we may index our variables.\n\n\n\nTable 4.3: Description of Dimensions for Indexing Data in NetCDF Files\n\n\n\n\n\n\n\n\n\n\nDimension\nSize\nDescription\n\n\n\n\nTSTEP\n51\nThis dimension represents the number of time steps in the file. Each file has 51 hours represented.\n\n\nVAR\n1\nThis dimension is a placeholder for the variables in the file.\n\n\nDATE-TIME\n2\nThis dimension stores the date and time information for each time step.\n\n\nLAY\n1\nThis dimension represents the number of layers in the file, which is 1 in this case.\n\n\nROW\n381\nThis dimension represents the number of rows in the spatial grid.\n\n\nCOL\n1041\nThis dimension represents the number of columns in the spatial grid.\n\n\n\n\n\n\n\n\n4.3.2.2 Variables:\nThe variables described in Table 4.4 contain the data in question that we would like to extract.\n\n\n\nTable 4.4: Description of Variables in NetCDF Files\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDimensions\nData Type\nDescription\n\n\n\n\nTFLAG\nTSTEP, VAR, DATE-TIME\nint32\nThis variable stores the date and time of each time step.\n\n\nPM25\nTSTEP, LAY, ROW, COL\nfloat32\nThis variable contains the concentration of particulate matter (PM2.5) for each time step, layer, row, and column in the spatial grid.\n\n\n\n\n\n\n\n\n4.3.2.3 Attributes\nOf the 33 available attributes we use the ones shown in Table 4.5: TODO NEED TO CONFIRM IF THIS IS RIGHT\n\n\n\nTable 4.5: Description of Attributes in NetCDF Files\n\n\n\n\n\n\n\n\n\n\nAttribute\nValue\nDescription\n\n\n\n\nCDATE\n2021063\nThe creation date of the dataset, in YYYYDDD format.\n\n\nCTIME\n101914\nThe creation time of the dataset, in HHMMSS format.\n\n\nWDATE\n2021063\nThe date for which the weather forecast is initiated, in YYYYDDD format.\n\n\nWTIME\n101914\nThe time for which the weather forecast is initiated, in HHMMSS format.\n\n\nSDATE\n2021063\nThe date for which the smoke forecast is initiated,in YYYYDDD format.\n\n\nSTIME\n90000\nThe time for which the weather forecast is initiated, in HHMMSS format.\n\n\nNCOLS\n1041\nThe number of columns in the spatial grid.\n\n\nNROWS\n381\nThe number of rows in the spatial grid.\n\n\nXORIG\n-156.0\nThe origin (starting point) of the grid in the x-direction.\n\n\nYORIG\n32.0\nThe origin (starting point) of the grid in the y-direction.\n\n\nXCELL\n0.10000000149011612\nThe cell size in the x-direction.\n\n\nYCELL\n0.10000000149011612\nThe cell size in the y-direction.\n\n\n\n\n\n\nNow that we understand what exactly is within one NetCDF file, let’s establish the information we have when we download all available NetCDF files.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Data Source</span>"
    ]
  },
  {
    "objectID": "data_source.html#metadata-of-collected-netcdf-files",
    "href": "data_source.html#metadata-of-collected-netcdf-files",
    "title": "4  The Data Source",
    "section": "4.4 Metadata of Collected NetCDF Files",
    "text": "4.4 Metadata of Collected NetCDF Files\nHere we describe metadata about the collection of NetCDF files we downloaded.\n\n4.4.1 Disk Size\nFor the time ranges we cover, Table 7.1 shows how large the set of files per forecast ID are.\n\n\n\nTable 4.6: File Sizes and Counts for Each Forecast ID within the Specified Date Range\n\n\n\n\n\n\n\n\n\n\n\nForecast ID\nDate Range\nSize\nFile Count\n\n\n\n\nBSC00CA12-01\nMarch 4, 2021 - June 27, 2024\n84G\n1077\n\n\nBSC06CA12-01\nMarch 4, 2021 - June 27, 2024\n78G\n1022\n\n\nBSC12CA12-01\nMarch 3, 2021 - June 27, 2024\n79G\n1022\n\n\nBSC18CA12-01\nMarch 4, 2021 - June 27, 2024\n79G\n1023\n\n\nTotal\n\n320G\n4144\n\n\n\n\n\n\n\n\n4.4.2 Data Uniformity\nTo create our dataset, we used the following understanding about the attributes across all files:\n\nAll files across all four forecast IDs have the same NROWS, XORIG, YORIG, XCELL, YCELL values.\nSome files have either NCOLS = 1041 or NCOLS = 1081.\n{C,W,S}_DATE and {C,W,S}_TIME values are unique to each file.\n\nNow we know what data is available and how to download and view it, we show a demo of how to visualize the data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Data Source</span>"
    ]
  },
  {
    "objectID": "netcdf_demo.html",
    "href": "netcdf_demo.html",
    "title": "5  NetCDF Visualization Demo",
    "section": "",
    "text": "5.1 NetCDF Demo\nIn this demo we load one dispersion.nc file and explore how to visualize the data within the file.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>NetCDF Visualization Demo</span>"
    ]
  },
  {
    "objectID": "netcdf_demo.html#accessing-the-file",
    "href": "netcdf_demo.html#accessing-the-file",
    "title": "5  NetCDF Visualization Demo",
    "section": "6.1 Accessing the File",
    "text": "6.1 Accessing the File\nWe use the forecast for March 4, 2021 where the weather forecast is initiated at 00:00:00 UTC and the smoke forecast is initialized at 08:00:00 UTC. You can download this file by navigating to the URL below.\n\n\nCode\nforecast_id = \"BSC00CA12-01\"\nyyyymmdd = \"20210304\"\ninit_time = \"08\"\n\nurl = (\n    f\"https://firesmoke.ca/forecasts/{forecast_id}/{yyyymmdd}{init_time}/dispersion.nc\"\n)\n\nprint(f\"Navigate to this URL in your browser: {url}\")\n\n\nNavigate to this URL in your browser: https://firesmoke.ca/forecasts/BSC00CA12-01/2021030408/dispersion.nc\n\n\n\n6.1.1 Opening the File\nWe use xarray to open the NetCDF file and preview it.\n\n\nCode\nimport xarray as xr\n\nds = xr.open_dataset(\"dispersion.nc\")\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (TSTEP: 51, VAR: 1, DATE-TIME: 2, LAY: 1, ROW: 381, COL: 1041)\nDimensions without coordinates: TSTEP, VAR, DATE-TIME, LAY, ROW, COL\nData variables:\n    TFLAG    (TSTEP, VAR, DATE-TIME) int32 ...\n    PM25     (TSTEP, LAY, ROW, COL) float32 ...\nAttributes: (12/33)\n    IOAPI_VERSION:  $Id: @(#) ioapi library version 3.0 $                    ...\n    EXEC_ID:        ????????????????                                         ...\n    FTYPE:          1\n    CDATE:          2021063\n    CTIME:          101914\n    WDATE:          2021063\n    ...             ...\n    VGLVLS:         [10.  0.]\n    GDNAM:          HYSPLIT CONC    \n    UPNAM:          hysplit2netCDF  \n    VAR-LIST:       PM25            \n    FILEDESC:       Hysplit Concentration Model Output                       ...\n    HISTORY:        xarray.DatasetDimensions:TSTEP: 51VAR: 1DATE-TIME: 2LAY: 1ROW: 381COL: 1041Coordinates: (0)Data variables: (2)TFLAG(TSTEP, VAR, DATE-TIME)int32...units :&lt;YYYYDDD,HHMMSS&gt;long_name :TFLAG           var_desc :Timestep-valid flags:  (1) YYYYDDD or (2) HHMMSS                                [102 values with dtype=int32]PM25(TSTEP, LAY, ROW, COL)float32...long_name :PM25            units :ug/m^3          var_desc :PM25                                                                            [20227671 values with dtype=float32]Indexes: (0)Attributes: (33)IOAPI_VERSION :$Id: @(#) ioapi library version 3.0 $                                           EXEC_ID :????????????????                                                                FTYPE :1CDATE :2021063CTIME :101914WDATE :2021063WTIME :101914SDATE :2021063STIME :90000TSTEP :10000NTHIK :1NCOLS :1041NROWS :381NLAYS :1NVARS :1GDTYP :1P_ALP :0.0P_BET :0.0P_GAM :0.0XCENT :-104.0YCENT :51.0XORIG :-156.0YORIG :32.0XCELL :0.10000000149011612YCELL :0.10000000149011612VGTYP :5VGTOP :-9999.0VGLVLS :[10.  0.]GDNAM :HYSPLIT CONC    UPNAM :hysplit2netCDF  VAR-LIST :PM25            FILEDESC :Hysplit Concentration Model Output                                              lat-lon coordinate system                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       HISTORY :",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>NetCDF Visualization Demo</span>"
    ]
  },
  {
    "objectID": "netcdf_demo.html#using-the-data",
    "href": "netcdf_demo.html#using-the-data",
    "title": "5  NetCDF Visualization Demo",
    "section": "6.2 Using the Data",
    "text": "6.2 Using the Data\n\n6.2.1 Accessing Arrays\nThe data we are interested in is the PM2.5 values. Let’s use xarray to preview the PM25 variable in our file.\n\n\nCode\nds[\"PM25\"]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'PM25' (TSTEP: 51, LAY: 1, ROW: 381, COL: 1041)&gt;\n[20227671 values with dtype=float32]\nDimensions without coordinates: TSTEP, LAY, ROW, COL\nAttributes:\n    long_name:  PM25            \n    units:      ug/m^3          \n    var_desc:   PM25                                                         ...xarray.DataArray'PM25'TSTEP: 51LAY: 1ROW: 381COL: 1041...[20227671 values with dtype=float32]Coordinates: (0)Indexes: (0)Attributes: (3)long_name :PM25            units :ug/m^3          var_desc :PM25                                                                            \n\n\nThe dimensions of the PM25 data array are composed of TSTEP, LAY, ROW, and COL. We do not need the LAY dimension, so let’s use numpy to remove it.\n\n\nCode\nimport numpy as np\n\n# use .values to get the four dimensional array.\nds_pm25_vals = ds[\"PM25\"].values\nprint(f'The shape of the data contained in our files is: {np.shape(ds_pm25_vals)}')\n\n# let's use np.squeeze to drop the LAY axis\nds_pm25_vals = np.squeeze(ds_pm25_vals)\nprint(f'After squeezing, the shape is: {np.shape(ds_pm25_vals)}')\n\n\nThe shape of the data contained in our files is: (51, 1, 381, 1041)\nAfter squeezing, the shape is: (51, 381, 1041)\n\n\n\n\n6.2.2 Visualize Array in matplotlib\nNow that we squeezed away the LAY dimension, we can index time step 15 and use matplotlib to visualize the timestep\n\n\nCode\nimport matplotlib.pyplot as plt\n\ntstep = 15\nsmoke_at_tstep = ds_pm25_vals[tstep, :, :]\n\nmy_fig, my_plt = plt.subplots(figsize=(15, 6))\n\n# color PM25 values on a log scale, since values are small\nmy_norm = \"log\" \n# ensure the aspect ratio of our plot fits all data, matplotlib can does this automatically\nmy_aspect = 'auto'\n# tell matplotlib, our origin is the lower-left corner\nmy_origin = 'lower'\n# select a colormap for our plot and the color bar on the right\nmy_cmap = 'viridis'\n \n# create our plot using imshow\nplot = my_plt.imshow(smoke_at_tstep, norm=my_norm,aspect=my_aspect, origin=my_origin, cmap=my_cmap)\n\n# add a colorbar to our figure, based on the plot we just made above\nmy_fig.colorbar(plot, location='right', label='ug/m^3')\n\n# Set title of our figure\nmy_fig.suptitle('Ground level concentration of PM2.5 microns and smaller')\n\n# Set title of our plot as the timestamp of our data\nmy_plt.set_title(f'Timestep {tstep}')\n\n# Show the resulting visualization\nplt.show()\n\n\n\n\n\n\n\n\n\nNotice there are no axis labels or metadata presented here. Next we will show how to use the metadata in dispersion.nc so the data is actually interpretable.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>NetCDF Visualization Demo</span>"
    ]
  },
  {
    "objectID": "netcdf_demo.html#incorporating-metadata-to-visualization-via-coordinates",
    "href": "netcdf_demo.html#incorporating-metadata-to-visualization-via-coordinates",
    "title": "5  NetCDF Visualization Demo",
    "section": "6.3 Incorporating Metadata to Visualization via Coordinates",
    "text": "6.3 Incorporating Metadata to Visualization via Coordinates\n\nLatitude and Longitude Coordinates\ndispersion.nc includes attributes to generate the latitude and longitude values on the grid defined by NCOLS and NROWS. We use this grid to match each data point in the PM25 variable to a lat/lon coordinate.\n\n\nCode\nxorig = ds.XORIG\nyorig = ds.YORIG\nxcell = ds.XCELL\nycell = ds.YCELL\nncols = ds.NCOLS\nnrows = ds.NROWS\n\nlongitude = np.linspace(xorig, xorig + xcell * (ncols - 1), ncols)\nlatitude = np.linspace(yorig, yorig + ycell * (nrows - 1), nrows)\n\nprint(\"Size of longitude & latitude arrays:\")\nprint(f'np.size(longitude) = {np.size(longitude)}')\nprint(f'np.size(latitude) = {np.size(latitude)}\\n')\nprint(\"Min & Max of longitude and latitude arrays:\")\nprint(f'longitude: min = {np.min(longitude)}, max = {np.max(longitude)}')\nprint(f'latitude: min = {np.min(latitude)}, max = {np.max(latitude)}')\n\n\nSize of longitude & latitude arrays:\nnp.size(longitude) = 1041\nnp.size(latitude) = 381\n\nMin & Max of longitude and latitude arrays:\nlongitude: min = -156.0, max = -51.999998450279236\nlatitude: min = 32.0, max = 70.00000056624413\n\n\nxarray allows us to create coordinates, which maps variable values to a value of our choice. In this case, we create coordinates mapping PM25 values to a latitude and longitude value.\n\n\nCode\n# Create coordinates for lat and lon\nds.coords['lat'] = ('ROW', latitude)\nds.coords['lon'] = ('COL', longitude)\n\n# Replace col and row dimensions with newly calculated lon and lat coordinates\nds = ds.swap_dims({'COL': 'lon', 'ROW': 'lat'})\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (TSTEP: 51, VAR: 1, DATE-TIME: 2, LAY: 1, lat: 381, lon: 1041)\nCoordinates:\n  * lat      (lat) float64 32.0 32.1 32.2 32.3 32.4 ... 69.6 69.7 69.8 69.9 70.0\n  * lon      (lon) float64 -156.0 -155.9 -155.8 -155.7 ... -52.2 -52.1 -52.0\nDimensions without coordinates: TSTEP, VAR, DATE-TIME, LAY\nData variables:\n    TFLAG    (TSTEP, VAR, DATE-TIME) int32 ...\n    PM25     (TSTEP, LAY, lat, lon) float32 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0\nAttributes: (12/33)\n    IOAPI_VERSION:  $Id: @(#) ioapi library version 3.0 $                    ...\n    EXEC_ID:        ????????????????                                         ...\n    FTYPE:          1\n    CDATE:          2021063\n    CTIME:          101914\n    WDATE:          2021063\n    ...             ...\n    VGLVLS:         [10.  0.]\n    GDNAM:          HYSPLIT CONC    \n    UPNAM:          hysplit2netCDF  \n    VAR-LIST:       PM25            \n    FILEDESC:       Hysplit Concentration Model Output                       ...\n    HISTORY:        xarray.DatasetDimensions:TSTEP: 51VAR: 1DATE-TIME: 2LAY: 1lat: 381lon: 1041Coordinates: (2)lat(lat)float6432.0 32.1 32.2 ... 69.8 69.9 70.0array([32.      , 32.1     , 32.2     , ..., 69.800001, 69.900001, 70.000001])lon(lon)float64-156.0 -155.9 ... -52.1 -52.0array([-156.      , -155.9     , -155.8     , ...,  -52.199998,  -52.099998,\n        -51.999998])Data variables: (2)TFLAG(TSTEP, VAR, DATE-TIME)int32...units :&lt;YYYYDDD,HHMMSS&gt;long_name :TFLAG           var_desc :Timestep-valid flags:  (1) YYYYDDD or (2) HHMMSS                                [102 values with dtype=int32]PM25(TSTEP, LAY, lat, lon)float320.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0long_name :PM25            units :ug/m^3          var_desc :PM25                                                                            array([[[[0.000000e+00, ..., 0.000000e+00],\n         ...,\n         [0.000000e+00, ..., 0.000000e+00]]],\n\n\n       ...,\n\n\n       [[[0.000000e+00, ..., 2.804227e-11],\n         ...,\n         [0.000000e+00, ..., 0.000000e+00]]]], dtype=float32)Indexes: (2)latPandasIndexPandasIndex(Index([              32.0, 32.100000001490116,  32.20000000298023,\n        32.30000000447035, 32.400000005960464,  32.50000000745058,\n         32.6000000089407,  32.70000001043081,  32.80000001192093,\n       32.900000013411045,\n       ...\n        69.10000055283308,   69.2000005543232,  69.30000055581331,\n        69.40000055730343,  69.50000055879354,  69.60000056028366,\n        69.70000056177378,   69.8000005632639,  69.90000056475401,\n        70.00000056624413],\n      dtype='float64', name='lat', length=381))lonPandasIndexPandasIndex(Index([             -156.0, -155.89999999850988, -155.79999999701977,\n       -155.69999999552965, -155.59999999403954, -155.49999999254942,\n        -155.3999999910593,  -155.2999999895692, -155.19999998807907,\n       -155.09999998658895,\n       ...\n        -52.89999846369028, -52.799998462200165,  -52.69999846071005,\n        -52.59999845921993, -52.499998457729816,   -52.3999984562397,\n       -52.299998454749584,  -52.19999845325947,  -52.09999845176935,\n       -51.999998450279236],\n      dtype='float64', name='lon', length=1041))Attributes: (33)IOAPI_VERSION :$Id: @(#) ioapi library version 3.0 $                                           EXEC_ID :????????????????                                                                FTYPE :1CDATE :2021063CTIME :101914WDATE :2021063WTIME :101914SDATE :2021063STIME :90000TSTEP :10000NTHIK :1NCOLS :1041NROWS :381NLAYS :1NVARS :1GDTYP :1P_ALP :0.0P_BET :0.0P_GAM :0.0XCENT :-104.0YCENT :51.0XORIG :-156.0YORIG :32.0XCELL :0.10000000149011612YCELL :0.10000000149011612VGTYP :5VGTOP :-9999.0VGLVLS :[10.  0.]GDNAM :HYSPLIT CONC    UPNAM :hysplit2netCDF  VAR-LIST :PM25            FILEDESC :Hysplit Concentration Model Output                                              lat-lon coordinate system                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       HISTORY :\n\n\nNow let’s move on to incorporating time stamp metadata.\n\n\nTime Coordinates\nRecall, there is a TFLAG variable in dispersion.nc.\n\n\nCode\nds['TFLAG']\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'TFLAG' (TSTEP: 51, VAR: 1, DATE-TIME: 2)&gt;\n[102 values with dtype=int32]\nDimensions without coordinates: TSTEP, VAR, DATE-TIME\nAttributes:\n    units:      &lt;YYYYDDD,HHMMSS&gt;\n    long_name:  TFLAG           \n    var_desc:   Timestep-valid flags:  (1) YYYYDDD or (2) HHMMSS             ...xarray.DataArray'TFLAG'TSTEP: 51VAR: 1DATE-TIME: 2...[102 values with dtype=int32]Coordinates: (0)Indexes: (0)Attributes: (3)units :&lt;YYYYDDD,HHMMSS&gt;long_name :TFLAG           var_desc :Timestep-valid flags:  (1) YYYYDDD or (2) HHMMSS                                \n\n\nThe first TFLAG looks like the following:\n\n\nCode\nds['TFLAG'].values[0][0]\n\n\narray([2021063,   90000], dtype=int32)\n\n\nThis time flag requires processing to be immediately legible. Let’s write a function to process the time flag accordingly. We use the datetime library.\n\n\nCode\nimport datetime\n\ndef parse_tflag(tflag):\n    \"\"\"\n    Return the tflag as a datetime object\n    :param list tflag: a list of two int32, the 1st representing date and 2nd representing time\n    \"\"\"\n    # obtain year and day of year from tflag[0] (date)\n    date = int(tflag[0])\n    year = date // 1000 # first 4 digits of tflag[0]\n    day_of_year = date % 1000 # last 3 digits of tflag[0]\n\n    # create datetime object representing date\n    final_date = datetime.datetime(year, 1, 1) + datetime.timedelta(days=day_of_year - 1)\n\n    # obtain hour, mins, and secs from tflag[1] (time)\n    time = int(tflag[1])\n    hours = time // 10000 # first 2 digits of tflag[1]\n    minutes = (time % 10000) // 100 # 3rd and 4th digits of tflag[1] \n    seconds = time % 100  # last 2 digits of tflag[1]\n\n    # create final datetime object\n    full_datetime = datetime.datetime(year, final_date.month, final_date.day, hours, minutes, seconds)\n    return full_datetime\n\n\nNow we have a datetime object to represent the timeflag in a more legible and usable format.\n\n\nCode\nprint(parse_tflag(ds['TFLAG'].values[0][0]))\n\n\n2021-03-04 09:00:00\n\n\n\n\n6.3.1 Visualize Array in matplotlib\nLet’s visualize timestep 15 again, but now we can label the data using latitudes and longitudes, and the corresponding time flag.\n\n\nCode\ntstep = 15\nsmoke_at_tstep = ds_pm25_vals[tstep, :, :]\ntstep_tflag = parse_tflag(ds['TFLAG'].values[tstep][0])\n\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\n\n# Initialize a figure and plot, so we can customize figure and plot of data\nmy_fig, my_plt = plt.subplots(figsize=(15, 6), subplot_kw=dict(projection=ccrs.PlateCarree()))\n\n# color PM25 values on a log scale, since values are small\nmy_norm = \"log\" \n# this will number our x and y axes based on the longitude latitude range\nmy_extent = [np.min(longitude), np.max(longitude), np.min(latitude), np.max(latitude)]\n# ensure the aspect ratio of our plot fits all data, matplotlib can does this automatically\nmy_aspect = 'auto'\n# tell matplotlib, our origin is the lower-left corner\nmy_origin = 'lower'\n# select a colormap for our plot and the color bar on the right\nmy_cmap = 'viridis'\n\n# create our plot using imshow\nplot = my_plt.imshow(smoke_at_tstep, norm=my_norm, extent=my_extent, \n          aspect=my_aspect, origin=my_origin, cmap=my_cmap)\n\n# draw coastlines\nmy_plt.coastlines()\n\n# draw latitude longitude lines\nmy_plt.gridlines(draw_labels=True)\n\n# add a colorbar to our figure, based on the plot we just made above\nmy_fig.colorbar(plot, location='right', label='ug/m^3')\n\n# Set x and y axis labels on our ax\nmy_fig.supxlabel('Longitude')\nmy_fig.supylabel('Latitude')\n\n# Set title of our figure\nmy_fig.suptitle('Ground level concentration of PM2.5 microns and smaller')\n\n# Set title of our plot as the timestamp of our data\nmy_plt.set_title(f'{tstep_tflag}')\n\n# Show the resulting visualization\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>NetCDF Visualization Demo</span>"
    ]
  },
  {
    "objectID": "data_loading.html",
    "href": "data_loading.html",
    "title": "6  Data Loading",
    "section": "",
    "text": "6.1 Downloading Data Locally\nWe decided to download all the available files from the data source onto our data staging machine to process from there.\nWe created 4 directories for each forecast ID that UBC provides at the following directory on our machine:\nThe following shows our approaches to doing this and discusses how our approach evolved. Note the scripts below refer to varying directories, but through simple copying operations we stored the final downloaded files to the directories listed above.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Loading</span>"
    ]
  },
  {
    "objectID": "data_loading.html#downloading-data-locally",
    "href": "data_loading.html#downloading-data-locally",
    "title": "6  Data Loading",
    "section": "",
    "text": "/usr/sci/cedmav/data/firesmoke\n├── BSC00CA12-01\n├── BSC06CA12-01\n├── BSC12CA12-01\n├── BSC18CA12-01\n\n\n6.1.1 First Approach\nWe delineate our first approach by detailing our download script, which is available in it’s entirety in the side bar.\n\n\n\nCode\nimport wget\nimport pandas as pd\n\n1ids = [\"BSC18CA12-01\", \"BSC00CA12-01\", \"BSC06CA12-01\", \"BSC12CA12-01\"]\nstart_dates = [\"20210304\", \"20210304\", \"20210304\", \"20210303\"]\nend_dates = [\"20231016\", \"20240210\", \"20231016\", \"20231015\"]\ninit_times = [\"02\", \"08\", \"14\", \"20\"]\n\n2for i in zip(start_dates, end_dates, ids, init_times):\n    start_date = i[0]\n    end_date = i[1]\n    forecast_id = i[2]\n    init_time = i[3]\n\n    dates = pd.date_range(start=start_date, end=end_date)\n    dates = dates.strftime(\"%Y%m%d\").tolist()\n\n3    for date in dates:\n      url = (\n          \"https://firesmoke.ca/forecasts/\"\n          + forecast_id\n          + \"/\"\n          + date\n          + init_time\n          + \"/dispersion.nc\"\n      )\n4      directory = \"/Users/arleth/Mount/firesmoke/\" + forecast_id + \"/dispersion_\" + date + \".nc\"\n      wget.download(url, out=directory)\n\n\n\n1\n\nFirst, create 4 lists containing forecast IDs, the start and end dates we wish to index on, and the smoke forecast initiation times. We will loop through the 4 sets of parameters.\n\n2\n\nIn a for loop, we use pandas to create a list of every date from the start date and end date of the current iteration. We will loop through these dates next.\n\n3\n\nFor each date in the list, we create the url to download the file.\n\n4\n\nFinally, we use wget to download the contents at urlto directory. We append date to the file name so each file downloaded is identifiable by date.\n\n\n\n\nWe assumed that for all URLs, there was an available NetCDF file for download\nHowever, we realized that we downloaded either a NetCDF file or an HTML webpage. Using wget forcibly saved the contents at the URL into a NetCDF file.\nThis issue was not identified until after we visualized each hour of the data, and we noticed gaps and errors in our scripts to create visualizations. See Chapter 8 for further details on identifying these issues. For now, we show our modified approach to downloading the NetCDF files.\n\n\n6.1.2 Second Approach\nOur second approach is similar to the first, except we use requests, an HTTP client library that allows us to see the headers returned from the URL we query. The script is available in the side bar.\n\n\n\nCode\nimport requests\nimport pandas as pd\nfrom datetime import datetime\n\n1ids = [\"BSC18CA12-01\", \"BSC00CA12-01\", \"BSC06CA12-01\", \"BSC12CA12-01\"]\nstart_dates = [\"20210304\", \"20210304\", \"20210304\", \"20210303\"]\ntoday = datetime.now().strftime(\"%Y%m%d\")\ninit_times = [\"02\", \"08\", \"14\", \"20\"]\n\n2for i in zip(start_dates, ids, init_times):\n    start_date = i[0]\n    forecast_id = i[1]\n    init_time = i[2]\n\n    dates = pd.date_range(start=start_date, end=today)\n    dates = dates.strftime(\"%Y%m%d\").tolist()\n\n3    for date in dates:\n        url = (\n            \"https://firesmoke.ca/forecasts/\"\n            + forecast_id\n            + \"/\"\n            + date\n            + init_time\n            + \"/dispersion.nc\"\n        )\n4        directory = (\n            \"/usr/sci/scratch_nvme/arleth/basura_total/\"\n            + forecast_id\n            + \"/dispersion_\"\n            + date\n            + \".nc\"\n        )\n\n5        response = requests.get(url, stream=True)\n        header = response.headers\n        if (\n            \"Content-Type\" in header\n            and header[\"Content-Type\"] == \"application/octet-stream\"\n        ):\n6            with open(directory, mode=\"wb\") as file:\n                file.write(response.content)\n                print(f\"Downloaded file {directory}\")\n        else:\n            print(header[\"Content-Type\"])\n\n\n\n1\n\nFirst, create 3 lists containing forecast IDs, the start dates we wish to index on, and the smoke forecast initiation times . Notice we define a variable today, this allows us to run this script and query all URLs up to today’s date. Note we ran up to June 27, 2024 for now. We will loop through these sets of parameters.\n\n2\n\nIn a for loop, we use pandas to create a list of every date from the start date and end date of the current iteration. We will loop through these dates next.\n\n3\n\nFor each date in the list, we create the url to download the file.\n\n4\n\nDefine directory, the directory and file name to save the file to.\n\n5\n\nWe use requests to get the HTTP header at url. We inspect the Content-Type and if it is application/octet-stream, we download the file. We confirmed that a URL with a NetCDF file had this content type header.\n\n6\n\nWe write the content to directory, else we print the content header out to check what it is.\n\n\n\n\nThis approach yielded the results we expected, we downloaded only NetCDF files. We had failed downloads which appeared during conversion as described in Chapter 7. We assumed those files were unavailable from UBC which we later confirmed as described in Chapter 8.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Loading</span>"
    ]
  },
  {
    "objectID": "data_conversion.html",
    "href": "data_conversion.html",
    "title": "7  Data Conversion",
    "section": "",
    "text": "7.1 On Data Validation\nWe decided to perform data validation after conversion to the IDX file format. However, we realized that performing data validation both before and after conversion would be best. This is explored further in Chapter 8.\nFor now, the reader should understand that data validation of the NetCDF files is different from data validation of the IDX file. In this chapter, we use the assumption that the NetCDF files we can open have complete and uncorrupted data for conversion to IDX.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Conversion</span>"
    ]
  },
  {
    "objectID": "data_conversion.html#overview",
    "href": "data_conversion.html#overview",
    "title": "7  Data Conversion",
    "section": "7.2 Overview",
    "text": "7.2 Overview\nFor all our conversion attempts, the same general process is followed:\n\nCheck which NetCDF files were successfully downloaded from the data source by attempting to open each downloaded file with xarray.\nObtain a subset of data from the files to create a dataset of chronological, hour by hour, data.\nSave this time series data to an IDX file using the OpenVisus framework.\n\nWe will describe the latest version of our conversion, version 4. Throughout, we will explain how previous attempts differed. To see previous attemps in their entirety, refer to the side bar. Please note that the previous scripts were working scripts, therefore they may be incomplete.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Conversion</span>"
    ]
  },
  {
    "objectID": "data_conversion.html#setting-system-directories",
    "href": "data_conversion.html#setting-system-directories",
    "title": "7  Data Conversion",
    "section": "7.3 Setting System Directories",
    "text": "7.3 Setting System Directories\nFirst we set the directory paths we want to use during the conversion process, which is to our 4 directories of NetCDF files for each forecast ID.\n\n\nCode\n1firesmoke_dir = \"/usr/sci/cedmav/data/firesmoke\"\nidx_dir = \"/usr/sci/scratch_nvme/arleth/idx/firesmoke\"\n\n2ids = [\"BSC18CA12-01\", \"BSC00CA12-01\", \"BSC06CA12-01\", \"BSC12CA12-01\"]\nstart_dates = [\"20210304\", \"20210304\", \"20210304\", \"20210303\"]\nend_dates = [\"20240627\", \"20240627\", \"20240627\", \"20240627\"]\n\n\n\n1\n\nEstablish the directory where all forecast ID NetCDFs are stored and where to save our IDX file on the ‘atlantis’ machine.\n\n2\n\nDefine the forecast IDs and dates we will loop over.\n\n\n\n\n\n7.3.1 Rationale and Future Improvements\nIn versions 1 and 2 of our conversion attempt, we did not use all four sets of forecast ID files. We only used BSC12CA12-01 files to compile a single dataset. We learned that by not using all four sets of data, the dataset we created was less accurate. See Chapter 8 for further details.\nTherefore we decided to use all four datasets. We elect to use dates up to June 27, 2024 as this was the last time we ran our scripts. We have yet to address the issue of how to keep the IDX file constantly up to date with data available up to the present day.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Conversion</span>"
    ]
  },
  {
    "objectID": "data_conversion.html#checking-the-netcdf-files",
    "href": "data_conversion.html#checking-the-netcdf-files",
    "title": "7  Data Conversion",
    "section": "7.4 Checking the NetCDF Files",
    "text": "7.4 Checking the NetCDF Files\nRecall we downloaded all NetCDF files available from UBC onto our machine in their respective directories as follows:\n/usr/sci/cedmav/data/firesmoke\n├── BSC00CA12-01\n├── BSC06CA12-01\n├── BSC12CA12-01\n├── BSC18CA12-01\nHere, we identify which NetCDF files for each forecast ID successfully open with xarray and store them in a dictionary.\nWe also confirm the following conditions we established in Chapter 4 by using dictionaries to track the max values and unique values of these attributes across all files:\n\nAll files across all four forecast IDs have the same NROWS, XORIG, YORIG, XCELL, YCELL values.\nSome files have either NCOLS = 1041 or NCOLS = 1081, but always NROWS = 381.\n\n\n\nCode\nimport os\nimport xarray as xr\nimport numpy as np\nimport tqdm\n\n1successful_files = {id_: [] for id_ in ids}\n\n2max_ncols = {id_: 0 for id_ in ids}\nmax_nrows = {id_: 0 for id_ in ids}\n3ncols = {id_: set() for id_ in ids}\nnrows = {id_: set() for id_ in ids}\n\n4max_grid_x = {id_: {\"xorig\": 0.0, \"xcell\": 0.0} for id_ in ids}\nmax_grid_y = {id_: {\"yorig\": 0.0, \"ycell\": 0.0} for id_ in ids}\n5xorigs = {id_: set() for id_ in ids}\nxcells = {id_: set() for id_ in ids}\nyorigs = {id_: set() for id_ in ids}\nycells = {id_: set() for id_ in ids}\n\n6for id_ in ids:\n7    file_names = os.listdir(f\"{firesmoke_dir}/{id_}/\")\n\n8    for file in tqdm(file_names):\n9        path = f\"{firesmoke_dir}/{id_}/{file}\"\n\n10        try:\n            ds = xr.open_dataset(path)\n\n11            successful_files[id_].append(file)\n\n12            max_ncols[id_] = max(max_ncols[id_], ds.NCOLS)\n            max_nrows[id_] = max(max_nrows[id_], ds.NROWS)\n            max_grid_x[id_][\"xorig\"] = max(max_grid_x[id_][\"xorig\"], ds.XORIG, key=abs)\n            max_grid_y[id_][\"yorig\"] = max(max_grid_y[id_][\"yorig\"], ds.YORIG, key=abs)\n            max_grid_x[id_][\"xcell\"] = max(max_grid_x[id_][\"xcell\"], ds.XCELL, key=abs)\n            max_grid_y[id_][\"ycell\"] = max(\n                max_grid_y[id_][\"ycell\"], ds.YCELL, key=abs\n            )\n\n13            ncols[id_].add(ds.NCOLS)\n            nrows[id_].add(ds.NROWS)\n            xorigs[id_].add(ds.XORIG)\n            yorigs[id_].add(ds.YORIG)\n            xcells[id_].add(ds.XCELL)\n            ycells[id_].add(ds.YCELL)\n\n14        except:\n            continue\n\n15for id_ in successful_files:\n    successful_files[id_] = np.sort(successful_files[id_]).tolist()\n\n\n\n1\n\nInitialize a dictionary to hold an empty list for each forecast ID. We update it with the file names that successfully open under the forecast ID directory.\n\n2\n\nInitialize dictionaries to hold an integer for each forecast ID. We update it to hold the maximum NCOLS/NROWS value available within forecast ID’s set of NetCDF files.\n\n3\n\nInitialize dictionaries to hold a set for each forecast ID. We update the set to hold all the unique NCOLS/NROWS values available within the forecast ID’s set of NetCDF files.\n\n4\n\nInitialize dictionaries to hold a dictionary of xorig/yorig and xcell/ycell values for each forecast ID. We update it to hold the maximum xorig/yorig and xcell/ycell pairs available within the forecast ID’s set of NetCDF files.\n\n5\n\nInitialize dictionaries to track unique xorig/yorig and xcell/ycell values.\n\n6\n\nFor each forecast ID, we populate the dictionaries above.\n\n7\n\nObtain a list of file names under the directory for id_. We loop through each file next.\n\n8\n\nBegin loop over each file. Note tqdm is just an accessory for generating a visible status bar in our Jupyter Notebook.\n\n9\n\nObtain absolute path name to current file.\n\n10\n\nHere we use a try statement since opening the file with xarray may lead to an error. except allows us to catch the exception accordingly and continue trying to open each file.\n\n11\n\nAt this line, the file opened without issue in xarray, so append this file name to the id_ list in the successful_files dictionary.\n\n12\n\nUse max to save the largest values in our dictionaries accordingly.\n\n13\n\nUpdate the dictionaries of sets with the file’s attributes, to ensure we catch all unique values.\n\n14\n\nIf the file did not open during the try continue to the next file.\n\n15\n\nSort the lists of successfully opened files by name, so they are in chronological order.\n\n\n\n\nThe following shows the information gathered:\n\nBSC18CA12-01BSC00CA12-01BSC06CA12-01BSC12CA12-01\n\n\ndataset: BSC18CA12-01\nNumber of successful files: 1010\nMax cell sizes: max_ncols = 1081 and max_nrows = 381\nMax xorig & xcell: {'xorig': -160.0, 'xcell': 0.10000000149011612}\nMax yorig & ycell: {'yorig': 32.0, 'ycell': 0.10000000149011612}\nncols: {1081, 1041}\nnrows: {381}\nxorigs: {-160.0, -156.0}\nyorigs: {32.0}\nxcells: {0.10000000149011612}\nycells: {0.10000000149011612}\n\n\ndataset: BSC00CA12-01\nNumber of successful files: 1067\nMax cell sizes: max_ncols = 1081 and max_nrows = 381\nMax xorig & xcell: {'xorig': -160.0, 'xcell': 0.10000000149011612}\nMax yorig & ycell: {'yorig': 32.0, 'ycell': 0.10000000149011612}\nncols: {1081, 1041}\nnrows: {381}\nxorigs: {-160.0, -156.0}\nyorigs: {32.0}\nxcells: {0.10000000149011612}\nycells: {0.10000000149011612}\n\n\ndataset: BSC06CA12-01\nNumber of successful files: 997\nMax cell sizes: max_ncols = 1081 and max_nrows = 381\nMax xorig & xcell: {'xorig': -160.0, 'xcell': 0.10000000149011612}\nMax yorig & ycell: {'yorig': 32.0, 'ycell': 0.10000000149011612}\nncols: {1081, 1041}\nnrows: {381}\nxorigs: {-160.0, -156.0}\nyorigs: {32.0}\nxcells: {0.10000000149011612}\nycells: {0.10000000149011612}\n\n\ndataset: BSC12CA12-01\nNumber of successful files: 1003\nMax cell sizes: max_ncols = 1081 and max_nrows = 381\nMax xorig & xcell: {'xorig': -160.0, 'xcell': 0.10000000149011612}\nMax yorig & ycell: {'yorig': 32.0, 'ycell': 0.10000000149011612}\nncols: {1081, 1041}\nnrows: {381}\nxorigs: {-160.0, -156.0}\nyorigs: {32.0}\nxcells: {0.10000000149011612}\nycells: {0.10000000149011612}\n\n\n\n\n7.4.1 Rationale and Future Improvements\nOn our first attempt to convert the data we discovered that various files failed to open. Therefore, we used a dictionary to keep track of which files successfully open.\nIn version 2 of our conversion, we discovered that some files had differing values for NCOLS. The IDX file format expects all arrays across all time steps to have the same number of rows and columns. We realized that our assumption that all attributes were the same across all files was unfounded. Therefore, we collected information about the values for all attributes in the NetCDF files as shown above so that we could resample arrays to use 1081 columns.\nOne improvement to this step is to stop tracking maxes and unique values seperately. Instead, we could just track unique values then get maxes from there.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Conversion</span>"
    ]
  },
  {
    "objectID": "data_conversion.html#preparing-resampling-grids",
    "href": "data_conversion.html#preparing-resampling-grids",
    "title": "7  Data Conversion",
    "section": "7.5 Preparing Resampling Grids",
    "text": "7.5 Preparing Resampling Grids\nTo resample arrays of shape 1041×381 to 1081×381, we use the SciPy griddata function from the interpolate package. This function gives interpolated values on set of points xi from a set of points with corresponding values. We refer the reader to SciPy’s documentation for details.\nRecall we can generate a set of latitude and longitude coordinates on the 1041×381 and 1081×381 grid by using the attributes given in each NetCDF file, see Chapter 5 for an example. We can thus sample a set of corresponding PM25 values on a 1081×381 grid by using using a 1041×381 array of latitude and longitudes as points and 1041×381 array of PM25 values as values with the griddata function. We willgenerate these grids of lat/lon points using the attribute information we collected in the previous step.\n\n7.5.1 Generate Grids of Latitude and Longitude Points\n1max_xorig = max_grid_x[ids[0]]['xorig']\nmax_xcell = max_grid_x[ids[0]]['xcell']\nmax_yorig = max_grid_y[ids[0]]['yorig']\nmax_ycell = max_grid_y[ids[0]]['ycell']\n\n2big_lon = np.linspace(max_xorig, max_xorig + max_xcell * (max_ncols[ids[0]] - 1), max_ncols[ids[0]])\nbig_lat = np.linspace(max_yorig, max_yorig + max_ycell * (max_nrows[ids[0]] - 1), max_nrows[ids[0]])\n\n3big_lon_pts, big_lat_pts = np.meshgrid(big_lon, big_lat)\nbig_tups = np.array([tup for tup in zip(big_lon_pts.flatten(), big_lat_pts.flatten())])\n\n4sml_ds = xr.open_dataset(firesmoke_dir + \"/BSC00CA12-01/dispersion_20210304.nc\")\nsml_lon = np.linspace(sml_ds.XORIG, sml_ds.XORIG + sml_ds.XCELL * (sml_ds.NCOLS - 1), sml_ds.NCOLS)\nsml_lat = np.linspace(sml_ds.YORIG, sml_ds.YORIG + sml_ds.YCELL * (sml_ds.NROWS - 1), sml_ds.NROWS)\n\n5sml_lon_pts, sml_lat_pts = np.meshgrid(sml_lon, sml_lat)\nsml_tups = np.array([tup for tup in zip(sml_lon_pts.flatten(), sml_lat_pts.flatten())])\n\n1\n\nGet the x/y origin and cell size parameters for the big 1081x381 grid.\n\n2\n\nGenerate one two lists, defining a grid of latitudes and longitudes.\n\n3\n\nUsing big_lon and big_lat, use meshgrid to generate our 1081x381 set of longitudes and latitudes.\n\n4\n\nOpen a file that uses the small 1041x381 grid. Then, use the attributes in that file to generate two lists defining a grid of latitudes and longitudes.\n\n5\n\nUsing sml_lon and sml_lat, use meshgrid to generate our 1041x381 set of longitudes and latitudes.\n\n\n\n\n7.5.2 Example with griddata\nNow that we have the two sets of latitude and longitude points, we show an example of how these are used to resample an array of data from a 1041×381 grid to a 1081×381 grid.\nTODO\n\n\n7.5.3 Rationale and Previous Revelations\nIDX expects all arrays at each time step to be the same dimensionality. However, as noted above the data has either NCOLS = 1041 or NCOLS = 1081.\nWe considered various approaches to the question of: How do we normalize variance in data dimensionality?\n\n\n\nTable 7.1: The Weaknesses of Approaches to Handling Varying Array Sizes\n\n\n\n\n\n\n\n\n\nApproach\nWeakness\n\n\n\n\nExclude arrays with 1041 columns.\nThrowing away those data points would discard all the information they hold.\n\n\nForce data with 1041 columns into an array with 1081 columns without resampling.\nThis results in unused columns within the 1081-column array, leading to discontinuities and potential artifacts in the data representation.\n\n\nCrop arrays on 1081 columns to 1041 columns\nCropping the data would result in loss of information.\n\n\n\n\n\n\nThe approach we chose was to resample the data with 1041 columns to arrays with 1081 columns. This produced the most visually appealing result and preserved the most information possible.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Conversion</span>"
    ]
  },
  {
    "objectID": "data_conversion.html#sequencing-of-netcdf-files",
    "href": "data_conversion.html#sequencing-of-netcdf-files",
    "title": "7  Data Conversion",
    "section": "7.6 Sequencing of NetCDF Files",
    "text": "7.6 Sequencing of NetCDF Files\nRecall that we have 2-day forecasts that are run 4 times a day.\nAcross all the NetCDF files we have downloaded, recall for every hour there exists 1-4 different forecasts to represent that hour. We aim to choose the forecast which best represents each hour. In order to understand the best representation for each hour, we need to know what hours are represented in each forecast.\n\n7.6.1 Finding Hours per Forecast ID\nRecall that within each set of Forecast ID files, some files failed to download or otherwise open. Therefore, we must check exactly what set of hours are available in each collection of forecast ID NetCDF files.\nIn order to make loading an hour from a specified forecast ID dataset as easy as possible, we create a dictionary for each forecast ID set. Below you can see the first few entries of this dictionary:\n#TODO\nTo generate this dictionary, we did the following:\n1id_sets = {id_: {} for id_ in ids}\n\nfor id_ in ids:\n    # get successful files to add all successful hours to set\n2    for file in tqdm(successful_files[id_]):\n3        path = f'{firesmoke_dir}/{id_}/{file}'\n        \n        ds = xr.open_dataset(path)\n\n4        for h in range(ds.sizes[\"TSTEP\"]):\n            id_sets[id_][(file, parse_tflag(ds['TFLAG'].values[h][0]))] = h\n\n1\n\nInitialize a dictionary to hold an empty dictionary for each forecast ID.\n\n2\n\nWe step through all the successfully opened files found for id_.\n\n3\n\nOpen the file and open it using xarray.\n\n4\n\nCreate a tuple with the file name and the parsed TFLAG for current hour h. Add index h to our dictionary assigning the tuple as its key.\n\n\n\n\n7.6.2 Creating the Sequence\nNow that we have an indexable set of all available hours for each forecast ID, we can generate the sequence to extract the time series dataset we would like to create.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Conversion</span>"
    ]
  },
  {
    "objectID": "data_conversion.html#creating-the-idx-file",
    "href": "data_conversion.html#creating-the-idx-file",
    "title": "7  Data Conversion",
    "section": "7.7 Creating the IDX File",
    "text": "7.7 Creating the IDX File\nAt this point, we have precomputed the order in which we will load and write each array to our IDX file. We will now put everything together to generate our single dataset.\n1f = Field(\"PM25\", \"float32\")\n\n2db = CreateIdx(\n    url=idx_dir + \"/firesmoke.idx\",\n    fields=[f],\n    dims=[int(max_ncols[ids[0]]), int(max_nrows[ids[0]])],\n    time=[0, len(idx_calls) - 1, \"%00000000d/\"],\n)\n\n3tstep = 0\n\n4thresh = 1e-15\n\n5for call in tqdm(idx_calls):\n    curr_id = call[0]\n    curr_file = call[1]\n    tstep_index = call[3]\n\n6    ds = xr.open_dataset(f\"{firesmoke_dir}/{curr_id}/{curr_file}\")\n\n7    file_vals = np.squeeze(ds[\"PM25\"].values)\n\n8    resamp = ds.XORIG != max_xorig\n\n9    if resamp:\n        file_vals_resamp = griddata(\n            sml_tups,\n            file_vals[tstep_index].flatten(),\n            big_tups,\n            method=\"cubic\",\n            fill_value=0,\n        )\n\n10        file_vals_resamp[file_vals_resamp &lt; thresh] = 0\n\n11        file_vals_resamp = file_vals_resamp.reshape((len(big_lat), len(big_lon)))\n\n12        db.write(data=file_vals_resamp.astype(np.float32), field=f, time=tstep)\n13    else:\n        db.write(data=file_vals[tstep_index], field=f, time=tstep)\n\n14    tstep = tstep + 1\n\n1\n\nCreate an OpenVisus field to hold the PM25 variable data.\n\n2\n\nCreate the IDX file wherein url is the location to write the file, fields holds the data variables we will save, dims represents the shape of each array, and time defines how many time steps there are.\n\n3\n\nWe will usethis to keep track of which time step we are on as we step through our idx_calls.\n\n4\n\nThreshold to use to change small-enough resampled values to 0.\n\n5\n\nGet the information for current time step, in particular the [curr_id, file_str, parse_tflag(ds[‘TFLAG’].values[tstep_idx][0]), tstep_idx]\n\n6\n\nLoad the current file with xarray.\n\n7\n\nGet the full array of PM25 values in the file.\n\n8\n\nIf ds.XORIG is not already for the 1081×381 grid, we need to resample it to the larger grid.\n\n9\n\nUsing gridddata, interpolate the values on a 1081×381 grid using the precomputed lat/lon points.\n\n10\n\nAny values that are less than our threshold should have a value of 0. WHY\n\n11\n\nReshape the interpolated values to 1081×381.\n\n12\n\nWrite the resampled values for hour h to timestep t and field f of our IDX file.\n\n13\n\nThese values are already on a 1081×381 grid, so write the values at hour h to timestep t and field f of our IDX file.\n\n14\n\nIncrement to the next timestep for writing to IDX.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Conversion</span>"
    ]
  },
  {
    "objectID": "data_validation.html",
    "href": "data_validation.html",
    "title": "8  Data Validation",
    "section": "",
    "text": "8.1 Data Validation vs Data Exploration\nData exploration enlightens one on the contents of the data and metadata one presumes they have. We performed data exploration by loading and visualizing a few files. This allowed us to understand what data UBC aims to provide.\nWhat data exploration does not do is explain the origin of issues such missing or seemingly corrupt data. Data exploration uses the assumption that the data is perfect.\nData validation forces one to consider, where do issues that appear in the data come from, and are these issues with the data or with the systems used to access and manipulate the data?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Validation</span>"
    ]
  },
  {
    "objectID": "data_validation.html#data-loading",
    "href": "data_validation.html#data-loading",
    "title": "8  Data Validation",
    "section": "8.2 Data Loading",
    "text": "8.2 Data Loading\nMost of the issues we faced during data curation resulted from failing to validate our data loading system.\n\n8.2.1",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Validation</span>"
    ]
  },
  {
    "objectID": "data_validation.html#data-conversion",
    "href": "data_validation.html#data-conversion",
    "title": "8  Data Validation",
    "section": "8.3 Data Conversion",
    "text": "8.3 Data Conversion",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Validation</span>"
    ]
  },
  {
    "objectID": "netcdf_demo.html#netcdf-demo",
    "href": "netcdf_demo.html#netcdf-demo",
    "title": "5  NetCDF Visualization Demo",
    "section": "",
    "text": "5.1.1 Accessing the File\nWe use the forecast for March 4, 2021 where the weather forecast is initiated at 00:00:00 UTC and the smoke forecast is initialized at 08:00:00 UTC. You can download this file by navigating to the URL below.\n\n\nCode\nforecast_id = \"BSC00CA12-01\"\nyyyymmdd = \"20210304\"\ninit_time = \"08\"\n\nurl = (\n    f\"https://firesmoke.ca/forecasts/{forecast_id}/{yyyymmdd}{init_time}/dispersion.nc\"\n)\n\nprint(f\"Navigate to this URL in your browser: {url}\")\n\n\nNavigate to this URL in your browser: https://firesmoke.ca/forecasts/BSC00CA12-01/2021030408/dispersion.nc\n\n\n\nOpening the File\nWe use xarray to open the NetCDF file and preview it.\n\n\nCode\nimport xarray as xr\n\nds = xr.open_dataset(\"dispersion.nc\")\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (TSTEP: 51, VAR: 1, DATE-TIME: 2, LAY: 1, ROW: 381, COL: 1041)\nDimensions without coordinates: TSTEP, VAR, DATE-TIME, LAY, ROW, COL\nData variables:\n    TFLAG    (TSTEP, VAR, DATE-TIME) int32 ...\n    PM25     (TSTEP, LAY, ROW, COL) float32 ...\nAttributes: (12/33)\n    IOAPI_VERSION:  $Id: @(#) ioapi library version 3.0 $                    ...\n    EXEC_ID:        ????????????????                                         ...\n    FTYPE:          1\n    CDATE:          2021063\n    CTIME:          101914\n    WDATE:          2021063\n    ...             ...\n    VGLVLS:         [10.  0.]\n    GDNAM:          HYSPLIT CONC    \n    UPNAM:          hysplit2netCDF  \n    VAR-LIST:       PM25            \n    FILEDESC:       Hysplit Concentration Model Output                       ...\n    HISTORY:        xarray.DatasetDimensions:TSTEP: 51VAR: 1DATE-TIME: 2LAY: 1ROW: 381COL: 1041Coordinates: (0)Data variables: (2)TFLAG(TSTEP, VAR, DATE-TIME)int32...units :&lt;YYYYDDD,HHMMSS&gt;long_name :TFLAG           var_desc :Timestep-valid flags:  (1) YYYYDDD or (2) HHMMSS                                [102 values with dtype=int32]PM25(TSTEP, LAY, ROW, COL)float32...long_name :PM25            units :ug/m^3          var_desc :PM25                                                                            [20227671 values with dtype=float32]Indexes: (0)Attributes: (33)IOAPI_VERSION :$Id: @(#) ioapi library version 3.0 $                                           EXEC_ID :????????????????                                                                FTYPE :1CDATE :2021063CTIME :101914WDATE :2021063WTIME :101914SDATE :2021063STIME :90000TSTEP :10000NTHIK :1NCOLS :1041NROWS :381NLAYS :1NVARS :1GDTYP :1P_ALP :0.0P_BET :0.0P_GAM :0.0XCENT :-104.0YCENT :51.0XORIG :-156.0YORIG :32.0XCELL :0.10000000149011612YCELL :0.10000000149011612VGTYP :5VGTOP :-9999.0VGLVLS :[10.  0.]GDNAM :HYSPLIT CONC    UPNAM :hysplit2netCDF  VAR-LIST :PM25            FILEDESC :Hysplit Concentration Model Output                                              lat-lon coordinate system                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       HISTORY :\n\n\n\n\n\n5.1.2 Using the Data\n\nAccessing Arrays\nThe data we are interested in is the PM2.5 values. Let’s use xarray to preview the PM25 variable in our file.\n\n\nCode\nds[\"PM25\"]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'PM25' (TSTEP: 51, LAY: 1, ROW: 381, COL: 1041)&gt;\n[20227671 values with dtype=float32]\nDimensions without coordinates: TSTEP, LAY, ROW, COL\nAttributes:\n    long_name:  PM25            \n    units:      ug/m^3          \n    var_desc:   PM25                                                         ...xarray.DataArray'PM25'TSTEP: 51LAY: 1ROW: 381COL: 1041...[20227671 values with dtype=float32]Coordinates: (0)Indexes: (0)Attributes: (3)long_name :PM25            units :ug/m^3          var_desc :PM25                                                                            \n\n\nThe dimensions of the PM25 data array are composed of TSTEP, LAY, ROW, and COL. We do not need the LAY dimension, so let’s use numpy to remove it.\n\n\nCode\nimport numpy as np\n\n# use .values to get the four dimensional array.\nds_pm25_vals = ds[\"PM25\"].values\nprint(f'The shape of the data contained in our files is: {np.shape(ds_pm25_vals)}')\n\n# let's use np.squeeze to drop the LAY axis\nds_pm25_vals = np.squeeze(ds_pm25_vals)\nprint(f'After squeezing, the shape is: {np.shape(ds_pm25_vals)}')\n\n\nThe shape of the data contained in our files is: (51, 1, 381, 1041)\nAfter squeezing, the shape is: (51, 381, 1041)\n\n\n\n\nVisualize Array in matplotlib\nNow that we squeezed away the LAY dimension, we can index time step 15 and use matplotlib to visualize the timestep\n\n\nCode\nimport matplotlib.pyplot as plt\n\ntstep = 15\nsmoke_at_tstep = ds_pm25_vals[tstep, :, :]\n\nmy_fig, my_plt = plt.subplots(figsize=(15, 6))\n\n# color PM25 values on a log scale, since values are small\nmy_norm = \"log\" \n# ensure the aspect ratio of our plot fits all data, matplotlib can does this automatically\nmy_aspect = 'auto'\n# tell matplotlib, our origin is the lower-left corner\nmy_origin = 'lower'\n# select a colormap for our plot and the color bar on the right\nmy_cmap = 'viridis'\n \n# create our plot using imshow\nplot = my_plt.imshow(smoke_at_tstep, norm=my_norm,aspect=my_aspect, origin=my_origin, cmap=my_cmap)\n\n# add a colorbar to our figure, based on the plot we just made above\nmy_fig.colorbar(plot, location='right', label='ug/m^3')\n\n# Set title of our figure\nmy_fig.suptitle('Ground level concentration of PM2.5 microns and smaller')\n\n# Set title of our plot as the timestamp of our data\nmy_plt.set_title(f'Timestep {tstep}')\n\n# Show the resulting visualization\nplt.show()\n\n\n\n\n\n\n\n\n\nNotice there are no axis labels or metadata presented here. Next we will show how to use the metadata in dispersion.nc so the data is actually interpretable.\n\n\n\n5.1.3 Incorporating Metadata to Visualization via Coordinates\n\nLatitude and Longitude Coordinates\ndispersion.nc includes attributes to generate the latitude and longitude values on the grid defined by NCOLS and NROWS. We use this grid to match each data point in the PM25 variable to a lat/lon coordinate.\n\n\nCode\nxorig = ds.XORIG\nyorig = ds.YORIG\nxcell = ds.XCELL\nycell = ds.YCELL\nncols = ds.NCOLS\nnrows = ds.NROWS\n\nlongitude = np.linspace(xorig, xorig + xcell * (ncols - 1), ncols)\nlatitude = np.linspace(yorig, yorig + ycell * (nrows - 1), nrows)\n\nprint(\"Size of longitude & latitude arrays:\")\nprint(f'np.size(longitude) = {np.size(longitude)}')\nprint(f'np.size(latitude) = {np.size(latitude)}\\n')\nprint(\"Min & Max of longitude and latitude arrays:\")\nprint(f'longitude: min = {np.min(longitude)}, max = {np.max(longitude)}')\nprint(f'latitude: min = {np.min(latitude)}, max = {np.max(latitude)}')\n\n\nSize of longitude & latitude arrays:\nnp.size(longitude) = 1041\nnp.size(latitude) = 381\n\nMin & Max of longitude and latitude arrays:\nlongitude: min = -156.0, max = -51.999998450279236\nlatitude: min = 32.0, max = 70.00000056624413\n\n\nxarray allows us to create coordinates, which maps variable values to a value of our choice. In this case, we create coordinates mapping PM25 values to a latitude and longitude value.\n\n\nCode\n# Create coordinates for lat and lon\nds.coords['lat'] = ('ROW', latitude)\nds.coords['lon'] = ('COL', longitude)\n\n# Replace col and row dimensions with newly calculated lon and lat coordinates\nds = ds.swap_dims({'COL': 'lon', 'ROW': 'lat'})\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (TSTEP: 51, VAR: 1, DATE-TIME: 2, LAY: 1, lat: 381, lon: 1041)\nCoordinates:\n  * lat      (lat) float64 32.0 32.1 32.2 32.3 32.4 ... 69.6 69.7 69.8 69.9 70.0\n  * lon      (lon) float64 -156.0 -155.9 -155.8 -155.7 ... -52.2 -52.1 -52.0\nDimensions without coordinates: TSTEP, VAR, DATE-TIME, LAY\nData variables:\n    TFLAG    (TSTEP, VAR, DATE-TIME) int32 ...\n    PM25     (TSTEP, LAY, lat, lon) float32 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0\nAttributes: (12/33)\n    IOAPI_VERSION:  $Id: @(#) ioapi library version 3.0 $                    ...\n    EXEC_ID:        ????????????????                                         ...\n    FTYPE:          1\n    CDATE:          2021063\n    CTIME:          101914\n    WDATE:          2021063\n    ...             ...\n    VGLVLS:         [10.  0.]\n    GDNAM:          HYSPLIT CONC    \n    UPNAM:          hysplit2netCDF  \n    VAR-LIST:       PM25            \n    FILEDESC:       Hysplit Concentration Model Output                       ...\n    HISTORY:        xarray.DatasetDimensions:TSTEP: 51VAR: 1DATE-TIME: 2LAY: 1lat: 381lon: 1041Coordinates: (2)lat(lat)float6432.0 32.1 32.2 ... 69.8 69.9 70.0array([32.      , 32.1     , 32.2     , ..., 69.800001, 69.900001, 70.000001])lon(lon)float64-156.0 -155.9 ... -52.1 -52.0array([-156.      , -155.9     , -155.8     , ...,  -52.199998,  -52.099998,\n        -51.999998])Data variables: (2)TFLAG(TSTEP, VAR, DATE-TIME)int32...units :&lt;YYYYDDD,HHMMSS&gt;long_name :TFLAG           var_desc :Timestep-valid flags:  (1) YYYYDDD or (2) HHMMSS                                [102 values with dtype=int32]PM25(TSTEP, LAY, lat, lon)float320.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0long_name :PM25            units :ug/m^3          var_desc :PM25                                                                            array([[[[0.000000e+00, ..., 0.000000e+00],\n         ...,\n         [0.000000e+00, ..., 0.000000e+00]]],\n\n\n       ...,\n\n\n       [[[0.000000e+00, ..., 2.804227e-11],\n         ...,\n         [0.000000e+00, ..., 0.000000e+00]]]], dtype=float32)Indexes: (2)latPandasIndexPandasIndex(Index([              32.0, 32.100000001490116,  32.20000000298023,\n        32.30000000447035, 32.400000005960464,  32.50000000745058,\n         32.6000000089407,  32.70000001043081,  32.80000001192093,\n       32.900000013411045,\n       ...\n        69.10000055283308,   69.2000005543232,  69.30000055581331,\n        69.40000055730343,  69.50000055879354,  69.60000056028366,\n        69.70000056177378,   69.8000005632639,  69.90000056475401,\n        70.00000056624413],\n      dtype='float64', name='lat', length=381))lonPandasIndexPandasIndex(Index([             -156.0, -155.89999999850988, -155.79999999701977,\n       -155.69999999552965, -155.59999999403954, -155.49999999254942,\n        -155.3999999910593,  -155.2999999895692, -155.19999998807907,\n       -155.09999998658895,\n       ...\n        -52.89999846369028, -52.799998462200165,  -52.69999846071005,\n        -52.59999845921993, -52.499998457729816,   -52.3999984562397,\n       -52.299998454749584,  -52.19999845325947,  -52.09999845176935,\n       -51.999998450279236],\n      dtype='float64', name='lon', length=1041))Attributes: (33)IOAPI_VERSION :$Id: @(#) ioapi library version 3.0 $                                           EXEC_ID :????????????????                                                                FTYPE :1CDATE :2021063CTIME :101914WDATE :2021063WTIME :101914SDATE :2021063STIME :90000TSTEP :10000NTHIK :1NCOLS :1041NROWS :381NLAYS :1NVARS :1GDTYP :1P_ALP :0.0P_BET :0.0P_GAM :0.0XCENT :-104.0YCENT :51.0XORIG :-156.0YORIG :32.0XCELL :0.10000000149011612YCELL :0.10000000149011612VGTYP :5VGTOP :-9999.0VGLVLS :[10.  0.]GDNAM :HYSPLIT CONC    UPNAM :hysplit2netCDF  VAR-LIST :PM25            FILEDESC :Hysplit Concentration Model Output                                              lat-lon coordinate system                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       HISTORY :\n\n\nNow let’s move on to incorporating time stamp metadata.\n\n\nTime Coordinates\nRecall, there is a TFLAG variable in dispersion.nc.\n\n\nCode\nds['TFLAG']\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'TFLAG' (TSTEP: 51, VAR: 1, DATE-TIME: 2)&gt;\n[102 values with dtype=int32]\nDimensions without coordinates: TSTEP, VAR, DATE-TIME\nAttributes:\n    units:      &lt;YYYYDDD,HHMMSS&gt;\n    long_name:  TFLAG           \n    var_desc:   Timestep-valid flags:  (1) YYYYDDD or (2) HHMMSS             ...xarray.DataArray'TFLAG'TSTEP: 51VAR: 1DATE-TIME: 2...[102 values with dtype=int32]Coordinates: (0)Indexes: (0)Attributes: (3)units :&lt;YYYYDDD,HHMMSS&gt;long_name :TFLAG           var_desc :Timestep-valid flags:  (1) YYYYDDD or (2) HHMMSS                                \n\n\nThe first TFLAG looks like the following:\n\n\nCode\nds['TFLAG'].values[0][0]\n\n\narray([2021063,   90000], dtype=int32)\n\n\nThis time flag requires processing to be immediately legible. Let’s write a function to process the time flag accordingly. We use the datetime library.\n\n\nCode\nimport datetime\n\ndef parse_tflag(tflag):\n    \"\"\"\n    Return the tflag as a datetime object\n    :param list tflag: a list of two int32, the 1st representing date and 2nd representing time\n    \"\"\"\n    # obtain year and day of year from tflag[0] (date)\n    date = int(tflag[0])\n    year = date // 1000 # first 4 digits of tflag[0]\n    day_of_year = date % 1000 # last 3 digits of tflag[0]\n\n    # create datetime object representing date\n    final_date = datetime.datetime(year, 1, 1) + datetime.timedelta(days=day_of_year - 1)\n\n    # obtain hour, mins, and secs from tflag[1] (time)\n    time = int(tflag[1])\n    hours = time // 10000 # first 2 digits of tflag[1]\n    minutes = (time % 10000) // 100 # 3rd and 4th digits of tflag[1] \n    seconds = time % 100  # last 2 digits of tflag[1]\n\n    # create final datetime object\n    full_datetime = datetime.datetime(year, final_date.month, final_date.day, hours, minutes, seconds)\n    return full_datetime\n\n\nNow we have a datetime object to represent the timeflag in a more legible and usable format.\n\n\nCode\nprint(parse_tflag(ds['TFLAG'].values[0][0]))\n\n\n2021-03-04 09:00:00\n\n\n\n\nVisualize Array in matplotlib\nLet’s visualize timestep 15 again, but now we can label the data using latitudes and longitudes, and the corresponding time flag.\n\n\nCode\ntstep = 15\nsmoke_at_tstep = ds_pm25_vals[tstep, :, :]\ntstep_tflag = parse_tflag(ds['TFLAG'].values[tstep][0])\n\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\n\n# Initialize a figure and plot, so we can customize figure and plot of data\nmy_fig, my_plt = plt.subplots(figsize=(15, 6), subplot_kw=dict(projection=ccrs.PlateCarree()))\n\n# color PM25 values on a log scale, since values are small\nmy_norm = \"log\" \n# this will number our x and y axes based on the longitude latitude range\nmy_extent = [np.min(longitude), np.max(longitude), np.min(latitude), np.max(latitude)]\n# ensure the aspect ratio of our plot fits all data, matplotlib can does this automatically\nmy_aspect = 'auto'\n# tell matplotlib, our origin is the lower-left corner\nmy_origin = 'lower'\n# select a colormap for our plot and the color bar on the right\nmy_cmap = 'viridis'\n\n# create our plot using imshow\nplot = my_plt.imshow(smoke_at_tstep, norm=my_norm, extent=my_extent, \n          aspect=my_aspect, origin=my_origin, cmap=my_cmap)\n\n# draw coastlines\nmy_plt.coastlines()\n\n# draw latitude longitude lines\nmy_plt.gridlines(draw_labels=True)\n\n# add a colorbar to our figure, based on the plot we just made above\nmy_fig.colorbar(plot, location='right', label='ug/m^3')\n\n# Set x and y axis labels on our ax\nmy_fig.supxlabel('Longitude')\nmy_fig.supylabel('Latitude')\n\n# Set title of our figure\nmy_fig.suptitle('Ground level concentration of PM2.5 microns and smaller')\n\n# Set title of our plot as the timestamp of our data\nmy_plt.set_title(f'{tstep_tflag}')\n\n# Show the resulting visualization\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>NetCDF Visualization Demo</span>"
    ]
  }
]