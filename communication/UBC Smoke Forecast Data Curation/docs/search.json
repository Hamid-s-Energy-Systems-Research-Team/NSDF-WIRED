[
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "firesmoke.ca",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "UBC Smoke Forecast Data Curation and Distribution",
    "section": "",
    "text": "Preface\nWelcome to NSDF’s UBC Firesmoke Data Curation website. Here we describe the data curation process of UBC’s Smoke Forecast datasets. We inform readers of the challenges and solutions we found when repurposing these short term datasets into a long term dataset.\nIt is important to note that although we will present the data curation process in a linear fashion here, it was not a linear process. Rather, the process was cyclical, and at each iteration we introduced new improvements.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#navigation-and-webpage-information",
    "href": "index.html#navigation-and-webpage-information",
    "title": "UBC Smoke Forecast Data Curation and Distribution",
    "section": "Navigation and Webpage Information",
    "text": "Navigation and Webpage Information\nThis webpage is produced using Quart.\nYou can find the source code for this page at TODO.\nAll files or directories in this website are hosted at our GitHub repository, unless otherwise specified.\nAll code blocks contain annotations, hover over the numbers on the right hand side to see the accompanying annotation.\n1Hover over me ----&gt;\n\n1\n\nI’m an annotation.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "demo.html#this-notebook-provide-the-instructions-on-how-to-read-ubc-firesmoke-data-from-firsmoke_metadata.nc-using-xarray-and-the-openvisus-xarray-backend.",
    "href": "demo.html#this-notebook-provide-the-instructions-on-how-to-read-ubc-firesmoke-data-from-firsmoke_metadata.nc-using-xarray-and-the-openvisus-xarray-backend.",
    "title": "1  IDX File Demo",
    "section": "1.1 This notebook provide the instructions on how to read UBC firesmoke data from firsmoke_metadata.nc using xarray and the OpenVisus xarray backend.",
    "text": "1.1 This notebook provide the instructions on how to read UBC firesmoke data from firsmoke_metadata.nc using xarray and the OpenVisus xarray backend.\nDashboard visible here: http://chpc3.nationalsciencedatafabric.org:9988/dashboards",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>IDX File Demo</span>"
    ]
  },
  {
    "objectID": "demo.html#step-1-importing-the-libraries",
    "href": "demo.html#step-1-importing-the-libraries",
    "title": "1  IDX File Demo",
    "section": "1.2 Step 1: Importing the libraries",
    "text": "1.2 Step 1: Importing the libraries\n\n1.2.1 Please be sure to have libraries installed\n\n\nCode\n# for numerical work\nimport numpy as np\n\n# for accessing file system\nimport os\n\n# for loading netcdf files, for metadata\nimport xarray as xr\n# for connecting OpenVisus framework to xarray\n# from https://github.com/sci-visus/openvisuspy, \nfrom openvisuspy.xarray_backend import OpenVisusBackendEntrypoint\n\n# Used for processing netCDF time data\nimport time\nimport datetime\nimport requests\n# Used for indexing via metadata\nimport pandas as pd\n\n# for plotting\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\n\n\n#Stores the OpenVisus cache in the local direcrtory \nimport os\nos.environ[\"VISUS_CACHE\"]=\"./visus_cache_can_be_erased\"\nos.environ['CURL_CA_BUNDLE'] = ''",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>IDX File Demo</span>"
    ]
  },
  {
    "objectID": "demo.html#step-2-reading-the-data-metadata-from-file",
    "href": "demo.html#step-2-reading-the-data-metadata-from-file",
    "title": "1  IDX File Demo",
    "section": "1.3 Step 2: Reading the data & metadata from file",
    "text": "1.3 Step 2: Reading the data & metadata from file\n\n1.3.1 In this section, we load our data using xr.open_dataset.\n\n\nCode\n# path to tiny NetCDF\nurl = 'https://github.com/sci-visus/NSDF-WIRED/raw/main/data/firesmoke_metadata.nc'\n\n# Download the file using requests\nresponse = requests.get(url)\nlocal_netcdf = 'firesmoke_metadata.nc'\nwith open(local_netcdf, 'wb') as f:\n    f.write(response.content)\n    \n# open tiny netcdf with xarray and OpenVisus backend\nds = xr.open_dataset(local_netcdf, engine=OpenVisusBackendEntrypoint)\n\n\nov.LoadDataset(http://atlantis.sci.utah.edu/mod_visus?dataset=UBC_fire_smoke_BSC&cached=1)\nPM25\nAdding field  PM25 shape  [27357, 381, 1081, 21] dtype  float32 labels  ['time', 'ROW', 'COL', 'resolution'] Max Resolution  20\n\n\n\n\nCode\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:            (time: 27357, ROW: 381, COL: 1081, resolution: 21,\n                        VAR: 1, DATE-TIME: 2)\nDimensions without coordinates: time, ROW, COL, resolution, VAR, DATE-TIME\nData variables:\n    PM25               (time, ROW, COL, resolution) float32 ...\n    TFLAG              (time, VAR, DATE-TIME) int32 ...\n    wrf_arw_init_time  (time, VAR, DATE-TIME) int32 ...\n    resampled          (time) bool ...\n    CDATE              (time) int32 ...\n    CTIME              (time) int32 ...\n    WDATE              (time) int32 ...\n    WTIME              (time) int32 ...\n    SDATE              (time) int32 ...\n    STIME              (time) int32 ...\nAttributes: (12/28)\n    IOAPI_VERSION:  $Id: @(#) ioapi library version 3.0 $                    ...\n    EXEC_ID:        ????????????????                                         ...\n    FTYPE:          1\n    TSTEP:          10000\n    NTHIK:          1\n    NCOLS:          1081\n    ...             ...\n    GDNAM:          HYSPLIT CONC    \n    UPNAM:          hysplit2netCDF  \n    VAR-LIST:       PM25            \n    FILEDESC:       Hysplit Concentration Model Output                       ...\n    HISTORY:        \n    idx_url:        http://atlantis.sci.utah.edu/mod_visus?dataset=UBC_fire_s...xarray.DatasetDimensions:time: 27357ROW: 381COL: 1081resolution: 21VAR: 1DATE-TIME: 2Coordinates: (0)Data variables: (10)PM25(time, ROW, COL, resolution)float32...long_name :PM25            units :ug/m^3          var_desc :PM25                                                                            [236612908917 values with dtype=float32]TFLAG(time, VAR, DATE-TIME)int32...[54714 values with dtype=int32]wrf_arw_init_time(time, VAR, DATE-TIME)int32...[54714 values with dtype=int32]resampled(time)bool...[27357 values with dtype=bool]CDATE(time)int32...[27357 values with dtype=int32]CTIME(time)int32...[27357 values with dtype=int32]WDATE(time)int32...[27357 values with dtype=int32]WTIME(time)int32...[27357 values with dtype=int32]SDATE(time)int32...[27357 values with dtype=int32]STIME(time)int32...[27357 values with dtype=int32]Indexes: (0)Attributes: (28)IOAPI_VERSION :$Id: @(#) ioapi library version 3.0 $                                           EXEC_ID :????????????????                                                                FTYPE :1TSTEP :10000NTHIK :1NCOLS :1081NROWS :381NLAYS :1NVARS :1GDTYP :1P_ALP :0.0P_BET :0.0P_GAM :0.0XCENT :-106.0YCENT :51.0XORIG :-160.0YORIG :32.0XCELL :0.10000000149011612YCELL :0.10000000149011612VGTYP :5VGTOP :-9999.0VGLVLS :[10.  0.]GDNAM :HYSPLIT CONC    UPNAM :hysplit2netCDF  VAR-LIST :PM25            FILEDESC :Hysplit Concentration Model Output                                              lat-lon coordinate system                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       HISTORY :idx_url :http://atlantis.sci.utah.edu/mod_visus?dataset=UBC_fire_smoke_BSC&cached=1\n\n\n\n1.3.1.1 Data Variables Description\n\n\n\n\n\n\n\nAttribute\nDescription\n\n\n\n\nPM25\nThe concentration of particulate matter (PM2.5) for each time step, layer, row, and column in the spatial grid.\n\n\nTFLAG\nThe date and time of each data point.\n\n\nwrf_arw_init_time\nThe time at which this prediction’s weather forecast was initiated.\n\n\nresampled\nWhether this timestamp was resampled from a 381x1041 to 381x1081 grid or not.\n\n\nCDATE\nThe creation date of the data point, in YYYYDDD format.\n\n\nCTIME\nThe creation time of the data point, in HHMMSS format.\n\n\nWDATE\nThe date for which the weather forecast is initiated, in YYYYDDD format.\n\n\nWTIME\nThe time for which the weather forecast is initiated, in HHMMSS format.\n\n\nSDATE\nThe date for which the smoke forecast is initiated, in YYYYDDD format.\n\n\nSTIME\nThe time for which the weather forecast is initiated, in HHMMSS format.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>IDX File Demo</span>"
    ]
  },
  {
    "objectID": "demo.html#step-2.5-calculate-derived-metadata-using-original-metadata-above-to-create-coordinates",
    "href": "demo.html#step-2.5-calculate-derived-metadata-using-original-metadata-above-to-create-coordinates",
    "title": "1  IDX File Demo",
    "section": "1.4 Step 2.5, Calculate derived metadata using original metadata above to create coordinates",
    "text": "1.4 Step 2.5, Calculate derived metadata using original metadata above to create coordinates\n\n1.4.1 This is required to allow for indexing of data via metadata\n\n1.4.1.1 Calculate latitude and longitude grid\n\n\nCode\n# Get metadata to compute lon and lat\nxorig = ds.XORIG\nyorig = ds.YORIG\nxcell = ds.XCELL\nycell = ds.YCELL\nncols = ds.NCOLS\nnrows = ds.NROWS\n\nlongitude = np.linspace(xorig, xorig + xcell * (ncols - 1), ncols)\nlatitude = np.linspace(yorig, yorig + ycell * (nrows - 1), nrows)\n\nprint(\"Size of longitude & latitude arrays:\")\nprint(f'np.size(longitude) = {np.size(longitude)}')\nprint(f'np.size(latitude) = {np.size(latitude)}\\n')\nprint(\"Min & Max of longitude and latitude arrays:\")\nprint(f'longitude: min = {np.min(longitude)}, max = {np.max(longitude)}')\nprint(f'latitude: min = {np.min(latitude)}, max = {np.max(latitude)}')\n\n\nSize of longitude & latitude arrays:\nnp.size(longitude) = 1081\nnp.size(latitude) = 381\n\nMin & Max of longitude and latitude arrays:\nlongitude: min = -160.0, max = -51.99999839067459\nlatitude: min = 32.0, max = 70.00000056624413\n\n\n\n\n1.4.1.2 Using calculated latitude and longitude, create coordinates allowing for indexing data using lat/lon\n\n\nCode\n# Create coordinates for lat and lon (credit: Aashish Panta)\nds.coords['lat'] = ('ROW', latitude)\nds.coords['lon'] = ('COL', longitude)\n\n# Replace col and row dimensions with newly calculated lon and lat arrays (credit: Aashish Panta)\nds = ds.swap_dims({'COL': 'lon', 'ROW': 'lat'})\n\n\n\n\n1.4.1.3 Create coordinates allowing for indexing data using timestamp\n\n1.4.1.3.1 First, convert tflags to timestamps that are compatible with xarray\n\n\nCode\ndef parse_tflag(tflag):\n    \"\"\"\n    Return the tflag as a datetime object\n    :param list tflag: a list of two int32, the 1st representing date and 2nd representing time\n    \"\"\"\n    # obtain year and day of year from tflag[0] (date)\n    date = int(tflag[0])\n    year = date // 1000 # first 4 digits of tflag[0]\n    day_of_year = date % 1000 # last 3 digits of tflag[0]\n\n    # create datetime object representing date\n    final_date = datetime.datetime(year, 1, 1) + datetime.timedelta(days=day_of_year - 1)\n\n    # obtain hour, mins, and secs from tflag[1] (time)\n    time = int(tflag[1])\n    hours = time // 10000 # first 2 digits of tflag[1]\n    minutes = (time % 10000) // 100 # 3rd and 4th digits of tflag[1] \n    seconds = time % 100  # last 2 digits of tflag[1]\n\n    # create final datetime object\n    full_datetime = datetime.datetime(year, final_date.month, final_date.day, hours, minutes, seconds)\n    return full_datetime\n\n\n\n\n1.4.1.3.2 Return an array of the tflags as pandas timestamps\n\n\nCode\n# get all tflags\ntflag_values = ds['TFLAG'].values\n\n# to store pandas timestamps\ntimestamps = []\n\n# convert all tflags to pandas timestamps, store in timestamps list\nfor tflag in tflag_values:\n    timestamps.append(pd.Timestamp(parse_tflag(tflag[0])))\n\n# check out the first 3 timestamps\ntimestamps[0:3]\n\n\n[Timestamp('2021-03-04 00:00:00'),\n Timestamp('2021-03-04 01:00:00'),\n Timestamp('2021-03-04 02:00:00')]\n\n\n\n\nCode\n# set coordinates to each timestep with these pandas timestamps\nds.coords['time'] = ('time', timestamps)\n\n\n\n\n\n1.4.1.4 The timestamps may not be intuitive. The following utility function returns the desired pandas timestamp based on your date and time of interest.\n\n1.4.1.4.1 When you index the data at a desired time, use this function to get the timestamp you need to index.\n\n\nCode\ndef get_timestamp(year, month, day, hour):\n    \"\"\"\n    return a pandas timestamp using the given date-time arguments\n    :param int year: year\n    :param int month: month\n    :param int day: day\n    :param int hour: hour\n    \"\"\"\n    # Convert year, month, day, and hour to a datetime object\n    full_datetime = datetime.datetime(year, month, day, hour)\n    \n    # Extract components from the datetime object\n    year = full_datetime.year\n    day_of_year = full_datetime.timetuple().tm_yday\n    hours = full_datetime.hour\n    minutes = full_datetime.minute\n    seconds = full_datetime.second\n\n    # Compute tflag[0] and tflag[1]\n    tflag0 = year * 1000 + day_of_year\n    tflag1 = hours * 10000 + minutes * 100 + seconds\n\n    # Return the Pandas Timestamp object\n    return pd.Timestamp(full_datetime)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>IDX File Demo</span>"
    ]
  },
  {
    "objectID": "demo.html#step-3-select-a-data_slice",
    "href": "demo.html#step-3-select-a-data_slice",
    "title": "1  IDX File Demo",
    "section": "1.5 Step 3: Select a data_slice",
    "text": "1.5 Step 3: Select a data_slice\n\n1.5.1 This section shows you how to load the data you want.\n\n1.5.1.1 You can index the data using indices, timestamps*, latitude & longitude, and by desired resolution**.\n*Not setting any time means the first timestep available is selected. **Not setting quality means full data resolution is selected.\n\n1.5.1.1.1 In this case, let’s get all available firesmoke data for March 5, 2021 00:00:00 and the time and date for which it’s weather and smoke forecast were initiated.\n\n\nCode\n# select timestamp\nmy_timestamp = get_timestamp(2021, 3, 5, 0)\n\n# select resolution, let's use full resolution since data isn't too big at one time slice\n# data resolution can be -19 for lowest res and 0 for highest res\ndata_resolution = 0\n\n# get PM25 values and provide 4 values, the colons mean select all lat and lon indices\ndata_array_at_time = ds['PM25'].loc[my_timestamp, :, :, data_resolution]\n\n# the metadata specifying weather and smoke forecast initialization times\nresampled = ds['resampled'].loc[my_timestamp]\nsdate = ds['SDATE'].loc[my_timestamp]\nstime = ds['STIME'].loc[my_timestamp]\nwdate = ds['WDATE'].loc[my_timestamp]\nwtime = ds['WTIME'].loc[my_timestamp]\n\n# notice, to access the data, you must append \".values\" to the data array we got above\nprint(f'timestamp: {my_timestamp}')\nprint(f'resampled: {resampled.values} (boolean)')\nprint(f'SDATE is {sdate.values} (YYYYDDD)')\nprint(f'STIME is {stime.values} (HHMMSS)')\nprint(f'WDATE is {wdate.values} (YYYYDDD)')\nprint(f'WTIME is {wtime.values} (HHMMSS)')\nprint(f'shape of data_array_at_time.values = {np.shape(data_array_at_time.values)}')\n\n\ntimestamp: 2021-03-05 00:00:00\nresampled: True (boolean)\nSDATE is 2021063 (YYYYDDD)\nSTIME is 210000 (HHMMSS)\nWDATE is 2021063 (YYYYDDD)\nWTIME is 204413 (HHMMSS)\nUsing Max Resolution:  20\nTime: 24, max_resolution: 20, logic_box=(0, 1081, 0, 381), field: PM25\nshape of data_array_at_time.values = (381, 1081)\n\n\n\n\n1.5.1.1.2 Perhaps we want to slice a specific latitude longitude range from our data_array_at_time, for example, latitude range [35, 50] and longitude range [-140, -80]. Let’s do that below.\n\n\nCode\n# # define range for latitude and longitude to use\nmin_lat = 35\nmax_lat = 50\nmin_lon = -140\nmax_lon = -80\n\n# get PM25 values and provide 4 values, but this time at our desired ranges\ndata_array_at_latlon = ds['PM25'].loc[my_timestamp, min_lat:max_lat, min_lon:max_lon, data_resolution]\n\n# notice, to access the data, you must append \".values\" to the data array we got above\nprint(f'timestamp: {my_timestamp}')\nprint(f'shape of data_array_at_time.values = {np.shape(data_array_at_latlon.values)}')\n\n\ntimestamp: 2021-03-05 00:00:00\nUsing Max Resolution:  20\nTime: 24, max_resolution: 20, logic_box=(200, 800, 30, 180), field: PM25\nshape of data_array_at_time.values = (150, 600)\n\n\nWe show how to obtain this attribute information for a time step of one’s choice, let’s use\n\n\n\n1.5.1.2 The following are the max and min timestamps, lon/lat values, and data resolutions you can index by\n\n1.5.1.2.1 Be sure you index within the data range, otherwise you may get errors since no data exists outside these ranges!\n\n\nCode\n# NOTE: there is one dummy date, ignore ds['time'].values[-1]\nprint(f\"earliest valid timestamp is: {ds['time'].values[0]}\")\nprint(f\"latest valid timestamp is: {ds['time'].values[-2]}\\n\")\n\nprint(f\"valid longitude range is: {ds['lon'].values[0]}, {ds['lon'].values[-1]}\")\nprint(f\"valid latitude range is: {ds['lat'].values[0]}, {ds['lat'].values[-1]}\\n\")\n\nprint(f\"valid data resolutions range is: [-19, 0]\")\n\n\nearliest valid timestamp is: 2021-03-04T00:00:00.000000000\nlatest valid timestamp is: 2024-06-27T22:00:00.000000000\n\nvalid longitude range is: -160.0, -51.99999839067459\nvalid latitude range is: 32.0, 70.00000056624413\n\nvalid data resolutions range is: [-19, 0]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>IDX File Demo</span>"
    ]
  },
  {
    "objectID": "demo.html#step-4-visualize-data_slice",
    "href": "demo.html#step-4-visualize-data_slice",
    "title": "1  IDX File Demo",
    "section": "1.6 Step 4: Visualize data_slice",
    "text": "1.6 Step 4: Visualize data_slice\n\n1.6.1 One can visualize the data either by:\n\n\n1.6.2 1. Get the values from your data_array_at_time and plot using your favorite python visualization library. We’ll use matplotlib.\n\n1.6.2.1 2. Use xarray’s built in plotting function (not recommended, as it is not robust)\nHere we plot data_array_at_time with matplotlib and its basemap extenstion to add geographic context.\n\n\nCode\n# Let's use matplotlib's imshow, since our data is on a grid\n# ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n\n# Initialize a figure and plot, so we can customize figure and plot of data\n# ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html\n# ref: https://scitools.org.uk/cartopy/docs/latest/getting_started/index.html\nmy_fig, my_plt = plt.subplots(figsize=(15, 6), subplot_kw=dict(projection=ccrs.PlateCarree()))\n\n# Let's set some parameters to get the visualization we want\n# ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n\n# color PM25 values on a log scale, since values are small\nmy_norm = \"log\" \n# this will number our x and y axes based on the longitude latitude range\nmy_extent = [np.min(longitude), np.max(longitude), np.min(latitude), np.max(latitude)]\n# ensure the aspect ratio of our plot fits all data, matplotlib can does this automatically\nmy_aspect = 'auto'\n# tell matplotlib, our origin is the lower-left corner\nmy_origin = 'lower'\n# select a colormap for our plot and the color bar on the right\nmy_cmap = 'Oranges'\n\n# create our plot using imshow\nplot = my_plt.imshow(data_array_at_time.values, norm=my_norm, extent=my_extent, \n          aspect=my_aspect, origin=my_origin, cmap=my_cmap)\n\n# draw coastlines\nmy_plt.coastlines()\n\n# draw latitude longitude lines\n# ref: https://scitools.org.uk/cartopy/docs/latest/gallery/gridlines_and_labels/gridliner.html\nmy_plt.gridlines(draw_labels=True)\n\n# add a colorbar to our figure, based on the plot we just made above\nmy_fig.colorbar(plot,location='right', label='ug/m^3')\n\n# Add metadata as text annotations\nmetadata_text = (\n    f'resampled: {resampled.values}\\n'\n    f'SDATE: {sdate.values}\\n'\n    f'STIME: {stime.values}\\n'\n    f'WDATE: {wdate.values}\\n'\n    f'WTIME: {wtime.values}'\n)\n\n# Place metadata text on the plot\nmy_plt.text(0.02, 0.02, metadata_text, transform=my_plt.transAxes,\n            fontsize=12, verticalalignment='bottom', bbox=dict(facecolor='white', alpha=0.8))\n\n# Set x and y axis labels on our ax\nmy_fig.supxlabel('Longitude')\nmy_fig.supylabel('Latitude')\n\n# Set title of our figure\nmy_fig.suptitle('Ground level concentration of PM2.5 microns and smaller')\n\n# Set title of our plot as the timestamp of our data\nmy_plt.set_title(f'{my_timestamp}')\n\n# Show the resulting visualization\nplt.show()\n\n\n\n\n\n\n\n\n\nHere we plot with xarray’s built-in matplotlib powered plotter.\n\n\nCode\ndata_array_at_time.plot(vmin=0, vmax=30)\n\n\n\n\n\n\n\n\n\nHere we plot data_array_at_latlon. We use the exact same code, but define my_extent accordingly.\n\n\nCode\n# Let's use matplotlib's imshow, since our data is on a grid\n# ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n\n# Initialize a figure and plot, so we can customize figure and plot of data\n# ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html\nmy_fig, my_plt = plt.subplots(figsize=(15, 6), subplot_kw=dict(projection=ccrs.PlateCarree()))\n\n# Let's set some parameters to get the visualization we want\n# ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n\n# color PM25 values on a log scale, since values are small\nmy_norm = \"log\" \n# ***this will number our x and y axes based on the longitude latitude range***\nmy_extent = [min_lon, max_lon, min_lat, max_lat]\n# ensure the aspect ratio of our plot fits all data, matplotlib can does this automatically\nmy_aspect = 'auto'\n# tell matplotlib, our origin is the lower-left corner\nmy_origin = 'lower'\n# select a colormap for our plot and the color bar on the right\nmy_cmap = 'Oranges'\n\n# create our plot using imshow\nplot = plt.imshow(data_array_at_latlon.values, norm=my_norm, extent=my_extent, \n          aspect=my_aspect, origin=my_origin, cmap=my_cmap)\n\n# draw coastlines\nmy_plt.coastlines()\n\n# draw latitude longitude lines\n# ref: https://scitools.org.uk/cartopy/docs/latest/gallery/gridlines_and_labels/gridliner.html\nmy_plt.gridlines(draw_labels=True)\n\n# add a colorbar to our figure, based on the plot we just made above\nmy_fig.colorbar(plot,location='right', label='ug/m^3')\n\n# Add metadata as text annotations\nmetadata_text = (\n    f'resampled: {resampled.values}\\n'\n    f'SDATE: {sdate.values}\\n'\n    f'STIME: {stime.values}\\n'\n    f'WDATE: {wdate.values}\\n'\n    f'WTIME: {wtime.values}'\n)\n\n# Place metadata text on the plot\nmy_plt.text(0.02, 0.02, metadata_text, transform=my_plt.transAxes,\n            fontsize=12, verticalalignment='bottom', bbox=dict(facecolor='white', alpha=0.8))\n\n# Set x and y axis labels on our ax\nmy_fig.supxlabel('Longitude')\nmy_fig.supylabel('Latitude')\n\n# Set title of our figure\nmy_fig.suptitle('Ground level concentration of PM2.5 microns and smaller')\n\n# Set title of our plot as the timestamp of our data\nmy_plt.set_title(f'{my_timestamp}')\n\n# Show the resulting visualization\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndata_array_at_latlon.plot(vmin=0, vmax=30)\n\n\n\n\n\n\n\n\n\n\n\n\n1.6.3 Please reach out to Arleth Salinas or Valerio Pascucci for any concerns about the notebook. Thank you!\n\nArleth Salinas (arleth.salinas@utah.edu)\nValerio Pascucci (pascucci.valerio@gmail.com)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>IDX File Demo</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2  Introduction",
    "section": "",
    "text": "Wildfires in North America have significantly impacted ecosystems and human society [1]. Climate change affects the frequency, duration, and severity of wildfires thus necessitating the use of wildfire prediction systems to effectively mitigate wildfire impact. However, data for understanding the impact of climate change on wildfires is limited, only available for a few regions and for only a few decades [2]. Furthermore, wildfire prediction systems in North America prioritize decision making and fire management on short timescales, from minutes to months. Therefore, long term wildfire prediction systems have limited access to aggregate short term data, due to resource constraints from fire management entities to share the data they collect and curate.\nThe Weather Forecast Research Team at the University of British Columbia (UBC) generates a short term dataset of PM2.5 smoke particulate presence in North America. Over the past 3 years, each day four times a day, UBC has created forecasts of PM2.5 smoke particulate on the ground for Canada and the continental United States. This is done using their The BlueSky Western Canada Wildfire Smoke Forecasting System. UBC provides access to this data to paying customers and for free on a daily basis via a web-based visualization and file download.\nThese smoke predictions are useful for those who must make decisions on how to deal with smoke as it comes. However, these years of forecasts are not available in a non-trivial fashion for long term forecasting. The data only exists among the hundreds of NetCDF files that UBC has generated.\nOur task is to obtain a single long term dataset from the smoke forecast files that are available from UBC.\n\n3 References",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "sys_specs.html",
    "href": "sys_specs.html",
    "title": "3  System and Environment",
    "section": "",
    "text": "3.1 Machine Specification\nThe data we curate is over 300 gigabytes large. Therefore we used the SCI institute’s in-house machine ‘atlantis’ for data staging and processing. See Table tbl-specs.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>System and Environment</span>"
    ]
  },
  {
    "objectID": "sys_specs.html#machine-specification",
    "href": "sys_specs.html#machine-specification",
    "title": "3  System and Environment",
    "section": "",
    "text": "Table 3.1: ‘atlantis’ System Specifications\n\n\n\n\n\nProperty\nValue\n\n\n\n\nArchitecture\nx86_64\n\n\nCPU op-mode(s)\n32-bit, 64-bit\n\n\nByte Order\nLittle Endian\n\n\nAddress sizes\n44 bits physical, 48 bits virtual\n\n\nCPU(s)\n48\n\n\nOn-line CPU(s) list\n0-47\n\n\nThread(s) per core\n2\n\n\nCore(s) per socket\n6\n\n\nSocket(s)\n4",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>System and Environment</span>"
    ]
  },
  {
    "objectID": "sys_specs.html#environment",
    "href": "sys_specs.html#environment",
    "title": "3  System and Environment",
    "section": "3.2 Environment",
    "text": "3.2 Environment\nWe aim to work in an environment that can be most easily reproduced and documented. Therefore we used Python 3.9.19 via conda. We did all our work within the Project Jupyter environment.\nTo find our the yml file containing our exported conda environment please see the sidebar.\nTo work on the ‘atlantis’ machine, we used SSH to connect to the machine.\nIn the proceeding chapters we will specify which tools and libraries were used and why.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>System and Environment</span>"
    ]
  },
  {
    "objectID": "data_source.html",
    "href": "data_source.html",
    "title": "4  The Data Source",
    "section": "",
    "text": "4.1 Overview\nThe Weather Forecast Research Team at the University of British Columbia (UBC) generates a short term dataset of PM2.5 smoke particulate presence in North America. This is done using their The BlueSky Western Canada Wildfire Smoke Forecasting System. Over the past 3 years, each day four times a day, UBC creates 2-day forecasts of PM2.5 smoke particulate on the ground for Canada and the continental United States. Each such forecast is downloadable as a NetCDF file or KMZ file. UBC provides access to these predictions for free on a daily basis at their website firesmoke.ca.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Data Source</span>"
    ]
  },
  {
    "objectID": "data_source.html#ubc-smoke-forecast-files-access",
    "href": "data_source.html#ubc-smoke-forecast-files-access",
    "title": "4  The Data Source",
    "section": "4.2 UBC Smoke Forecast Files Access",
    "text": "4.2 UBC Smoke Forecast Files Access\n\n\n4.2.1 Available Forecasts\nAll forecast files are uniquely identifiable with a forecast ID based on when their meteorology forecast is initiated, a smoke forecast initialization time, and by date. The time ranges of available files by forecast ID is shown in Table 4.1. Please note, there are occassional failed forecasts or otherwise unavailable files within the date ranges specified Table 4.1, see Section 4.4 for further details.\n\n\n\nTable 4.1: Dates for which all forecast ID datasets are publicly available. All times are in UTC and the grid size is 12 km.\n\n\n\n\n\n\n\n\n\n\n\n\nForecast ID\nMeteorology Forecast Initialization (UTC)\nSmoke Forecast Initialization (UTC)\nStart Date\nEnd Date\n\n\n\n\nBSC00CA12-01\n00Z\n08Z\nMarch 4, 2021\nPresent Day\n\n\nBSC06CA12-01\n06Z\n14Z\nMarch 4, 2021\nPresent Day\n\n\nBSC12CA12-01\n12Z\n20Z\nMarch 3, 2021\nPresent Day\n\n\nBSC18CA12-01\n18Z\n02Z\nMarch 4, 2021\nPresent Day\n\n\n\n\n\n\n\nThe smoke forecasts are updated daily, including the present day, so there is no fixed end date. Therefore, the latest data must be downloaded on a regular basis. We have not implemented this process yet, so the latest forecast files we use are up to June 27, 2024.\nThere is no official source stating the earliest available date for each forecast. So, knowing the project began in 2021, we inferenced that the earliest available date would be in 2021. Via trial and error we found the earliest available dates.\n\n\n4.2.2 Download Instructions\nTo download the 2-day forecast for the forecast initialization date of one’s choice, one follows the instructions below. The downloaded file can be a NetCDF or KMZ file.\nGo to the URL: https://firesmoke.ca/forecasts/{Forecast ID}/{YYYYMMDD}{InitTime}/{File Type}\nWhere:\n\nYYYYMMDD is the date of choice.\nForecastID and InitTime are the chosen values as described in Table 4.2.\nFile Type is either dispersion.nc or dispersion.kmz for either the NetCDF file or KMZ file, respectively.\n\n\n\n\nTable 4.2: UBC Smoke Forecast Data Download Parameters.\n\n\n\n\n\nForecast ID\nSmoke Forecast Initialization (UTC)\n\n\n\n\nBSC00CA12-01\n08\n\n\nBSC06CA12-01\n14\n\n\nBSC12CA12-01\n20\n\n\nBSC18CA12-01\n02\n\n\n\n\n\n\n\n4.2.2.1 Download Example\nLet’s try downloading the forecast for January 1, 2024 where the weather forecast is initiated at 00:00:00 UTC and the smoke forecast is initialized at 08:00:00 UTC by navigating to the corresponding URL.\n\n\nCode\nforecast_id = \"BSC00CA12-01\"\nyyyymmdd = \"20210304\"\ninit_time = \"08\"\n\nurl = (\n    f\"https://firesmoke.ca/forecasts/{forecast_id}/{yyyymmdd}{init_time}/dispersion.nc\"\n)\n\nprint(f\"Navigate to this URL in your browser: {url}\")\n\n\nNavigate to this URL in your browser: https://firesmoke.ca/forecasts/BSC00CA12-01/2021030408/dispersion.nc",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Data Source</span>"
    ]
  },
  {
    "objectID": "data_source.html#the-netcdf-file",
    "href": "data_source.html#the-netcdf-file",
    "title": "4  The Data Source",
    "section": "4.3 The NetCDF File",
    "text": "4.3 The NetCDF File\nNext, let’s look at what is within the NetCDF file located at the URL in our previous example.\n\n4.3.1 File Preview\n\nWe load dispersion.nc using xarray, which provides a preview of the file.\n\n\nCode\nimport xarray as xr\n\nds = xr.open_dataset(\"data_notebooks/data_source/dispersion.nc\")\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (TSTEP: 51, VAR: 1, DATE-TIME: 2, LAY: 1, ROW: 381, COL: 1041)\nDimensions without coordinates: TSTEP, VAR, DATE-TIME, LAY, ROW, COL\nData variables:\n    TFLAG    (TSTEP, VAR, DATE-TIME) int32 ...\n    PM25     (TSTEP, LAY, ROW, COL) float32 ...\nAttributes: (12/33)\n    IOAPI_VERSION:  $Id: @(#) ioapi library version 3.0 $                    ...\n    EXEC_ID:        ????????????????                                         ...\n    FTYPE:          1\n    CDATE:          2021063\n    CTIME:          101914\n    WDATE:          2021063\n    ...             ...\n    VGLVLS:         [10.  0.]\n    GDNAM:          HYSPLIT CONC    \n    UPNAM:          hysplit2netCDF  \n    VAR-LIST:       PM25            \n    FILEDESC:       Hysplit Concentration Model Output                       ...\n    HISTORY:        xarray.DatasetDimensions:TSTEP: 51VAR: 1DATE-TIME: 2LAY: 1ROW: 381COL: 1041Coordinates: (0)Data variables: (2)TFLAG(TSTEP, VAR, DATE-TIME)int32...units :&lt;YYYYDDD,HHMMSS&gt;long_name :TFLAG           var_desc :Timestep-valid flags:  (1) YYYYDDD or (2) HHMMSS                                [102 values with dtype=int32]PM25(TSTEP, LAY, ROW, COL)float32...long_name :PM25            units :ug/m^3          var_desc :PM25                                                                            [20227671 values with dtype=float32]Indexes: (0)Attributes: (33)IOAPI_VERSION :$Id: @(#) ioapi library version 3.0 $                                           EXEC_ID :????????????????                                                                FTYPE :1CDATE :2021063CTIME :101914WDATE :2021063WTIME :101914SDATE :2021063STIME :90000TSTEP :10000NTHIK :1NCOLS :1041NROWS :381NLAYS :1NVARS :1GDTYP :1P_ALP :0.0P_BET :0.0P_GAM :0.0XCENT :-104.0YCENT :51.0XORIG :-156.0YORIG :32.0XCELL :0.10000000149011612YCELL :0.10000000149011612VGTYP :5VGTOP :-9999.0VGLVLS :[10.  0.]GDNAM :HYSPLIT CONC    UPNAM :hysplit2netCDF  VAR-LIST :PM25            FILEDESC :Hysplit Concentration Model Output                                              lat-lon coordinate system                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       HISTORY :\n\n\n\n\n4.3.2 File Attributes\ndispersion.nc contains the following attributes. Note that for all files across forecast IDs, they have the same dimension and variable names:\n\n4.3.2.1 Dimensions:\nThe dimensions described in Table 4.3 determine on which indicies we may index our variables.\n\n\n\nTable 4.3: Description of Dimensions for Indexing Data in NetCDF Files\n\n\n\n\n\n\n\n\n\n\nDimension\nSize\nDescription\n\n\n\n\nTSTEP\n51\nThis dimension represents the number of time steps in the file. Each file has 51 hours represented.\n\n\nVAR\n1\nThis dimension is a placeholder for the variables in the file.\n\n\nDATE-TIME\n2\nThis dimension stores the date and time information for each time step.\n\n\nLAY\n1\nThis dimension represents the number of layers in the file, which is 1 in this case.\n\n\nROW\n381\nThis dimension represents the number of rows in the spatial grid.\n\n\nCOL\n1041\nThis dimension represents the number of columns in the spatial grid.\n\n\n\n\n\n\n\n\n4.3.2.2 Variables:\nThe variables described in Table 4.4 contain the data in question that we would like to extract.\n\n\n\nTable 4.4: Description of Variables in NetCDF Files\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDimensions\nData Type\nDescription\n\n\n\n\nTFLAG\nTSTEP, VAR, DATE-TIME\nint32\nThis variable stores the date and time of each time step.\n\n\nPM25\nTSTEP, LAY, ROW, COL\nfloat32\nThis variable contains the concentration of particulate matter (PM2.5) for each time step, layer, row, and column in the spatial grid.\n\n\n\n\n\n\n\n\n4.3.2.3 Attributes\nOf the 33 available attributes we use the ones shown in Table 4.5: \n\n\n\nTable 4.5: Description of Attributes in dispersion.nc\n\n\n\n\n\n\n\n\n\n\nAttribute\nValue\nDescription\n\n\n\n\nCDATE\n2021063\nThe creation date of the dataset, in YYYYDDD format.\n\n\nCTIME\n101914\nThe creation time of the dataset, in HHMMSS format.\n\n\nWDATE\n2021063\nThe date for which the weather forecast is initiated, in YYYYDDD format.\n\n\nWTIME\n101914\nThe time for which the weather forecast is initiated, in HHMMSS format.\n\n\nSDATE\n2021063\nThe date for which the smoke forecast is initiated,in YYYYDDD format.\n\n\nSTIME\n90000\nThe time for which the weather forecast is initiated, in HHMMSS format.\n\n\nNCOLS\n1041\nThe number of columns in the spatial grid.\n\n\nNROWS\n381\nThe number of rows in the spatial grid.\n\n\nXORIG\n-156.0\nThe origin (starting point) of the grid in the x-direction.\n\n\nYORIG\n32.0\nThe origin (starting point) of the grid in the y-direction.\n\n\nXCELL\n0.10000000149011612\nThe cell size in the x-direction.\n\n\nYCELL\n0.10000000149011612\nThe cell size in the y-direction.\n\n\n\n\n\n\nLet’s look closer at what exactly is within one NetCDF file in the following demo.\n\n\n\n4.3.3 NetCDF Visualization Demo\n\nIn this demo we load a different dispersion.nc file and explore how to visualize the data within the file.\n\n4.3.3.1 Accessing the File\nWe use the forecast for March 4, 2021 where the weather forecast is initiated at 00:00:00 UTC and the smoke forecast is initialized at 08:00:00 UTC. You can download this file by navigating to the URL below.\n\n\nCode\nforecast_id = \"BSC00CA12-01\"\nyyyymmdd = \"20210304\"\ninit_time = \"08\"\n\nurl = (\n    f\"https://firesmoke.ca/forecasts/{forecast_id}/{yyyymmdd}{init_time}/dispersion.nc\"\n)\n\nprint(f\"Download this file from URL: {url}\")\n\n# import urllib.request\n# urllib.request.urlretrieve(url, \"dispersion.nc\")\n\n\nDownload this file from URL: https://firesmoke.ca/forecasts/BSC00CA12-01/2021030408/dispersion.nc\n\n\n\n4.3.3.1.1 Opening the File\nWe use xarray to open the NetCDF file and preview it.\n\n\nCode\nimport xarray as xr\n\nds = xr.open_dataset(\"dispersion.nc\")\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (TSTEP: 51, VAR: 1, DATE-TIME: 2, LAY: 1, ROW: 381, COL: 1041)\nDimensions without coordinates: TSTEP, VAR, DATE-TIME, LAY, ROW, COL\nData variables:\n    TFLAG    (TSTEP, VAR, DATE-TIME) int32 ...\n    PM25     (TSTEP, LAY, ROW, COL) float32 ...\nAttributes: (12/33)\n    IOAPI_VERSION:  $Id: @(#) ioapi library version 3.0 $                    ...\n    EXEC_ID:        ????????????????                                         ...\n    FTYPE:          1\n    CDATE:          2021063\n    CTIME:          101914\n    WDATE:          2021063\n    ...             ...\n    VGLVLS:         [10.  0.]\n    GDNAM:          HYSPLIT CONC    \n    UPNAM:          hysplit2netCDF  \n    VAR-LIST:       PM25            \n    FILEDESC:       Hysplit Concentration Model Output                       ...\n    HISTORY:        xarray.DatasetDimensions:TSTEP: 51VAR: 1DATE-TIME: 2LAY: 1ROW: 381COL: 1041Coordinates: (0)Data variables: (2)TFLAG(TSTEP, VAR, DATE-TIME)int32...units :&lt;YYYYDDD,HHMMSS&gt;long_name :TFLAG           var_desc :Timestep-valid flags:  (1) YYYYDDD or (2) HHMMSS                                [102 values with dtype=int32]PM25(TSTEP, LAY, ROW, COL)float32...long_name :PM25            units :ug/m^3          var_desc :PM25                                                                            [20227671 values with dtype=float32]Indexes: (0)Attributes: (33)IOAPI_VERSION :$Id: @(#) ioapi library version 3.0 $                                           EXEC_ID :????????????????                                                                FTYPE :1CDATE :2021063CTIME :101914WDATE :2021063WTIME :101914SDATE :2021063STIME :90000TSTEP :10000NTHIK :1NCOLS :1041NROWS :381NLAYS :1NVARS :1GDTYP :1P_ALP :0.0P_BET :0.0P_GAM :0.0XCENT :-104.0YCENT :51.0XORIG :-156.0YORIG :32.0XCELL :0.10000000149011612YCELL :0.10000000149011612VGTYP :5VGTOP :-9999.0VGLVLS :[10.  0.]GDNAM :HYSPLIT CONC    UPNAM :hysplit2netCDF  VAR-LIST :PM25            FILEDESC :Hysplit Concentration Model Output                                              lat-lon coordinate system                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       HISTORY :\n\n\n\n\n\n4.3.3.2 Using the Data\n\n4.3.3.2.1 Accessing Arrays\nThe data we are interested in is the PM2.5 values. Let’s use xarray to get the array in the PM25 variable.\n\n\nCode\nds[\"PM25\"]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'PM25' (TSTEP: 51, LAY: 1, ROW: 381, COL: 1041)&gt;\n[20227671 values with dtype=float32]\nDimensions without coordinates: TSTEP, LAY, ROW, COL\nAttributes:\n    long_name:  PM25            \n    units:      ug/m^3          \n    var_desc:   PM25                                                         ...xarray.DataArray'PM25'TSTEP: 51LAY: 1ROW: 381COL: 1041...[20227671 values with dtype=float32]Coordinates: (0)Indexes: (0)Attributes: (3)long_name :PM25            units :ug/m^3          var_desc :PM25                                                                            \n\n\nThe dimensions of the PM25 data array are composed of TSTEP, LAY, ROW, and COL. We do not need the LAY dimension, so let’s use numpy to drop it.\n\n\nCode\nimport numpy as np\n\n1ds_pm25_vals = ds[\"PM25\"].values\nprint(f'The shape of the data contained in PM25 variable is: {np.shape(ds_pm25_vals)}')\n\n2ds_pm25_vals = np.squeeze(ds_pm25_vals)\nprint(f'After squeezing, the shape is: {np.shape(ds_pm25_vals)}')\n\n\n\n1\n\nUse .values to get the four dimensional array.\n\n2\n\nUse np.squeeze to drop the LAY axis\n\n\n\n\nThe shape of the data contained in PM25 variable is: (51, 1, 381, 1041)\nAfter squeezing, the shape is: (51, 381, 1041)\n\n\nWe now have ds_pm25_vals. Next, let’s select a time step and visualize the data.\n\n\n4.3.3.2.2 Visualize Array in matplotlib\nWe can index time step 10 and use matplotlib to visualize the timestep\n\n\nCode\nimport matplotlib.pyplot as plt\n\n1tstep = 10\nsmoke_at_tstep = ds_pm25_vals[tstep, :, :]\n\nmy_fig, my_plt = plt.subplots(figsize=(15, 6))\n\n2my_norm = \"log\"\n3my_aspect = 'auto'\n4my_origin = 'lower'\n5my_cmap = 'viridis'\n \n6plot = my_plt.imshow(smoke_at_tstep, norm=my_norm, aspect=my_aspect, origin=my_origin, cmap=my_cmap)\n\n7my_fig.colorbar(plot, location='right', label='ug/m^3')\n\n8my_fig.suptitle('Ground level concentration of PM2.5 microns and smaller')\n\n9my_plt.set_title(f'Timestep {tstep}')\n\n10plt.show()\n\n\n\n1\n\nIndex ds_pm25_vals at TSTEP = 10, selecting all ROWs and COLs.\n\n2\n\nColor PM25 values on a log scale, since values are small.\n\n3\n\nEnsure the aspect ratio of our plot fits all data, matplotlib can do this automatically.\n\n4\n\nTell matplotlib our origin is the lower-left corner.\n\n5\n\nSelect a colormap for our plot and draw the color bar on the right.\n\n6\n\nCreate our plot using imshow.\n\n7\n\nAdd a colorbar to our figure, based on the plot we just made above.\n\n8\n\nSet title of our figure.\n\n9\n\nSet title of our plot as the timestamp of our data.\n\n10\n\nShow the resulting visualization.\n\n\n\n\n\n\n\n\n\n\n\nNotice there are no axis labels or metadata presented here. Next we will show how to use the metadata in dispersion.nc so the data is actually interpretable.\n\n\n\n4.3.3.3 Incorporating Metadata to Visualization via Coordinates\n\n4.3.3.3.1 Latitude and Longitude Coordinates\ndispersion.nc includes attributes to generate the latitude and longitude values on the grid defined by NCOLS and NROWS. We use this grid to match each data point in the PM25 variable to a lat/lon coordinate.\n\n\nCode\nxorig = ds.XORIG\nyorig = ds.YORIG\nxcell = ds.XCELL\nycell = ds.YCELL\nncols = ds.NCOLS\nnrows = ds.NROWS\n\nlongitude = np.linspace(xorig, xorig + xcell * (ncols - 1), ncols)\nlatitude = np.linspace(yorig, yorig + ycell * (nrows - 1), nrows)\n\nprint(\"Size of longitude & latitude arrays:\")\nprint(f'np.size(longitude) = {np.size(longitude)}')\nprint(f'np.size(latitude) = {np.size(latitude)}\\n')\nprint(\"Min & Max of longitude and latitude arrays:\")\nprint(f'longitude: min = {np.min(longitude)}, max = {np.max(longitude)}')\nprint(f'latitude: min = {np.min(latitude)}, max = {np.max(latitude)}')\n\n\nSize of longitude & latitude arrays:\nnp.size(longitude) = 1041\nnp.size(latitude) = 381\n\nMin & Max of longitude and latitude arrays:\nlongitude: min = -156.0, max = -51.999998450279236\nlatitude: min = 32.0, max = 70.00000056624413\n\n\nxarray allows us to create coordinates, which maps variable values to a value of our choice. In this case, we create coordinates mapping PM25 values to a latitude and longitude value.\n\n\nCode\n1ds.coords['lat'] = ('ROW', latitude)\nds.coords['lon'] = ('COL', longitude)\n\n2ds = ds.swap_dims({'COL': 'lon', 'ROW': 'lat'})\n\nds\n\n\n\n1\n\nCreate coordinates for latitude and longitude.\n\n2\n\nReplace COL and ROW dimensions with newly calculated longitude and latitude coordinates.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (TSTEP: 51, VAR: 1, DATE-TIME: 2, LAY: 1, lat: 381, lon: 1041)\nCoordinates:\n  * lat      (lat) float64 32.0 32.1 32.2 32.3 32.4 ... 69.6 69.7 69.8 69.9 70.0\n  * lon      (lon) float64 -156.0 -155.9 -155.8 -155.7 ... -52.2 -52.1 -52.0\nDimensions without coordinates: TSTEP, VAR, DATE-TIME, LAY\nData variables:\n    TFLAG    (TSTEP, VAR, DATE-TIME) int32 ...\n    PM25     (TSTEP, LAY, lat, lon) float32 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0\nAttributes: (12/33)\n    IOAPI_VERSION:  $Id: @(#) ioapi library version 3.0 $                    ...\n    EXEC_ID:        ????????????????                                         ...\n    FTYPE:          1\n    CDATE:          2021063\n    CTIME:          101914\n    WDATE:          2021063\n    ...             ...\n    VGLVLS:         [10.  0.]\n    GDNAM:          HYSPLIT CONC    \n    UPNAM:          hysplit2netCDF  \n    VAR-LIST:       PM25            \n    FILEDESC:       Hysplit Concentration Model Output                       ...\n    HISTORY:        xarray.DatasetDimensions:TSTEP: 51VAR: 1DATE-TIME: 2LAY: 1lat: 381lon: 1041Coordinates: (2)lat(lat)float6432.0 32.1 32.2 ... 69.8 69.9 70.0array([32.      , 32.1     , 32.2     , ..., 69.800001, 69.900001, 70.000001])lon(lon)float64-156.0 -155.9 ... -52.1 -52.0array([-156.      , -155.9     , -155.8     , ...,  -52.199998,  -52.099998,\n        -51.999998])Data variables: (2)TFLAG(TSTEP, VAR, DATE-TIME)int32...units :&lt;YYYYDDD,HHMMSS&gt;long_name :TFLAG           var_desc :Timestep-valid flags:  (1) YYYYDDD or (2) HHMMSS                                [102 values with dtype=int32]PM25(TSTEP, LAY, lat, lon)float320.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0long_name :PM25            units :ug/m^3          var_desc :PM25                                                                            array([[[[0.000000e+00, ..., 0.000000e+00],\n         ...,\n         [0.000000e+00, ..., 0.000000e+00]]],\n\n\n       ...,\n\n\n       [[[0.000000e+00, ..., 2.804227e-11],\n         ...,\n         [0.000000e+00, ..., 0.000000e+00]]]], dtype=float32)Indexes: (2)latPandasIndexPandasIndex(Index([              32.0, 32.100000001490116,  32.20000000298023,\n        32.30000000447035, 32.400000005960464,  32.50000000745058,\n         32.6000000089407,  32.70000001043081,  32.80000001192093,\n       32.900000013411045,\n       ...\n        69.10000055283308,   69.2000005543232,  69.30000055581331,\n        69.40000055730343,  69.50000055879354,  69.60000056028366,\n        69.70000056177378,   69.8000005632639,  69.90000056475401,\n        70.00000056624413],\n      dtype='float64', name='lat', length=381))lonPandasIndexPandasIndex(Index([             -156.0, -155.89999999850988, -155.79999999701977,\n       -155.69999999552965, -155.59999999403954, -155.49999999254942,\n        -155.3999999910593,  -155.2999999895692, -155.19999998807907,\n       -155.09999998658895,\n       ...\n        -52.89999846369028, -52.799998462200165,  -52.69999846071005,\n        -52.59999845921993, -52.499998457729816,   -52.3999984562397,\n       -52.299998454749584,  -52.19999845325947,  -52.09999845176935,\n       -51.999998450279236],\n      dtype='float64', name='lon', length=1041))Attributes: (33)IOAPI_VERSION :$Id: @(#) ioapi library version 3.0 $                                           EXEC_ID :????????????????                                                                FTYPE :1CDATE :2021063CTIME :101914WDATE :2021063WTIME :101914SDATE :2021063STIME :90000TSTEP :10000NTHIK :1NCOLS :1041NROWS :381NLAYS :1NVARS :1GDTYP :1P_ALP :0.0P_BET :0.0P_GAM :0.0XCENT :-104.0YCENT :51.0XORIG :-156.0YORIG :32.0XCELL :0.10000000149011612YCELL :0.10000000149011612VGTYP :5VGTOP :-9999.0VGLVLS :[10.  0.]GDNAM :HYSPLIT CONC    UPNAM :hysplit2netCDF  VAR-LIST :PM25            FILEDESC :Hysplit Concentration Model Output                                              lat-lon coordinate system                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       HISTORY :\n\n\nNow let’s move on to incorporating time stamp metadata.\n\n\n4.3.3.3.2 Time Coordinates\nRecall, there is a TFLAG variable in dispersion.nc.\n\n\nCode\nds['TFLAG']\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'TFLAG' (TSTEP: 51, VAR: 1, DATE-TIME: 2)&gt;\n[102 values with dtype=int32]\nDimensions without coordinates: TSTEP, VAR, DATE-TIME\nAttributes:\n    units:      &lt;YYYYDDD,HHMMSS&gt;\n    long_name:  TFLAG           \n    var_desc:   Timestep-valid flags:  (1) YYYYDDD or (2) HHMMSS             ...xarray.DataArray'TFLAG'TSTEP: 51VAR: 1DATE-TIME: 2...[102 values with dtype=int32]Coordinates: (0)Indexes: (0)Attributes: (3)units :&lt;YYYYDDD,HHMMSS&gt;long_name :TFLAG           var_desc :Timestep-valid flags:  (1) YYYYDDD or (2) HHMMSS                                \n\n\nThe earliest and latest TFLAGs look like the following:\n\n\nCode\nprint(f\"Earliest available TFLAG is {ds['TFLAG'].values[0][0]}\")\nprint(f\"Latest available TFLAG is {ds['TFLAG'].values[-1][0]}\")\n\n\nEarliest available TFLAG is [2021063   90000]\nLatest available TFLAG is [2021065  110000]\n\n\nThis time flags require processing to be immediately legible. Let’s write a function to process the time flag accordingly. We use the datetime library.\n\n\nCode\nimport datetime\n\ndef parse_tflag(tflag):\n    \"\"\"\n    Return the tflag as a datetime object\n    :param list tflag: a list of two int32, the 1st representing date and 2nd representing time\n    \"\"\"\n1    date = int(tflag[0])\n2    year = date // 1000\n3    day_of_year = date % 1000\n\n4    final_date = datetime.datetime(year, 1, 1) + datetime.timedelta(days=day_of_year - 1)\n\n5    time = int(tflag[1])\n6    hours = time // 10000\n7    minutes = (time % 10000) // 100\n8    seconds = time % 100\n\n9    full_datetime = datetime.datetime(year, final_date.month, final_date.day, hours, minutes, seconds)\n    return full_datetime\n\n\n\n1\n\nObtain year and day of year from tflag[0] (date).\n\n2\n\nExtract the year from the first 4 digits of tflag[0].\n\n3\n\nExtract the day of the year from the last 3 digits of tflag[0].\n\n4\n\nCreate a datetime object representing the date.\n\n5\n\nObtain hour, minutes, and seconds from tflag[1] (time).\n\n6\n\nExtract hours from the first 2 digits of tflag[1].\n\n7\n\nExtract minutes from the 3rd and 4th digits of tflag[1].\n\n8\n\nExtract seconds from the last 2 digits of tflag[1].\n\n9\n\nCreate the final datetime object with the extracted date and time components.\n\n\n\n\nNow we have datetime objects to represent the timeflag in a more legible and usable format.\n\n\nCode\nprint(f\"Earliest available TFLAG is {parse_tflag(ds['TFLAG'].values[0][0])}\")\nprint(f\"Latest available TFLAG is {parse_tflag(ds['TFLAG'].values[-1][0])}\")\n\n\nEarliest available TFLAG is 2021-03-04 09:00:00\nLatest available TFLAG is 2021-03-06 11:00:00\n\n\n\n\n4.3.3.3.3 Visualize Array in matplotlib\nLet’s visualize timestep 10 again, but now we can label the data using latitudes and longitudes, and the corresponding time flag.\n\n\nCode\n1tstep = 10\n2smoke_at_tstep = ds_pm25_vals[tstep, :, :]\n3tstep_tflag = parse_tflag(ds['TFLAG'].values[tstep][0])\n\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\n\n4my_fig, my_plt = plt.subplots(figsize=(15, 6), subplot_kw=dict(projection=ccrs.PlateCarree()))\n\n5my_norm = \"log\"\n6my_extent = [np.min(longitude), np.max(longitude), np.min(latitude), np.max(latitude)]\n7my_aspect = 'auto'\n8my_origin = 'lower'\n9my_cmap = 'viridis'\n\nplot = my_plt.imshow(smoke_at_tstep, norm=my_norm, extent=my_extent, \n10          aspect=my_aspect, origin=my_origin, cmap=my_cmap)\n\n11my_plt.coastlines()\n\n12my_plt.gridlines(draw_labels=True)\n\n13my_fig.colorbar(plot, location='right', label='ug/m^3')\n\n14my_fig.supxlabel('Longitude')\n15my_fig.supylabel('Latitude')\n\n16my_fig.suptitle('Ground level concentration of PM2.5 microns and smaller')\n\n17my_plt.set_title(f'{tstep_tflag}')\n\n18plt.show()\n\n\n\n1\n\nDefine the time step.\n\n2\n\nExtract the PM2.5 data for the specified time step.\n\n3\n\nParse the time flag for the specified time step.\n\n4\n\nInitialize a figure and plot with a specific projection.\n\n5\n\nSet the normalization for PM2.5 values to a logarithmic scale.\n\n6\n\nDefine the extent of the plot based on the longitude and latitude range.\n\n7\n\nSet the aspect ratio of the plot to fit all data automatically.\n\n8\n\nSpecify the origin of the plot as the lower-left corner.\n\n9\n\nChoose a colormap for the plot.\n\n10\n\nCreate the plot using imshow with the specified parameters.\n\n11\n\nDraw coastlines on the plot.\n\n12\n\nDraw latitude and longitude lines with labels.\n\n13\n\nAdd a colorbar to the figure based on the plot.\n\n14\n\nSet the x-axis label.\n\n15\n\nSet the y-axis label.\n\n16\n\nSet the title of the figure.\n\n17\n\nSet the title of the plot as the timestamp of the data.\n\n18\n\nDisplay the resulting visualization.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nds.close()\n\n\n\n\n\nNow that we understand how to load the data and metadata from the file and process it for visualization, let’s establish the data and metadata available to us across all NetCDF files.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Data Source</span>"
    ]
  },
  {
    "objectID": "data_source.html#sec-collection-info",
    "href": "data_source.html#sec-collection-info",
    "title": "4  The Data Source",
    "section": "4.4 Information Across all NetCDF Files",
    "text": "4.4 Information Across all NetCDF Files\nKnowing what is within one NetCDF file as well as the date range for which we can download them, let’s establish the metadata associated with the NetCDF files as a collection.\n\n4.4.1 Disk Size\nFor the time ranges we cover, Table 6.1 shows how large the set of files per forecast ID are.\n\n\n\nTable 4.6: File Sizes and Counts for Each Forecast ID within the Specified Date Range\n\n\n\n\n\n\n\n\n\n\n\nForecast ID\nDate Range\nSize\nFile Count\n\n\n\n\nBSC00CA12-01\nMarch 4, 2021 - June 27, 2024\n84G\n1077\n\n\nBSC06CA12-01\nMarch 4, 2021 - June 27, 2024\n78G\n1022\n\n\nBSC12CA12-01\nMarch 3, 2021 - June 27, 2024\n79G\n1022\n\n\nBSC18CA12-01\nMarch 4, 2021 - June 27, 2024\n79G\n1023\n\n\nTotal\n\n320G\n4144\n\n\n\n\n\n\n\n\n4.4.2 Temporal Data Availability\n\nHere we show for what temporal range there is PM25 data available from the NetCDF files as a collection.\n\n4.4.2.1 Loading Files\nWe have downloaded all NetCDF files available up to June 27, 2022.\nLet’s demonstrate the staggered nature of the forecasts by opening up the files for March 2, 2024 to March 4, 2022. We use the parse_tflag function.\n\n\nCode\n1files_dir = 'dispersion_files'\n2ids = [\"BSC18CA12-01\", \"BSC00CA12-01\", \"BSC06CA12-01\", \"BSC12CA12-01\"]\n3file_names = ['dispersion_20220302.nc', 'dispersion_20220303.nc', 'dispersion_20220304.nc']\n\n\n\n1\n\nDefine the directory location of our files.\n\n2\n\nList of the forecast IDs we use.\n\n3\n\nfile_names is each file we will open per forecast ID next.\n\n\n\n\n\n\nCode\nimport xarray as xr\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\ncolors = ['pink', 'orange', 'green']\n\n1for id_ in ids:\n    for i, f in enumerate(file_names):\n2        curr_file = f'{files_dir}/{id_[3:5]}/{f}'\n3        ds = xr.open_dataset(curr_file)\n        tflags = ds['TFLAG'].values\n        \n4        earliest_time = parse_tflag(tflags[0][0])\n        latest_time = parse_tflag(tflags[-1][0])\n        \n5        bar = ax.barh(id_, latest_time - earliest_time, left=earliest_time, height=0.3, color = colors[i])\n\nax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d %H:%M\"))\nax.xaxis.set_major_locator(mdates.HourLocator(interval=6))\nfig.autofmt_xdate()\n\nax.set_xlabel(\"Time\")\nax.set_ylabel(\"Forecast ID\")\nax.set_title(\"Time Range Covered by `dispersion.nc` per Forecast ID, for 03/02/22-03/05/22\")\n\nfrom matplotlib.lines import Line2D\nlegend_elements = []\nfor i in range(3):\n    legend_elements.append(Line2D([0], [0], linewidth=4, color=colors[i], \n                                  label=file_names[i], markersize=15))\nax.legend(handles=legend_elements, loc='center')\nplt.show()\n\n\n\n1\n\nOpen each file for each forecast ID.\n\n2\n\nCreate the path string.\n\n3\n\nOpen the file with xarray and get the TFLAG values.\n\n4\n\nGet the earliest and latest available time flags.\n\n5\n\nPlot the time range represented with the time flags as a horizontal bar.\n\n\n\n\n\n\n\n\n\n\n\nThe 6 loaded files cover the time ranges shown above.\nNotice, for any given hour, there can be various predictions available to choose from. For example, for 2022-03-04 01:00, we can find it represented in dispersion_20220303.nc across all forecast IDs.\nGiven that these predictions are forecasts, we run on the assumption that the earliest available predictions per file are the most accurate.\nTherefore, for example, if we want to choose the most accurate PM2.5 prediction for 2022-03-04 01:00 we would load the prediction with the BSC12CA12-01 forecast ID.\nFor further details on how we load the best prediction for every hour of the dataset, see Section 6.6.\n\n\nNow that we know what exactly is within a NetCDF file and across all the NetCDF files, we will continue to describe our data curation process. Next we describe how we load all of the data available from UBC onto our machine.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Data Source</span>"
    ]
  },
  {
    "objectID": "data_loading.html",
    "href": "data_loading.html",
    "title": "5  Data Loading",
    "section": "",
    "text": "5.1 Downloading Data Locally\nWe decided to download all the available files from the data source onto our data staging machine to process from there.\nWe created 4 directories for each forecast ID that UBC provides at the following directory on our machine:\nThe following shows our approaches to doing this and discusses how our approach evolved. Note the scripts below refer to varying directories, but through simple copying operations we stored the final downloaded files to the directories listed above.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Loading</span>"
    ]
  },
  {
    "objectID": "data_loading.html#downloading-data-locally",
    "href": "data_loading.html#downloading-data-locally",
    "title": "5  Data Loading",
    "section": "",
    "text": "/usr/sci/cedmav/data/firesmoke\n├── BSC00CA12-01\n├── BSC06CA12-01\n├── BSC12CA12-01\n├── BSC18CA12-01\n\n\n5.1.1 First Approach\nWe delineate our first approach by detailing our download script, which is available in it’s entirety in the side bar.\n\n\nCode\nimport wget\nimport pandas as pd\n\n1ids = [\"BSC18CA12-01\", \"BSC00CA12-01\", \"BSC06CA12-01\", \"BSC12CA12-01\"]\nstart_dates = [\"20210304\", \"20210304\", \"20210304\", \"20210303\"]\nend_dates = [\"20231016\", \"20240210\", \"20231016\", \"20231015\"]\ninit_times = [\"02\", \"08\", \"14\", \"20\"]\n\n2for i in zip(start_dates, end_dates, ids, init_times):\n    start_date = i[0]\n    end_date = i[1]\n    forecast_id = i[2]\n    init_time = i[3]\n\n    dates = pd.date_range(start=start_date, end=end_date)\n    dates = dates.strftime(\"%Y%m%d\").tolist()\n\n3    for date in dates:\n      url = (\n          \"https://firesmoke.ca/forecasts/\"\n          + forecast_id\n          + \"/\"\n          + date\n          + init_time\n          + \"/dispersion.nc\"\n      )\n4      directory = \"/Users/arleth/Mount/firesmoke/\" + forecast_id + \"/dispersion_\" + date + \".nc\"\n      wget.download(url, out=directory)\n\n\n\n1\n\nFirst, create 4 lists containing forecast IDs, the start and end dates we wish to index on, and the smoke forecast initiation times. We will loop through the 4 sets of parameters.\n\n2\n\nIn a for loop, we use pandas to create a list of every date from the start date and end date of the current iteration. We will loop through these dates next.\n\n3\n\nFor each date in the list, we create the url to download the file.\n\n4\n\nFinally, we use wget to download the contents at urlto directory. We append date to the file name so each file downloaded is identifiable by date.\n\n\n\n\nWe assumed that for all URLs, there was an available NetCDF file for download\nHowever, we realized that we downloaded either a NetCDF file or an HTML webpage. Using wget forcibly saved the contents at the URL into a NetCDF file.\nThis issue was not identified until after we visualized each hour of the data, and we noticed gaps and errors in our scripts to create visualizations. See Chapter 7 for further details on identifying these issues. For now, we show our modified approach to downloading the NetCDF files.\n\n\n5.1.2 Second Approach\nOur second approach is similar to the first, except we use requests, an HTTP client library that allows us to see the headers returned from the URL we query. The script is available in the side bar.\n\n\nCode\nimport requests\nimport pandas as pd\nfrom datetime import datetime\n\n1ids = [\"BSC18CA12-01\", \"BSC00CA12-01\", \"BSC06CA12-01\", \"BSC12CA12-01\"]\nstart_dates = [\"20210304\", \"20210304\", \"20210304\", \"20210303\"]\ntoday = datetime.now().strftime(\"%Y%m%d\")\ninit_times = [\"02\", \"08\", \"14\", \"20\"]\n\n2for i in zip(start_dates, ids, init_times):\n    start_date = i[0]\n    forecast_id = i[1]\n    init_time = i[2]\n\n    dates = pd.date_range(start=start_date, end=today)\n    dates = dates.strftime(\"%Y%m%d\").tolist()\n\n3    for date in dates:\n        url = (\n            \"https://firesmoke.ca/forecasts/\"\n            + forecast_id\n            + \"/\"\n            + date\n            + init_time\n            + \"/dispersion.nc\"\n        )\n4        directory = (\n            \"/usr/sci/scratch_nvme/arleth/basura_total/\"\n            + forecast_id\n            + \"/dispersion_\"\n            + date\n            + \".nc\"\n        )\n\n5        response = requests.get(url, stream=True)\n        header = response.headers\n        if (\n            \"Content-Type\" in header\n            and header[\"Content-Type\"] == \"application/octet-stream\"\n        ):\n6            with open(directory, mode=\"wb\") as file:\n                file.write(response.content)\n                print(f\"Downloaded file {directory}\")\n        else:\n            print(header[\"Content-Type\"])\n\n\n\n1\n\nFirst, create 3 lists containing forecast IDs, the start dates we wish to index on, and the smoke forecast initiation times . Notice we define a variable today, this allows us to run this script and query all URLs up to today’s date. Note we ran up to June 27, 2024 for now. We will loop through these sets of parameters.\n\n2\n\nIn a for loop, we use pandas to create a list of every date from the start date and end date of the current iteration. We will loop through these dates next.\n\n3\n\nFor each date in the list, we create the url to download the file.\n\n4\n\nDefine directory, the directory and file name to save the file to.\n\n5\n\nWe use requests to get the HTTP header at url. We inspect the Content-Type and if it is application/octet-stream, we download the file. We confirmed that a URL with a NetCDF file had this content type header.\n\n6\n\nWe write the content to directory, else we print the content header out to check what it is.\n\n\n\n\nThis approach yielded the results we expected, we downloaded only NetCDF files. We had failed downloads which appeared during conversion as described in Chapter 6. We assumed those files were unavailable from UBC which we later confirmed as described in Chapter 7.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Loading</span>"
    ]
  },
  {
    "objectID": "data_conversion.html",
    "href": "data_conversion.html",
    "title": "6  Data Conversion",
    "section": "",
    "text": "6.1 On Data Validation\nWe decided to perform data validation after conversion to the IDX file format. However, we realized that performing data validation both before and after conversion would be best. This is explored further in Chapter 7.\nFor now, the reader should understand that data validation of the NetCDF files is different from data validation of the IDX file. In this chapter, we use the assumption that the NetCDF files we can open have complete and uncorrupted data for conversion to IDX.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Conversion</span>"
    ]
  },
  {
    "objectID": "data_conversion.html#overview",
    "href": "data_conversion.html#overview",
    "title": "6  Data Conversion",
    "section": "6.2 Overview",
    "text": "6.2 Overview\nFor all our conversion script versions, the same general process is followed:\n\nCheck which NetCDF files were successfully downloaded from the data source by attempting to open each downloaded file with xarray.\nObtain a subset of data from the files to create a dataset of chronological, hour by hour, data.\nSave this time series data to an IDX file using the OpenVisus framework.\n\nWe will describe the latest version of our conversion, version 4. Throughout, we will explain how previous attempts were deficient.\nTo see previous attemps in their entirety, refer to the side bar. Please note that the previous scripts were working scripts, therefore they may be incomplete.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Conversion</span>"
    ]
  },
  {
    "objectID": "data_conversion.html#setting-system-directories",
    "href": "data_conversion.html#setting-system-directories",
    "title": "6  Data Conversion",
    "section": "6.3 Setting System Directories",
    "text": "6.3 Setting System Directories\nFirst we set the directory paths we want to use during the conversion process, which is to our 4 directories of NetCDF files for each forecast ID.\n\n\nCode\n1firesmoke_dir = \"/usr/sci/cedmav/data/firesmoke\"\nidx_dir = \"/usr/sci/scratch_nvme/arleth/idx/firesmoke\"\n\n2ids = [\"BSC18CA12-01\", \"BSC00CA12-01\", \"BSC06CA12-01\", \"BSC12CA12-01\"]\nstart_dates = [\"20210304\", \"20210304\", \"20210304\", \"20210303\"]\nend_dates = [\"20240627\", \"20240627\", \"20240627\", \"20240627\"]\n\n\n\n1\n\nEstablish the directory where all forecast ID NetCDFs are stored and where to save our IDX file on the ‘atlantis’ machine.\n\n2\n\nDefine the forecast IDs and dates we will loop over.\n\n\n\n\n\n6.3.1 Rationale and Future Improvements\n\n6.3.1.1 Data Usage\nIn versions 1 and 2 of our conversion attempt, we did not use all four sets of forecast ID files. We only used BSC12CA12-01 files to compile a single dataset. We learned that by not using all four sets of data, the dataset we created was less accurate. See Chapter 7 for further details.\nTherefore we decided to use all four datasets, see Section 6.6 for details. We elect to use dates up to June 27, 2024 as this was the last time we ran our scripts. We have yet to address the issue of how to keep the IDX file constantly up to date with data available up to the present day.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Conversion</span>"
    ]
  },
  {
    "objectID": "data_conversion.html#checking-the-netcdf-files",
    "href": "data_conversion.html#checking-the-netcdf-files",
    "title": "6  Data Conversion",
    "section": "6.4 Checking the NetCDF Files",
    "text": "6.4 Checking the NetCDF Files\nRecall we downloaded all NetCDF files available from UBC onto our machine in their respective directories as follows:\n/usr/sci/cedmav/data/firesmoke\n├── BSC00CA12-01\n├── BSC06CA12-01\n├── BSC12CA12-01\n├── BSC18CA12-01\nHere, we identify which NetCDF files for each forecast ID successfully open with xarray and store them in a dictionary.\nWe also confirm the following conditions we established in Chapter 4 by using dictionaries to track the max values and unique values of these attributes across all files:\n\nAll files across all four forecast IDs have the same NROWS, XORIG, YORIG, XCELL, YCELL values.\nSome files have either NCOLS = 1041 or NCOLS = 1081, but always NROWS = 381.\n\n\n\nCode\nimport os\nimport xarray as xr\nimport numpy as np\nimport tqdm\n\n1successful_files = {id_: [] for id_ in ids}\n\n2max_ncols = {id_: 0 for id_ in ids}\nmax_nrows = {id_: 0 for id_ in ids}\n3ncols = {id_: set() for id_ in ids}\nnrows = {id_: set() for id_ in ids}\n\n4max_grid_x = {id_: {\"xorig\": 0.0, \"xcell\": 0.0} for id_ in ids}\nmax_grid_y = {id_: {\"yorig\": 0.0, \"ycell\": 0.0} for id_ in ids}\n5xorigs = {id_: set() for id_ in ids}\nxcells = {id_: set() for id_ in ids}\nyorigs = {id_: set() for id_ in ids}\nycells = {id_: set() for id_ in ids}\n\n6for id_ in ids:\n7    file_names = os.listdir(f\"{firesmoke_dir}/{id_}/\")\n\n8    for file in tqdm(file_names):\n9        path = f\"{firesmoke_dir}/{id_}/{file}\"\n\n10        try:\n            ds = xr.open_dataset(path)\n\n11            successful_files[id_].append(file)\n\n12            max_ncols[id_] = max(max_ncols[id_], ds.NCOLS)\n            max_nrows[id_] = max(max_nrows[id_], ds.NROWS)\n            max_grid_x[id_][\"xorig\"] = max(max_grid_x[id_][\"xorig\"], ds.XORIG, key=abs)\n            max_grid_y[id_][\"yorig\"] = max(max_grid_y[id_][\"yorig\"], ds.YORIG, key=abs)\n            max_grid_x[id_][\"xcell\"] = max(max_grid_x[id_][\"xcell\"], ds.XCELL, key=abs)\n            max_grid_y[id_][\"ycell\"] = max(\n                max_grid_y[id_][\"ycell\"], ds.YCELL, key=abs\n            )\n\n13            ncols[id_].add(ds.NCOLS)\n            nrows[id_].add(ds.NROWS)\n            xorigs[id_].add(ds.XORIG)\n            yorigs[id_].add(ds.YORIG)\n            xcells[id_].add(ds.XCELL)\n            ycells[id_].add(ds.YCELL)\n\n14        except:\n            continue\n\n15for id_ in successful_files:\n    successful_files[id_] = np.sort(successful_files[id_]).tolist()\n\n\n\n1\n\nInitialize a dictionary to hold an empty list for each forecast ID. We update it with the file names that successfully open under the forecast ID directory.\n\n2\n\nInitialize dictionaries to hold an integer for each forecast ID. We update it to hold the maximum NCOLS/NROWS value available within forecast ID’s set of NetCDF files.\n\n3\n\nInitialize dictionaries to hold a set for each forecast ID. We update the set to hold all the unique NCOLS/NROWS values available within the forecast ID’s set of NetCDF files.\n\n4\n\nInitialize dictionaries to hold a dictionary of xorig/yorig and xcell/ycell values for each forecast ID. We update it to hold the maximum xorig/yorig and xcell/ycell pairs available within the forecast ID’s set of NetCDF files.\n\n5\n\nInitialize dictionaries to track unique xorig/yorig and xcell/ycell values.\n\n6\n\nFor each forecast ID, we populate the dictionaries above.\n\n7\n\nObtain a list of file names under the directory for id_. We loop through each file next.\n\n8\n\nBegin loop over each file. Note tqdm is just an accessory for generating a visible status bar in our Jupyter Notebook.\n\n9\n\nObtain absolute path name to current file.\n\n10\n\nHere we use a try statement since opening the file with xarray may lead to an error. except allows us to catch the exception accordingly and continue trying to open each file.\n\n11\n\nAt this line, the file opened without issue in xarray, so append this file name to the id_ list in the successful_files dictionary.\n\n12\n\nUse max to save the largest values in our dictionaries accordingly.\n\n13\n\nUpdate the dictionaries of sets with the file’s attributes, to ensure we catch all unique values.\n\n14\n\nIf the file did not open during the try continue to the next file.\n\n15\n\nSort the lists of successfully opened files by name, so they are in chronological order.\n\n\n\n\nThe following shows the information gathered:\n\nBSC18CA12-01BSC00CA12-01BSC06CA12-01BSC12CA12-01\n\n\ndataset: BSC18CA12-01\nNumber of successful files: 1010\nMax cell sizes: max_ncols = 1081 and max_nrows = 381\nMax xorig & xcell: {'xorig': -160.0, 'xcell': 0.10000000149011612}\nMax yorig & ycell: {'yorig': 32.0, 'ycell': 0.10000000149011612}\nncols: {1081, 1041}\nnrows: {381}\nxorigs: {-160.0, -156.0}\nyorigs: {32.0}\nxcells: {0.10000000149011612}\nycells: {0.10000000149011612}\n\n\ndataset: BSC00CA12-01\nNumber of successful files: 1067\nMax cell sizes: max_ncols = 1081 and max_nrows = 381\nMax xorig & xcell: {'xorig': -160.0, 'xcell': 0.10000000149011612}\nMax yorig & ycell: {'yorig': 32.0, 'ycell': 0.10000000149011612}\nncols: {1081, 1041}\nnrows: {381}\nxorigs: {-160.0, -156.0}\nyorigs: {32.0}\nxcells: {0.10000000149011612}\nycells: {0.10000000149011612}\n\n\ndataset: BSC06CA12-01\nNumber of successful files: 997\nMax cell sizes: max_ncols = 1081 and max_nrows = 381\nMax xorig & xcell: {'xorig': -160.0, 'xcell': 0.10000000149011612}\nMax yorig & ycell: {'yorig': 32.0, 'ycell': 0.10000000149011612}\nncols: {1081, 1041}\nnrows: {381}\nxorigs: {-160.0, -156.0}\nyorigs: {32.0}\nxcells: {0.10000000149011612}\nycells: {0.10000000149011612}\n\n\ndataset: BSC12CA12-01\nNumber of successful files: 1003\nMax cell sizes: max_ncols = 1081 and max_nrows = 381\nMax xorig & xcell: {'xorig': -160.0, 'xcell': 0.10000000149011612}\nMax yorig & ycell: {'yorig': 32.0, 'ycell': 0.10000000149011612}\nncols: {1081, 1041}\nnrows: {381}\nxorigs: {-160.0, -156.0}\nyorigs: {32.0}\nxcells: {0.10000000149011612}\nycells: {0.10000000149011612}\n\n\n\n\n6.4.1 Rationale and Future Improvements\n\n6.4.1.1 Unloadable Files\nOn our first attempt to convert the data we discovered that various files failed to open. Therefore, we used a dictionary to keep track of which files successfully open.\n\n\n6.4.1.2 Varying Grid Size\nIn this step we collect attribute information about the two different grids used in the dataset. We proceed to use these attributes to resample the grids accordingly, see Section 6.5.\n\n\n6.4.1.3 Optimization and Scaling\nOne improvement to this step is to stop tracking maxes and unique values seperately. Instead, we could just track unique values then get maxes from there. Furthermore, modularizing this process such that it handles the possibility of new grids being used in the dataset would make the conversion script more scalable.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Conversion</span>"
    ]
  },
  {
    "objectID": "data_conversion.html#sec-resampling",
    "href": "data_conversion.html#sec-resampling",
    "title": "6  Data Conversion",
    "section": "6.5 Preparing Resampling Grids",
    "text": "6.5 Preparing Resampling Grids\nNow that we have the sets of openable files, we begin to handle the need to resample data. To resample arrays of shape 381×1041 to 381×1081, we use the SciPy griddata function from the interpolate package. This function gives interpolated values on set of points xi from a set of points with corresponding values. We refer the reader to SciPy’s documentation for details.\nIn this step, we obtain the grids we wish to use as our points and xi. See Section 6.5.2 for how we use these grids with the griddata function.\n\n\n6.5.1 Generate Grids of Latitude and Longitude Points\nRecall we can generate a set of latitude and longitude coordinates by using the attributes given in each NetCDF file, see Section 4.3.3 for an example. Here we generate two sets of latitude and longitude coordinates for each grid size.\n\n\nCode\n1max_xorig = max_grid_x[ids[0]]['xorig']\nmax_xcell = max_grid_x[ids[0]]['xcell']\nmax_yorig = max_grid_y[ids[0]]['yorig']\nmax_ycell = max_grid_y[ids[0]]['ycell']\n\n2big_lon = np.linspace(max_xorig, max_xorig + max_xcell * (max_ncols[ids[0]] - 1), max_ncols[ids[0]])\nbig_lat = np.linspace(max_yorig, max_yorig + max_ycell * (max_nrows[ids[0]] - 1), max_nrows[ids[0]])\n\n3big_lon_pts, big_lat_pts = np.meshgrid(big_lon, big_lat)\nbig_tups = np.array([tup for tup in zip(big_lon_pts.flatten(), big_lat_pts.flatten())])\n\n4sml_ds = xr.open_dataset(firesmoke_dir + \"/BSC00CA12-01/dispersion_20210304.nc\")\nsml_lon = np.linspace(sml_ds.XORIG, sml_ds.XORIG + sml_ds.XCELL * (sml_ds.NCOLS - 1), sml_ds.NCOLS)\nsml_lat = np.linspace(sml_ds.YORIG, sml_ds.YORIG + sml_ds.YCELL * (sml_ds.NROWS - 1), sml_ds.NROWS)\n\n5sml_lon_pts, sml_lat_pts = np.meshgrid(sml_lon, sml_lat)\nsml_tups = np.array([tup for tup in zip(sml_lon_pts.flatten(), sml_lat_pts.flatten())])\n\n\n\n1\n\nGet the x/y origin and cell size parameters for the big 381×1081 grid.\n\n2\n\nGenerate one two lists, defining a grid of latitudes and longitudes.\n\n3\n\nUsing big_lon and big_lat, use meshgrid to generate our 381×1081 set of longitudes and latitudes.\n\n4\n\nOpen a file that uses the small 381×1041 grid. Then, use the attributes in that file to generate two lists defining a grid of latitudes and longitudes.\n\n5\n\nUsing sml_lon and sml_lat, use meshgrid to generate our 381×1041 set of longitudes and latitudes.\n\n\n\n\nSee below for an example of using these latitude and longitude grids to resample data on a 381×1041 grid to a 381×1081 grid.\n\n\n6.5.2 Example with griddata\nNow that we have the two sets of latitude and longitude points, we show an example of how these are used to resample an array of data from a 381×1041 grid to a 381×1081 grid.\n\nIn this example we use the latitude and longitude points generated from the attributes determine across all NetCDF files.\n\n\nCode\nimport numpy as np\nimport xarray as xr\n\n\n\n\nCode\n1max_xorig = -160.0\n2max_xcell = 0.10000000149011612\n3max_yorig = 32.0\n4max_ycell = 0.10000000149011612\n\n5big_lon = np.linspace(max_xorig, max_xorig + max_xcell * (1081 - 1), 1081)\n6big_lat = np.linspace(max_yorig, max_yorig + max_ycell * (381 - 1), 381)\n\n7big_lon_pts, big_lat_pts = np.meshgrid(big_lon, big_lat)\n8big_tups = np.array([tup for tup in zip(big_lon_pts.flatten(), big_lat_pts.flatten())])\n\n9sml_ds = xr.open_dataset(\"dispersion_20210304.nc\")\n10sml_lon = np.linspace(sml_ds.XORIG, sml_ds.XORIG + sml_ds.XCELL * (sml_ds.NCOLS - 1), sml_ds.NCOLS)\n11sml_lat = np.linspace(sml_ds.YORIG, sml_ds.YORIG + sml_ds.YCELL * (sml_ds.NROWS - 1), sml_ds.NROWS)\n\n12sml_lon_pts, sml_lat_pts = np.meshgrid(sml_lon, sml_lat)\n13sml_tups = np.array([tup for tup in zip(sml_lon_pts.flatten(), sml_lat_pts.flatten())])\n\n\n\n1\n\nDefine the maximum x-origin coordinate.\n\n2\n\nDefine the maximum x-cell size.\n\n3\n\nDefine the maximum y-origin coordinate.\n\n4\n\nDefine the maximum y-cell size.\n\n5\n\nCreate an array for the large longitude grid.\n\n6\n\nCreate an array for the large latitude grid.\n\n7\n\nCreate a meshgrid of points using the large longitude and latitude arrays.\n\n8\n\nCreate a flattened array of tuples representing the large grid points.\n\n9\n\nOpen the small dataset using xarray.\n\n10\n\nCreate an array for the small longitude grid.\n\n11\n\nCreate an array for the small latitude grid.\n\n12\n\nCreate a meshgrid of points using the small longitude and latitude arrays.\n\n13\n\nCreate a flattened array of tuples representing the small grid points.\n\n\n\n\n\n\nCode\nprint(f'Using the large grid, we have {np.shape(big_tups)[0]} lat/lon points to sample on.')\nprint(f'Using the small grid, we have {np.shape(sml_tups)[0]} lat/lon points to sample from.')\n\n\nUsing the large grid, we have 411861 lat/lon points to sample on.\nUsing the small grid, we have 396621 lat/lon points to sample from.\n\n\nLet’s get the data at timestep 0 inside dispersion_20210304.nc, which uses a grid of size 381×1041.\n\n\nCode\ntimestep = 0\n\nvals = np.squeeze(sml_ds['PM25'].values)\n\nprint(f'The shape of the PM25 array at timestep {timestep} is {np.shape(vals[timestep])}')\n\n\nThe shape of the PM25 array at timestep 0 is (381, 1041)\n\n\nWe use the following parameters with griddata to resample vals:\ngriddata(points, values, xi, method='cubic', fill_value)\n\npoints: Data point coordinates.\nvalues: Data values.\nxi: Points at which to interpolate data.\nmethod: cubic, 2D: Return the value determined from a piecewise cubic, continuously differentiable (C1), and approximately curvature-minimizing polynomial surface.\nfill_value: Value used to fill in for requested points outside of the convex hull of the input points.\n\n\n\nCode\nfrom scipy.interpolate import griddata\n\n1points = sml_tups\n2values = vals[timestep].flatten()\n3xi = big_tups\n4method = 'cubic'\n5fill_value = 0\n\n6arr = griddata(points, values, xi, method, fill_value)\n\n7print(f'We have interpolated 381×1081 = {np.shape(arr)[0]} points.')\n\n\n\n1\n\nThe points we want to sample from.\n\n2\n\nThe values for the grid we want to sample from, flattened into a 1D array.\n\n3\n\nThe points we want to sample to.\n\n4\n\nThe interpolation method used (‘cubic’ in this case).\n\n5\n\nThe fill value to use (0 instead of NaN).\n\n6\n\nPerform the interpolation.\n\n7\n\nPrint the number of points interpolated.\n\n\n\n\nWe have interpolated 381×1081 = 411861 points.\n\n\nNotice that we have interpolated values that are negative.\n\n\nCode\nprint(f'The minimum PM25 value in our interpolated values is {np.min(arr)}')\n\n\nThe minimum PM25 value in our interpolated values is -0.0004518490243624799\n\n\nWe change values less than our specified threshold to 0. We then reshape the values to be 381×1081 and make all the values of type float32, as this number type is used in the original NetCDF file for PM25 values.\n\n\nCode\n1arr[arr &lt; 1e-15] = 0\n\n2arr = arr.reshape((len(big_lat), len(big_lon)))\n\n3arr = arr.astype(np.float32)\n\n4print(f'The shape of the resampled PM25 array at timestep {timestep} is {np.shape(arr)}')\n\n\n\n1\n\nAny values that are less than a given threshold, make it 0.\n\n2\n\nReshape the result to match the new grid shape.\n\n3\n\nCast number to float32.\n\n4\n\nPrint the shape of the resampled PM25 array at the given timestep.\n\n\n\n\nThe shape of the resampled PM25 array at timestep 0 is (381, 1081)\n\n\nNow we can visualize the resampled values:\n\n\nCode\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\n\n1my_fig, my_plt = plt.subplots(figsize=(15, 6), subplot_kw=dict(projection=ccrs.PlateCarree()))\n\n2my_norm = \"log\"\n3my_extent = [np.min(big_lon), np.max(big_lon), np.min(big_lat), np.max(big_lat)]\n4my_aspect = 'auto'\n5my_origin = 'lower'\n6my_cmap = 'viridis'\n\nplot = my_plt.imshow(arr, norm=my_norm, extent=my_extent, \n7          aspect=my_aspect, origin=my_origin, cmap=my_cmap)\n\n8my_plt.coastlines()\n\n9my_plt.gridlines(draw_labels=True)\n\n10my_fig.colorbar(plot, location='right', label='ug/m^3')\n\n11my_fig.supxlabel('Longitude')\n12my_fig.supylabel('Latitude')\n\n13my_fig.suptitle('Ground level concentration of PM2.5 microns and smaller')\n\n14my_plt.set_title(f'Timestep {timestep}')\n\n15plt.show()\n\n\n\n1\n\nInitialize a figure and plot, so we can customize figure and plot of data.\n\n2\n\nColor PM25 values on a log scale, since values are small.\n\n3\n\nThis will number our x and y axes based on the longitude latitude range.\n\n4\n\nEnsure the aspect ratio of our plot fits all data, matplotlib can do this automatically.\n\n5\n\nTell matplotlib our origin is the lower-left corner.\n\n6\n\nSelect a colormap for our plot and the color bar on the right.\n\n7\n\nCreate our plot using imshow.\n\n8\n\nDraw coastlines.\n\n9\n\nDraw latitude longitude lines.\n\n10\n\nAdd a colorbar to our figure, based on the plot we just made above.\n\n11\n\nSet x axis label on our ax.\n\n12\n\nSet y axis label on our ax.\n\n13\n\nSet title of our figure.\n\n14\n\nSet title of our plot as the timestamp of our data.\n\n15\n\nShow the resulting visualization.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.5.3 Rationale and Future Improvements\n\n6.5.3.1 Discovering Varying Grids\nWe discovered the two grid shapes in version 1 of our conversion script. We found this after noticing the smoke data visualizations were nonsensical.\nFor example, the visualizations showed smoke eminating from the ocean, as shown in Figure 6.1.\n\n\n\n\n\n\n\n\n\n\n\n(a) Plotting Data from 381×1041 Grid on 381×1081 Grid\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Plotting Data Resampled from 381×1041 Grid onto 381×1081 Grid\n\n\n\n\n\n\n\nFigure 6.1: Visualization of Timestamp March 5, 2021 00:00:00 using IDX file Created in Conversion Script Version 1\n\n\n\nFor further details on how we investigated this issue, see Chapter 7.\n\n\n6.5.3.2 Handling Various Grids\nWe considered various approaches to handling the fact that the data was on a 381×1041 grid or 381×1081 grid.\n\n\n\nTable 6.1: The Weaknesses of Approaches to Handling Varying Array Sizes\n\n\n\n\n\n\n\n\n\nApproach\nWeakness\n\n\n\n\nExclude arrays with 1041 columns.\nThrowing away those data points would discard all the information they hold.\n\n\nForce data with 1041 columns into an array with 1081 columns without resampling.\nThis results in unused columns within the 1081-column array, leading to discontinuities and potential artifacts in the data representation.\n\n\nCrop arrays on 1081 columns to 1041 columns\nCropping the data would result in loss of information.\n\n\n\n\n\n\nThe approach we chose was to resample the data with 1041 columns to arrays with 1081 columns. This produced the most visually appealing result and preserved the most information possible.\n\n\n6.5.3.3 Scaling\nOne future improvement is to generalize precomputing additional grids if in the future the smoke forecasts change their grid size again. As of now we manually compute points for the 2 grid sizes.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Conversion</span>"
    ]
  },
  {
    "objectID": "data_conversion.html#sec-sequencing",
    "href": "data_conversion.html#sec-sequencing",
    "title": "6  Data Conversion",
    "section": "6.6 Sequencing of NetCDF Files",
    "text": "6.6 Sequencing of NetCDF Files\nAt this point we have the lists of openable files and our resampling grids. Now we will determine which files to use and in what order.\n\n6.6.1 Understanding Staggered Forecasts\nThe unique challenge of UBC’s short term dataset is that the forecasts overlap, creating staggered predictions.\nLet’s explore these staggered forecasts to understand, how to select the optimal predictions to create a single dataset consisting of the most optimal predictions. For details on each forecast’s time information, please see Chapter 4.\nAcross all the NetCDF files we have downloaded, recall for every hour there exists 1-4 different forecasts to represent that hour. We aim to choose the forecast which best represents each hour. In order to understand the best representation for each hour, we need to know what hours are represented in each forecast.\n\n\n6.6.2 Finding Hours per Forecast ID\nRecall that within each set of Forecast ID files, some files failed to download or otherwise open. Therefore, we must check exactly what set of hours are available in each collection of forecast ID NetCDF files.\nIn order to make loading an hour from a specified forecast ID dataset as easy as possible, we create a dictionary for each forecast ID set. Below you can see the first few entries of this dictionary:\n\n\nCode\n#TODO\n\n\nTo generate this dictionary, we did the following:\n\n\nCode\n1id_sets = {id_: {} for id_ in ids}\n\nfor id_ in ids:\n    # get successful files to add all successful hours to set\n2    for file in tqdm(successful_files[id_]):\n3        path = f'{firesmoke_dir}/{id_}/{file}'\n        \n        ds = xr.open_dataset(path)\n\n4        for h in range(ds.sizes[\"TSTEP\"]):\n            id_sets[id_][(file, parse_tflag(ds['TFLAG'].values[h][0]))] = h\n\n\n\n1\n\nInitialize a dictionary to hold an empty dictionary for each forecast ID.\n\n2\n\nWe step through all the successfully opened files found for id_.\n\n3\n\nOpen the file and open it using xarray.\n\n4\n\nCreate a tuple with the file name and the parsed TFLAG for current hour h. Add index h to our dictionary assigning the tuple as its key.\n\n\n\n\n\n\n6.6.3 Creating the Sequence\nNow that we have an indexable set of all available hours for each forecast ID, we can generate the sequence to extract the time series dataset we would like to create.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Conversion</span>"
    ]
  },
  {
    "objectID": "data_conversion.html#creating-the-idx-file",
    "href": "data_conversion.html#creating-the-idx-file",
    "title": "6  Data Conversion",
    "section": "6.7 Creating the IDX File",
    "text": "6.7 Creating the IDX File\nAt this point, we have precomputed the order in which we will load and write each array to our IDX file. We will now put everything together to generate our single dataset.\n\n\nCode\n1f = Field(\"PM25\", \"float32\")\n\n2db = CreateIdx(\n    url=idx_dir + \"/firesmoke.idx\",\n    fields=[f],\n    dims=[int(max_ncols[ids[0]]), int(max_nrows[ids[0]])],\n    time=[0, len(idx_calls) - 1, \"%00000000d/\"],\n)\n\n3tstep = 0\n\n4thresh = 1e-15\n\n5for call in tqdm(idx_calls):\n    curr_id = call[0]\n    curr_file = call[1]\n    tstep_index = call[3]\n\n6    ds = xr.open_dataset(f\"{firesmoke_dir}/{curr_id}/{curr_file}\")\n\n7    file_vals = np.squeeze(ds[\"PM25\"].values)\n\n8    resamp = ds.XORIG != max_xorig\n\n9    if resamp:\n        file_vals_resamp = griddata(\n            sml_tups,\n            file_vals[tstep_index].flatten(),\n            big_tups,\n            method=\"cubic\",\n            fill_value=0,\n        )\n\n10        file_vals_resamp[file_vals_resamp &lt; thresh] = 0\n\n11        file_vals_resamp = file_vals_resamp.reshape((len(big_lat), len(big_lon)))\n\n12        db.write(data=file_vals_resamp.astype(np.float32), field=f, time=tstep)\n13    else:\n        db.write(data=file_vals[tstep_index], field=f, time=tstep)\n\n14    tstep = tstep + 1\n\n\n\n1\n\nCreate an OpenVisus field to hold the PM25 variable data.\n\n2\n\nCreate the IDX file wherein url is the location to write the file, fields holds the data variables we will save, dims represents the shape of each array, and time defines how many time steps there are.\n\n3\n\nWe will usethis to keep track of which time step we are on as we step through our idx_calls.\n\n4\n\nThreshold to use to change small-enough resampled values to 0.\n\n5\n\nGet the information for current time step, in particular the [curr_id, file_str, parse_tflag(ds[‘TFLAG’].values[tstep_idx][0]), tstep_idx]\n\n6\n\nLoad the current file with xarray.\n\n7\n\nGet the full array of PM25 values in the file.\n\n8\n\nIf ds.XORIG is not already for the 381×1081 grid, we need to resample it to the larger grid.\n\n9\n\nUsing gridddata, interpolate the values on a 381×1081 grid using the precomputed lat/lon points.\n\n10\n\nAny values that are less than our threshold should have a value of 0. WHY\n\n11\n\nReshape the interpolated values to 381×1081.\n\n12\n\nWrite the resampled values for hour h to timestep t and field f of our IDX file.\n\n13\n\nThese values are already on a 381×1081 grid, so write the values at hour h to timestep t and field f of our IDX file.\n\n14\n\nIncrement to the next timestep for writing to IDX.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Conversion</span>"
    ]
  },
  {
    "objectID": "data_validation.html",
    "href": "data_validation.html",
    "title": "7  Data Validation",
    "section": "",
    "text": "7.1 Data Validation vs Data Exploration\nData exploration enlightens one on the contents of the data and metadata one presumes they have. We performed data exploration by loading and visualizing a few files. This allowed us to understand what data UBC aims to provide.\nWhat data exploration does not do is explain the origin of issues such missing or seemingly corrupt data. Data exploration uses the assumption that the data is perfect.\nData validation forces one to consider, where do issues that appear in the data come from, and are these issues with the data or with the systems used to access and manipulate the data?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Validation</span>"
    ]
  },
  {
    "objectID": "data_validation.html#data-loading",
    "href": "data_validation.html#data-loading",
    "title": "7  Data Validation",
    "section": "7.2 Data Loading",
    "text": "7.2 Data Loading\nMost of the issues we faced during data curation resulted from failing to validate our data loading system.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Validation</span>"
    ]
  },
  {
    "objectID": "data_validation.html#data-conversion",
    "href": "data_validation.html#data-conversion",
    "title": "7  Data Validation",
    "section": "7.3 Data Conversion",
    "text": "7.3 Data Conversion",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Validation</span>"
    ]
  }
]