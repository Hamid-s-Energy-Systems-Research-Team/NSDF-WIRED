[
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "firesmoke.ca",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "UBC Smoke Forecast Data Curation and Distribution",
    "section": "",
    "text": "Preface\nWelcome to NSDF’s UBC Firesmoke Data Curation website. Here we describe the data curation process of UBC’s Smoke Forecast datasets. We inform readers of the challenges and solutions we found when repurposing these short term datasets into a long term dataset.\nIt is important to note that although we will present the data curation process in a linear fashion here, it was not a linear process. Rather, the process was cyclical, and at each iteration we introduced new improvements.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#navigation-and-webpage-information",
    "href": "index.html#navigation-and-webpage-information",
    "title": "UBC Smoke Forecast Data Curation and Distribution",
    "section": "Navigation and Webpage Information",
    "text": "Navigation and Webpage Information\nThis webpage is produced using Quart.\nYou can find the source code for this page at TODO.\nAll files or directories in this website are hosted at our GitHub repository, unless otherwise specified.\nAll code blocks contain annotations, hover over the numbers on the right hand side to see the accompanying annotation.\n1Hover over me ----&gt;\n\n1\n\nI’m an annotation.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "demo.html#this-notebook-provide-the-instructions-on-how-to-read-ubc-firesmoke-data-from-firsmoke_metadata.nc-using-xarray-and-the-openvisus-xarray-backend.",
    "href": "demo.html#this-notebook-provide-the-instructions-on-how-to-read-ubc-firesmoke-data-from-firsmoke_metadata.nc-using-xarray-and-the-openvisus-xarray-backend.",
    "title": "1  IDX File Demo",
    "section": "1.1 This notebook provide the instructions on how to read UBC firesmoke data from firsmoke_metadata.nc using xarray and the OpenVisus xarray backend.",
    "text": "1.1 This notebook provide the instructions on how to read UBC firesmoke data from firsmoke_metadata.nc using xarray and the OpenVisus xarray backend.\nDashboard visible here: http://chpc3.nationalsciencedatafabric.org:9988/dashboards",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>IDX File Demo</span>"
    ]
  },
  {
    "objectID": "demo.html#step-1-importing-the-libraries",
    "href": "demo.html#step-1-importing-the-libraries",
    "title": "1  IDX File Demo",
    "section": "1.2 Step 1: Importing the libraries",
    "text": "1.2 Step 1: Importing the libraries\n\n1.2.1 Please be sure to have libraries installed\n\n\nCode\n# for numerical work\nimport numpy as np\n\n# for accessing file system\nimport os\n\n# for loading netcdf files, for metadata\nimport xarray as xr\n# for connecting OpenVisus framework to xarray\n# from https://github.com/sci-visus/openvisuspy, \nfrom openvisuspy.xarray_backend import OpenVisusBackendEntrypoint\n\n# Used for processing netCDF time data\nimport time\nimport datetime\nimport requests\n# Used for indexing via metadata\nimport pandas as pd\n\n# for plotting\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\n\n\n#Stores the OpenVisus cache in the local direcrtory \nimport os\nos.environ[\"VISUS_CACHE\"]=\"./visus_cache_can_be_erased\"\nos.environ['CURL_CA_BUNDLE'] = ''",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>IDX File Demo</span>"
    ]
  },
  {
    "objectID": "demo.html#step-2-reading-the-data-metadata-from-file",
    "href": "demo.html#step-2-reading-the-data-metadata-from-file",
    "title": "1  IDX File Demo",
    "section": "1.3 Step 2: Reading the data & metadata from file",
    "text": "1.3 Step 2: Reading the data & metadata from file\n\n1.3.1 In this section, we load our data using xr.open_dataset.\n\n\nCode\n# path to tiny NetCDF\nurl = 'https://github.com/sci-visus/NSDF-WIRED/raw/main/data/firesmoke_metadata.nc'\n\n# Download the file using requests\nresponse = requests.get(url)\nlocal_netcdf = 'firesmoke_metadata.nc'\nwith open(local_netcdf, 'wb') as f:\n    f.write(response.content)\n    \n# open tiny netcdf with xarray and OpenVisus backend\nds = xr.open_dataset(local_netcdf, engine=OpenVisusBackendEntrypoint)\n\n\nov.LoadDataset(http://atlantis.sci.utah.edu/mod_visus?dataset=UBC_fire_smoke_BSC&cached=1)\nPM25\nAdding field  PM25 shape  [27357, 381, 1081, 21] dtype  float32 labels  ['time', 'ROW', 'COL', 'resolution'] Max Resolution  20\n\n\n\n\nCode\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:            (time: 27357, ROW: 381, COL: 1081, resolution: 21,\n                        VAR: 1, DATE-TIME: 2)\nDimensions without coordinates: time, ROW, COL, resolution, VAR, DATE-TIME\nData variables:\n    PM25               (time, ROW, COL, resolution) float32 ...\n    TFLAG              (time, VAR, DATE-TIME) int32 ...\n    wrf_arw_init_time  (time, VAR, DATE-TIME) int32 ...\n    resampled          (time) bool ...\n    CDATE              (time) int32 ...\n    CTIME              (time) int32 ...\n    WDATE              (time) int32 ...\n    WTIME              (time) int32 ...\n    SDATE              (time) int32 ...\n    STIME              (time) int32 ...\nAttributes: (12/28)\n    IOAPI_VERSION:  $Id: @(#) ioapi library version 3.0 $                    ...\n    EXEC_ID:        ????????????????                                         ...\n    FTYPE:          1\n    TSTEP:          10000\n    NTHIK:          1\n    NCOLS:          1081\n    ...             ...\n    GDNAM:          HYSPLIT CONC    \n    UPNAM:          hysplit2netCDF  \n    VAR-LIST:       PM25            \n    FILEDESC:       Hysplit Concentration Model Output                       ...\n    HISTORY:        \n    idx_url:        http://atlantis.sci.utah.edu/mod_visus?dataset=UBC_fire_s...xarray.DatasetDimensions:time: 27357ROW: 381COL: 1081resolution: 21VAR: 1DATE-TIME: 2Coordinates: (0)Data variables: (10)PM25(time, ROW, COL, resolution)float32...long_name :PM25            units :ug/m^3          var_desc :PM25                                                                            [236612908917 values with dtype=float32]TFLAG(time, VAR, DATE-TIME)int32...[54714 values with dtype=int32]wrf_arw_init_time(time, VAR, DATE-TIME)int32...[54714 values with dtype=int32]resampled(time)bool...[27357 values with dtype=bool]CDATE(time)int32...[27357 values with dtype=int32]CTIME(time)int32...[27357 values with dtype=int32]WDATE(time)int32...[27357 values with dtype=int32]WTIME(time)int32...[27357 values with dtype=int32]SDATE(time)int32...[27357 values with dtype=int32]STIME(time)int32...[27357 values with dtype=int32]Indexes: (0)Attributes: (28)IOAPI_VERSION :$Id: @(#) ioapi library version 3.0 $                                           EXEC_ID :????????????????                                                                FTYPE :1TSTEP :10000NTHIK :1NCOLS :1081NROWS :381NLAYS :1NVARS :1GDTYP :1P_ALP :0.0P_BET :0.0P_GAM :0.0XCENT :-106.0YCENT :51.0XORIG :-160.0YORIG :32.0XCELL :0.10000000149011612YCELL :0.10000000149011612VGTYP :5VGTOP :-9999.0VGLVLS :[10.  0.]GDNAM :HYSPLIT CONC    UPNAM :hysplit2netCDF  VAR-LIST :PM25            FILEDESC :Hysplit Concentration Model Output                                              lat-lon coordinate system                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       HISTORY :idx_url :http://atlantis.sci.utah.edu/mod_visus?dataset=UBC_fire_smoke_BSC&cached=1\n\n\n\n1.3.1.1 Data Variables Description\n\n\n\n\n\n\n\nAttribute\nDescription\n\n\n\n\nPM25\nThe concentration of particulate matter (PM2.5) for each time step, layer, row, and column in the spatial grid.\n\n\nTFLAG\nThe date and time of each data point.\n\n\nwrf_arw_init_time\nThe time at which this prediction’s weather forecast was initiated.\n\n\nresampled\nWhether this timestamp was resampled from a 381x1041 to 381x1081 grid or not.\n\n\nCDATE\nThe creation date of the data point, in YYYYDDD format.\n\n\nCTIME\nThe creation time of the data point, in HHMMSS format.\n\n\nWDATE\nThe date for which the weather forecast is initiated, in YYYYDDD format.\n\n\nWTIME\nThe time for which the weather forecast is initiated, in HHMMSS format.\n\n\nSDATE\nThe date for which the smoke forecast is initiated, in YYYYDDD format.\n\n\nSTIME\nThe time for which the weather forecast is initiated, in HHMMSS format.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>IDX File Demo</span>"
    ]
  },
  {
    "objectID": "demo.html#step-2.5-calculate-derived-metadata-using-original-metadata-above-to-create-coordinates",
    "href": "demo.html#step-2.5-calculate-derived-metadata-using-original-metadata-above-to-create-coordinates",
    "title": "1  IDX File Demo",
    "section": "1.4 Step 2.5, Calculate derived metadata using original metadata above to create coordinates",
    "text": "1.4 Step 2.5, Calculate derived metadata using original metadata above to create coordinates\n\n1.4.1 This is required to allow for indexing of data via metadata\n\n1.4.1.1 Calculate latitude and longitude grid\n\n\nCode\n# Get metadata to compute lon and lat\nxorig = ds.XORIG\nyorig = ds.YORIG\nxcell = ds.XCELL\nycell = ds.YCELL\nncols = ds.NCOLS\nnrows = ds.NROWS\n\nlongitude = np.linspace(xorig, xorig + xcell * (ncols - 1), ncols)\nlatitude = np.linspace(yorig, yorig + ycell * (nrows - 1), nrows)\n\nprint(\"Size of longitude & latitude arrays:\")\nprint(f'np.size(longitude) = {np.size(longitude)}')\nprint(f'np.size(latitude) = {np.size(latitude)}\\n')\nprint(\"Min & Max of longitude and latitude arrays:\")\nprint(f'longitude: min = {np.min(longitude)}, max = {np.max(longitude)}')\nprint(f'latitude: min = {np.min(latitude)}, max = {np.max(latitude)}')\n\n\nSize of longitude & latitude arrays:\nnp.size(longitude) = 1081\nnp.size(latitude) = 381\n\nMin & Max of longitude and latitude arrays:\nlongitude: min = -160.0, max = -51.99999839067459\nlatitude: min = 32.0, max = 70.00000056624413\n\n\n\n\n1.4.1.2 Using calculated latitude and longitude, create coordinates allowing for indexing data using lat/lon\n\n\nCode\n# Create coordinates for lat and lon (credit: Aashish Panta)\nds.coords['lat'] = ('ROW', latitude)\nds.coords['lon'] = ('COL', longitude)\n\n# Replace col and row dimensions with newly calculated lon and lat arrays (credit: Aashish Panta)\nds = ds.swap_dims({'COL': 'lon', 'ROW': 'lat'})\n\n\n\n\n1.4.1.3 Create coordinates allowing for indexing data using timestamp\n\n1.4.1.3.1 First, convert tflags to timestamps that are compatible with xarray\n\n\nCode\ndef parse_tflag(tflag):\n    \"\"\"\n    Return the tflag as a datetime object\n    :param list tflag: a list of two int32, the 1st representing date and 2nd representing time\n    \"\"\"\n    # obtain year and day of year from tflag[0] (date)\n    date = int(tflag[0])\n    year = date // 1000 # first 4 digits of tflag[0]\n    day_of_year = date % 1000 # last 3 digits of tflag[0]\n\n    # create datetime object representing date\n    final_date = datetime.datetime(year, 1, 1) + datetime.timedelta(days=day_of_year - 1)\n\n    # obtain hour, mins, and secs from tflag[1] (time)\n    time = int(tflag[1])\n    hours = time // 10000 # first 2 digits of tflag[1]\n    minutes = (time % 10000) // 100 # 3rd and 4th digits of tflag[1] \n    seconds = time % 100  # last 2 digits of tflag[1]\n\n    # create final datetime object\n    full_datetime = datetime.datetime(year, final_date.month, final_date.day, hours, minutes, seconds)\n    return full_datetime\n\n\n\n\n1.4.1.3.2 Return an array of the tflags as pandas timestamps\n\n\nCode\n# get all tflags\ntflag_values = ds['TFLAG'].values\n\n# to store pandas timestamps\ntimestamps = []\n\n# convert all tflags to pandas timestamps, store in timestamps list\nfor tflag in tflag_values:\n    timestamps.append(pd.Timestamp(parse_tflag(tflag[0])))\n\n# check out the first 3 timestamps\ntimestamps[0:3]\n\n\n[Timestamp('2021-03-04 00:00:00'),\n Timestamp('2021-03-04 01:00:00'),\n Timestamp('2021-03-04 02:00:00')]\n\n\n\n\nCode\n# set coordinates to each timestep with these pandas timestamps\nds.coords['time'] = ('time', timestamps)\n\n\n\n\n\n1.4.1.4 The timestamps may not be intuitive. The following utility function returns the desired pandas timestamp based on your date and time of interest.\n\n1.4.1.4.1 When you index the data at a desired time, use this function to get the timestamp you need to index.\n\n\nCode\ndef get_timestamp(year, month, day, hour):\n    \"\"\"\n    return a pandas timestamp using the given date-time arguments\n    :param int year: year\n    :param int month: month\n    :param int day: day\n    :param int hour: hour\n    \"\"\"\n    # Convert year, month, day, and hour to a datetime object\n    full_datetime = datetime.datetime(year, month, day, hour)\n    \n    # Extract components from the datetime object\n    year = full_datetime.year\n    day_of_year = full_datetime.timetuple().tm_yday\n    hours = full_datetime.hour\n    minutes = full_datetime.minute\n    seconds = full_datetime.second\n\n    # Compute tflag[0] and tflag[1]\n    tflag0 = year * 1000 + day_of_year\n    tflag1 = hours * 10000 + minutes * 100 + seconds\n\n    # Return the Pandas Timestamp object\n    return pd.Timestamp(full_datetime)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>IDX File Demo</span>"
    ]
  },
  {
    "objectID": "demo.html#step-3-select-a-data_slice",
    "href": "demo.html#step-3-select-a-data_slice",
    "title": "1  IDX File Demo",
    "section": "1.5 Step 3: Select a data_slice",
    "text": "1.5 Step 3: Select a data_slice\n\n1.5.1 This section shows you how to load the data you want.\n\n1.5.1.1 You can index the data using indices, timestamps*, latitude & longitude, and by desired resolution**.\n*Not setting any time means the first timestep available is selected. **Not setting quality means full data resolution is selected.\n\n1.5.1.1.1 In this case, let’s get all available firesmoke data for March 5, 2021 00:00:00 and the time and date for which it’s weather and smoke forecast were initiated.\n\n\nCode\n# select timestamp\nmy_timestamp = get_timestamp(2021, 3, 5, 0)\n\n# select resolution, let's use full resolution since data isn't too big at one time slice\n# data resolution can be -19 for lowest res and 0 for highest res\ndata_resolution = 0\n\n# get PM25 values and provide 4 values, the colons mean select all lat and lon indices\ndata_array_at_time = ds['PM25'].loc[my_timestamp, :, :, data_resolution]\n\n# the metadata specifying weather and smoke forecast initialization times\nresampled = ds['resampled'].loc[my_timestamp]\nsdate = ds['SDATE'].loc[my_timestamp]\nstime = ds['STIME'].loc[my_timestamp]\nwdate = ds['WDATE'].loc[my_timestamp]\nwtime = ds['WTIME'].loc[my_timestamp]\n\n# notice, to access the data, you must append \".values\" to the data array we got above\nprint(f'timestamp: {my_timestamp}')\nprint(f'resampled: {resampled.values} (boolean)')\nprint(f'SDATE is {sdate.values} (YYYYDDD)')\nprint(f'STIME is {stime.values} (HHMMSS)')\nprint(f'WDATE is {wdate.values} (YYYYDDD)')\nprint(f'WTIME is {wtime.values} (HHMMSS)')\nprint(f'shape of data_array_at_time.values = {np.shape(data_array_at_time.values)}')\n\n\ntimestamp: 2021-03-05 00:00:00\nresampled: True (boolean)\nSDATE is 2021063 (YYYYDDD)\nSTIME is 210000 (HHMMSS)\nWDATE is 2021063 (YYYYDDD)\nWTIME is 204413 (HHMMSS)\nUsing Max Resolution:  20\nTime: 24, max_resolution: 20, logic_box=(0, 1081, 0, 381), field: PM25\nshape of data_array_at_time.values = (381, 1081)\n\n\n\n\n1.5.1.1.2 Perhaps we want to slice a specific latitude longitude range from our data_array_at_time, for example, latitude range [35, 50] and longitude range [-140, -80]. Let’s do that below.\n\n\nCode\n# # define range for latitude and longitude to use\nmin_lat = 35\nmax_lat = 50\nmin_lon = -140\nmax_lon = -80\n\n# get PM25 values and provide 4 values, but this time at our desired ranges\ndata_array_at_latlon = ds['PM25'].loc[my_timestamp, min_lat:max_lat, min_lon:max_lon, data_resolution]\n\n# notice, to access the data, you must append \".values\" to the data array we got above\nprint(f'timestamp: {my_timestamp}')\nprint(f'shape of data_array_at_time.values = {np.shape(data_array_at_latlon.values)}')\n\n\ntimestamp: 2021-03-05 00:00:00\nUsing Max Resolution:  20\nTime: 24, max_resolution: 20, logic_box=(200, 800, 30, 180), field: PM25\nshape of data_array_at_time.values = (150, 600)\n\n\nWe show how to obtain this attribute information for a time step of one’s choice, let’s use\n\n\n\n1.5.1.2 The following are the max and min timestamps, lon/lat values, and data resolutions you can index by\n\n1.5.1.2.1 Be sure you index within the data range, otherwise you may get errors since no data exists outside these ranges!\n\n\nCode\n# NOTE: there is one dummy date, ignore ds['time'].values[-1]\nprint(f\"earliest valid timestamp is: {ds['time'].values[0]}\")\nprint(f\"latest valid timestamp is: {ds['time'].values[-2]}\\n\")\n\nprint(f\"valid longitude range is: {ds['lon'].values[0]}, {ds['lon'].values[-1]}\")\nprint(f\"valid latitude range is: {ds['lat'].values[0]}, {ds['lat'].values[-1]}\\n\")\n\nprint(f\"valid data resolutions range is: [-19, 0]\")\n\n\nearliest valid timestamp is: 2021-03-04T00:00:00.000000000\nlatest valid timestamp is: 2024-06-27T22:00:00.000000000\n\nvalid longitude range is: -160.0, -51.99999839067459\nvalid latitude range is: 32.0, 70.00000056624413\n\nvalid data resolutions range is: [-19, 0]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>IDX File Demo</span>"
    ]
  },
  {
    "objectID": "demo.html#step-4-visualize-data_slice",
    "href": "demo.html#step-4-visualize-data_slice",
    "title": "1  IDX File Demo",
    "section": "1.6 Step 4: Visualize data_slice",
    "text": "1.6 Step 4: Visualize data_slice\n\n1.6.1 One can visualize the data either by:\n\n\n1.6.2 1. Get the values from your data_array_at_time and plot using your favorite python visualization library. We’ll use matplotlib.\n\n1.6.2.1 2. Use xarray’s built in plotting function (not recommended, as it is not robust)\nHere we plot data_array_at_time with matplotlib and its basemap extenstion to add geographic context.\n\n\nCode\n# Let's use matplotlib's imshow, since our data is on a grid\n# ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n\n# Initialize a figure and plot, so we can customize figure and plot of data\n# ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html\n# ref: https://scitools.org.uk/cartopy/docs/latest/getting_started/index.html\nmy_fig, my_plt = plt.subplots(figsize=(15, 6), subplot_kw=dict(projection=ccrs.PlateCarree()))\n\n# Let's set some parameters to get the visualization we want\n# ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n\n# color PM25 values on a log scale, since values are small\nmy_norm = \"log\" \n# this will number our x and y axes based on the longitude latitude range\nmy_extent = [np.min(longitude), np.max(longitude), np.min(latitude), np.max(latitude)]\n# ensure the aspect ratio of our plot fits all data, matplotlib can does this automatically\nmy_aspect = 'auto'\n# tell matplotlib, our origin is the lower-left corner\nmy_origin = 'lower'\n# select a colormap for our plot and the color bar on the right\nmy_cmap = 'Oranges'\n\n# create our plot using imshow\nplot = my_plt.imshow(data_array_at_time.values, norm=my_norm, extent=my_extent, \n          aspect=my_aspect, origin=my_origin, cmap=my_cmap)\n\n# draw coastlines\nmy_plt.coastlines()\n\n# draw latitude longitude lines\n# ref: https://scitools.org.uk/cartopy/docs/latest/gallery/gridlines_and_labels/gridliner.html\nmy_plt.gridlines(draw_labels=True)\n\n# add a colorbar to our figure, based on the plot we just made above\nmy_fig.colorbar(plot,location='right', label='ug/m^3')\n\n# Add metadata as text annotations\nmetadata_text = (\n    f'resampled: {resampled.values}\\n'\n    f'SDATE: {sdate.values}\\n'\n    f'STIME: {stime.values}\\n'\n    f'WDATE: {wdate.values}\\n'\n    f'WTIME: {wtime.values}'\n)\n\n# Place metadata text on the plot\nmy_plt.text(0.02, 0.02, metadata_text, transform=my_plt.transAxes,\n            fontsize=12, verticalalignment='bottom', bbox=dict(facecolor='white', alpha=0.8))\n\n# Set x and y axis labels on our ax\nmy_fig.supxlabel('Longitude')\nmy_fig.supylabel('Latitude')\n\n# Set title of our figure\nmy_fig.suptitle('Ground level concentration of PM2.5 microns and smaller')\n\n# Set title of our plot as the timestamp of our data\nmy_plt.set_title(f'{my_timestamp}')\n\n# Show the resulting visualization\nplt.show()\n\n\n\n\n\n\n\n\n\nHere we plot with xarray’s built-in matplotlib powered plotter.\n\n\nCode\ndata_array_at_time.plot(vmin=0, vmax=30)\n\n\n\n\n\n\n\n\n\nHere we plot data_array_at_latlon. We use the exact same code, but define my_extent accordingly.\n\n\nCode\n# Let's use matplotlib's imshow, since our data is on a grid\n# ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n\n# Initialize a figure and plot, so we can customize figure and plot of data\n# ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html\nmy_fig, my_plt = plt.subplots(figsize=(15, 6), subplot_kw=dict(projection=ccrs.PlateCarree()))\n\n# Let's set some parameters to get the visualization we want\n# ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n\n# color PM25 values on a log scale, since values are small\nmy_norm = \"log\" \n# ***this will number our x and y axes based on the longitude latitude range***\nmy_extent = [min_lon, max_lon, min_lat, max_lat]\n# ensure the aspect ratio of our plot fits all data, matplotlib can does this automatically\nmy_aspect = 'auto'\n# tell matplotlib, our origin is the lower-left corner\nmy_origin = 'lower'\n# select a colormap for our plot and the color bar on the right\nmy_cmap = 'Oranges'\n\n# create our plot using imshow\nplot = plt.imshow(data_array_at_latlon.values, norm=my_norm, extent=my_extent, \n          aspect=my_aspect, origin=my_origin, cmap=my_cmap)\n\n# draw coastlines\nmy_plt.coastlines()\n\n# draw latitude longitude lines\n# ref: https://scitools.org.uk/cartopy/docs/latest/gallery/gridlines_and_labels/gridliner.html\nmy_plt.gridlines(draw_labels=True)\n\n# add a colorbar to our figure, based on the plot we just made above\nmy_fig.colorbar(plot,location='right', label='ug/m^3')\n\n# Add metadata as text annotations\nmetadata_text = (\n    f'resampled: {resampled.values}\\n'\n    f'SDATE: {sdate.values}\\n'\n    f'STIME: {stime.values}\\n'\n    f'WDATE: {wdate.values}\\n'\n    f'WTIME: {wtime.values}'\n)\n\n# Place metadata text on the plot\nmy_plt.text(0.02, 0.02, metadata_text, transform=my_plt.transAxes,\n            fontsize=12, verticalalignment='bottom', bbox=dict(facecolor='white', alpha=0.8))\n\n# Set x and y axis labels on our ax\nmy_fig.supxlabel('Longitude')\nmy_fig.supylabel('Latitude')\n\n# Set title of our figure\nmy_fig.suptitle('Ground level concentration of PM2.5 microns and smaller')\n\n# Set title of our plot as the timestamp of our data\nmy_plt.set_title(f'{my_timestamp}')\n\n# Show the resulting visualization\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndata_array_at_latlon.plot(vmin=0, vmax=30)\n\n\n\n\n\n\n\n\n\n\n\n\n1.6.3 Please reach out to Arleth Salinas or Valerio Pascucci for any concerns about the notebook. Thank you!\n\nArleth Salinas (arleth.salinas@utah.edu)\nValerio Pascucci (pascucci.valerio@gmail.com)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>IDX File Demo</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2  Introduction",
    "section": "",
    "text": "Wildfires in North America have significantly impacted ecosystems and human society [1]. Climate change affects the frequency, duration, and severity of wildfires thus necessitating the use of wildfire prediction systems to effectively mitigate wildfire impact. However, data for understanding the impact of climate change on wildfires is limited, only available for a few regions and for only a few decades [2]. Furthermore, wildfire prediction systems in North America prioritize decision making and fire management on short timescales, from minutes to months. Therefore, long term wildfire prediction systems have limited access to aggregate short term data, due to resource constraints from fire management entities to share the data they collect and curate.\nThe Weather Forecast Research Team at the University of British Columbia (UBC) generates a short term dataset of PM2.5 smoke particulate presence in North America. Over the past 3 years, each day four times a day, UBC has created forecasts of PM2.5 smoke particulate on the ground for Canada and the continental United States. This is done using their The BlueSky Western Canada Wildfire Smoke Forecasting System. UBC provides access to this data to paying customers and for free on a daily basis via a web-based visualization and file download.\nThese smoke predictions are useful for those who must make decisions on how to deal with smoke as it comes. However, these years of forecasts are not available in a non-trivial fashion for long term forecasting. The data only exists among the hundreds of NetCDF files that UBC has generated.\nOur task is to obtain a single long term dataset from the smoke forecast files that are available from UBC.\n\n3 References",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "sys_specs.html",
    "href": "sys_specs.html",
    "title": "3  System and Environment",
    "section": "",
    "text": "3.1 Machine Specification\nThe data we curate is over 300 gigabytes large. Therefore we used the SCI institute’s in-house machine ‘atlantis’ for data staging and processing. See Table 3.1.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>System and Environment</span>"
    ]
  },
  {
    "objectID": "sys_specs.html#machine-specification",
    "href": "sys_specs.html#machine-specification",
    "title": "3  System and Environment",
    "section": "",
    "text": "Table 3.1: ‘atlantis’ System Specifications\n\n\n\n\n\nProperty\nValue\n\n\n\n\nArchitecture\nx86_64\n\n\nCPU op-mode(s)\n32-bit, 64-bit\n\n\nByte Order\nLittle Endian\n\n\nAddress sizes\n44 bits physical, 48 bits virtual\n\n\nCPU(s)\n48\n\n\nOn-line CPU(s) list\n0-47\n\n\nThread(s) per core\n2\n\n\nCore(s) per socket\n6\n\n\nSocket(s)\n4",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>System and Environment</span>"
    ]
  },
  {
    "objectID": "sys_specs.html#environment",
    "href": "sys_specs.html#environment",
    "title": "3  System and Environment",
    "section": "3.2 Environment",
    "text": "3.2 Environment\nWe aim to work in an environment that can be most easily reproduced and documented. Therefore we used Python 3.9.19 via conda. We did all our work within the Project Jupyter environment.\nTo find our the yml file containing our exported conda environment please see the sidebar.\nTo work on the ‘atlantis’ machine, we used SSH to connect to the machine.\nIn the proceeding chapters we will specify which tools and libraries were used and why.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>System and Environment</span>"
    ]
  },
  {
    "objectID": "data_source.html",
    "href": "data_source.html",
    "title": "4  The Data Source",
    "section": "",
    "text": "4.1 Overview\nThe Weather Forecast Research Team at the University of British Columbia (UBC) generates a short term dataset of PM2.5 smoke particulate presence in North America. This is done using their The BlueSky Western Canada Wildfire Smoke Forecasting System. Over the past 3 years, each day four times a day, UBC creates 2-day forecasts of PM2.5 smoke particulate on the ground for Canada and the continental United States. Each such forecast is downloadable as a NetCDF file or KMZ file. UBC provides access to these predictions for free on a daily basis at their website firesmoke.ca.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Data Source</span>"
    ]
  },
  {
    "objectID": "data_source.html#ubc-smoke-forecast-files-access",
    "href": "data_source.html#ubc-smoke-forecast-files-access",
    "title": "4  The Data Source",
    "section": "4.2 UBC Smoke Forecast Files Access",
    "text": "4.2 UBC Smoke Forecast Files Access\n\n\n4.2.1 Available Forecasts\nAll forecast files are uniquely identifiable with a forecast ID based on when their meteorology forecast is initiated, a smoke forecast initialization time, and by date. The time ranges of available files by forecast ID is shown in Table 4.1. Please note, there are occassional failed forecasts or otherwise unavailable files within the date ranges specified Table 4.1, see Section 4.4 for further details.\n\n\n\nTable 4.1: Dates for which all forecast ID datasets are publicly available. All times are in UTC and the grid size is 12 km.\n\n\n\n\n\n\n\n\n\n\n\n\nForecast ID\nMeteorology Forecast Initialization (UTC)\nSmoke Forecast Initialization (UTC)\nStart Date\nEnd Date\n\n\n\n\nBSC00CA12-01\n00Z\n08Z\nMarch 4, 2021\nPresent Day\n\n\nBSC06CA12-01\n06Z\n14Z\nMarch 4, 2021\nPresent Day\n\n\nBSC12CA12-01\n12Z\n20Z\nMarch 3, 2021\nPresent Day\n\n\nBSC18CA12-01\n18Z\n02Z\nMarch 4, 2021\nPresent Day\n\n\n\n\n\n\n\nThe smoke forecasts are updated daily, including the present day, so there is no fixed end date. Therefore, the latest data must be downloaded on a regular basis. We have not implemented this process yet, so the latest forecast files we use are up to June 27, 2024.\nThere is no official source stating the earliest available date for each forecast. So, knowing the project began in 2021, we inferenced that the earliest available date would be in 2021. Via trial and error we found the earliest available dates.\n\n\n4.2.2 Download Instructions\nTo download the 2-day forecast for the forecast initialization date of one’s choice, one follows the instructions below. The downloaded file can be a NetCDF or KMZ file.\nGo to the URL: https://firesmoke.ca/forecasts/{Forecast ID}/{YYYYMMDD}{InitTime}/{File Type}\nWhere:\n\nYYYYMMDD is the date of choice.\nForecastID and InitTime are the chosen values as described in Table 4.2.\nFile Type is either dispersion.nc or dispersion.kmz for either the NetCDF file or KMZ file, respectively.\n\n\n\n\nTable 4.2: UBC Smoke Forecast Data Download Parameters.\n\n\n\n\n\nForecast ID\nSmoke Forecast Initialization (UTC)\n\n\n\n\nBSC00CA12-01\n08\n\n\nBSC06CA12-01\n14\n\n\nBSC12CA12-01\n20\n\n\nBSC18CA12-01\n02\n\n\n\n\n\n\n\n4.2.2.1 Download Example\nLet’s try downloading the forecast for January 1, 2024 where the weather forecast is initiated at 00:00:00 UTC and the smoke forecast is initialized at 08:00:00 UTC by navigating to the corresponding URL.\n\n\nCode\nforecast_id = \"BSC00CA12-01\"\nyyyymmdd = \"20210304\"\ninit_time = \"08\"\n\nurl = (\n    f\"https://firesmoke.ca/forecasts/{forecast_id}/{yyyymmdd}{init_time}/dispersion.nc\"\n)\n\nprint(f\"Navigate to this URL in your browser: {url}\")\n\n\nNavigate to this URL in your browser: https://firesmoke.ca/forecasts/BSC00CA12-01/2021030408/dispersion.nc",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Data Source</span>"
    ]
  },
  {
    "objectID": "data_source.html#the-netcdf-file",
    "href": "data_source.html#the-netcdf-file",
    "title": "4  The Data Source",
    "section": "4.3 The NetCDF File",
    "text": "4.3 The NetCDF File\nNext, let’s look at what is within the NetCDF file located at the URL in our previous example.\n\n4.3.1 File Preview\n\nWe load dispersion.nc using xarray, which provides a preview of the file.\n\n\n\n4.3.2 File Attributes\ndispersion.nc contains the following attributes. Note that for all files across forecast IDs, they have the same dimension and variable names:\n\n4.3.2.1 Dimensions:\nThe dimensions described in Table 4.3 determine on which indicies we may index our variables.\n\n\n\nTable 4.3: Description of Dimensions for Indexing Data in NetCDF Files\n\n\n\n\n\n\n\n\n\n\nDimension\nSize\nDescription\n\n\n\n\nTSTEP\n51\nThis dimension represents the number of time steps in the file. Each file has 51 hours represented.\n\n\nVAR\n1\nThis dimension is a placeholder for the variables in the file.\n\n\nDATE-TIME\n2\nThis dimension stores the date and time information for each time step.\n\n\nLAY\n1\nThis dimension represents the number of layers in the file, which is 1 in this case.\n\n\nROW\n381\nThis dimension represents the number of rows in the spatial grid.\n\n\nCOL\n1041\nThis dimension represents the number of columns in the spatial grid.\n\n\n\n\n\n\n\n\n4.3.2.2 Variables:\nThe variables described in Table 4.4 contain the data in question that we would like to extract.\n\n\n\nTable 4.4: Description of Variables in NetCDF Files\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDimensions\nData Type\nDescription\n\n\n\n\nTFLAG\nTSTEP, VAR, DATE-TIME\nint32\nThis variable stores the date and time of each time step.\n\n\nPM25\nTSTEP, LAY, ROW, COL\nfloat32\nThis variable contains the concentration of particulate matter (PM2.5) for each time step, layer, row, and column in the spatial grid.\n\n\n\n\n\n\n\n\n4.3.2.3 Attributes\nOf the 33 available attributes we use the ones shown in Table 4.5: \n\n\n\nTable 4.5: Description of Attributes in dispersion.nc\n\n\n\n\n\n\n\n\n\n\nAttribute\nValue\nDescription\n\n\n\n\nCDATE\n2021063\nThe creation date of the dataset, in YYYYDDD format.\n\n\nCTIME\n101914\nThe creation time of the dataset, in HHMMSS format.\n\n\nWDATE\n2021063\nThe date for which the weather forecast is initiated, in YYYYDDD format.\n\n\nWTIME\n101914\nThe time for which the weather forecast is initiated, in HHMMSS format.\n\n\nSDATE\n2021063\nThe date for which the smoke forecast is initiated,in YYYYDDD format.\n\n\nSTIME\n90000\nThe time for which the weather forecast is initiated, in HHMMSS format.\n\n\nNCOLS\n1041\nThe number of columns in the spatial grid.\n\n\nNROWS\n381\nThe number of rows in the spatial grid.\n\n\nXORIG\n-156.0\nThe origin (starting point) of the grid in the x-direction.\n\n\nYORIG\n32.0\nThe origin (starting point) of the grid in the y-direction.\n\n\nXCELL\n0.10000000149011612\nThe cell size in the x-direction.\n\n\nYCELL\n0.10000000149011612\nThe cell size in the y-direction.\n\n\n\n\n\n\nLet’s look closer at what exactly is within one NetCDF file in the following demo.\n\n\n\n4.3.3 NetCDF Visualization Demo\n{{&lt; embed data_notebooks/data_source/netcdf_demo.ipynb true &gt;}}\nNow that we understand how to load the data and metadata from the file and process it for visualization, let’s establish the data and metadata available to us across all NetCDF files.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Data Source</span>"
    ]
  },
  {
    "objectID": "data_source.html#sec-collection-info",
    "href": "data_source.html#sec-collection-info",
    "title": "4  The Data Source",
    "section": "4.4 Information Across all NetCDF Files",
    "text": "4.4 Information Across all NetCDF Files\nKnowing what is within one NetCDF file as well as the date range for which we can download them, let’s establish the metadata associated with the NetCDF files as a collection.\n\n4.4.1 Disk Size\nFor the time ranges we cover, Table 6.1 shows how large the set of files per forecast ID are.\n\n\n\nTable 4.6: File Sizes and Counts for Each Forecast ID within the Specified Date Range\n\n\n\n\n\n\n\n\n\n\n\nForecast ID\nDate Range\nSize\nFile Count\n\n\n\n\nBSC00CA12-01\nMarch 4, 2021 - June 27, 2024\n84G\n1077\n\n\nBSC06CA12-01\nMarch 4, 2021 - June 27, 2024\n78G\n1022\n\n\nBSC12CA12-01\nMarch 3, 2021 - June 27, 2024\n79G\n1022\n\n\nBSC18CA12-01\nMarch 4, 2021 - June 27, 2024\n79G\n1023\n\n\nTotal\n\n320G\n4144\n\n\n\n\n\n\n\n\n4.4.2 Temporal Data Availability\n{{&lt; embed data_notebooks/data_source/netcdf_collection_info.ipynb true &gt;}}\nNow that we know what exactly is within a NetCDF file and across all the NetCDF files, we will continue to describe our data curation process. Next we describe how we load all of the data available from UBC onto our machine.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Data Source</span>"
    ]
  },
  {
    "objectID": "data_loading.html",
    "href": "data_loading.html",
    "title": "5  Data Loading",
    "section": "",
    "text": "5.1 Downloading Data Locally\nWe decided to download all the available files from the data source onto our data staging machine to process from there.\nWe created 4 directories for each forecast ID that UBC provides at the following directory on our machine:\nThe following shows our approaches to doing this and discusses how our approach evolved. Note the scripts below refer to varying directories, but through simple copying operations we stored the final downloaded files to the directories listed above.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Loading</span>"
    ]
  },
  {
    "objectID": "data_loading.html#downloading-data-locally",
    "href": "data_loading.html#downloading-data-locally",
    "title": "5  Data Loading",
    "section": "",
    "text": "/usr/sci/cedmav/data/firesmoke\n├── BSC00CA12-01\n├── BSC06CA12-01\n├── BSC12CA12-01\n├── BSC18CA12-01\n\n\n5.1.1 First Approach\nWe delineate our first approach by detailing our download script, which is available in it’s entirety in the side bar.\n\n\nCode\nimport wget\nimport pandas as pd\n\n1ids = [\"BSC18CA12-01\", \"BSC00CA12-01\", \"BSC06CA12-01\", \"BSC12CA12-01\"]\nstart_dates = [\"20210304\", \"20210304\", \"20210304\", \"20210303\"]\nend_dates = [\"20231016\", \"20240210\", \"20231016\", \"20231015\"]\ninit_times = [\"02\", \"08\", \"14\", \"20\"]\n\n2for i in zip(start_dates, end_dates, ids, init_times):\n    start_date = i[0]\n    end_date = i[1]\n    forecast_id = i[2]\n    init_time = i[3]\n\n    dates = pd.date_range(start=start_date, end=end_date)\n    dates = dates.strftime(\"%Y%m%d\").tolist()\n\n3    for date in dates:\n      url = (\n          \"https://firesmoke.ca/forecasts/\"\n          + forecast_id\n          + \"/\"\n          + date\n          + init_time\n          + \"/dispersion.nc\"\n      )\n4      directory = \"/Users/arleth/Mount/firesmoke/\" + forecast_id + \"/dispersion_\" + date + \".nc\"\n      wget.download(url, out=directory)\n\n\n\n1\n\nFirst, create 4 lists containing forecast IDs, the start and end dates we wish to index on, and the smoke forecast initiation times. We will loop through the 4 sets of parameters.\n\n2\n\nIn a for loop, we use pandas to create a list of every date from the start date and end date of the current iteration. We will loop through these dates next.\n\n3\n\nFor each date in the list, we create the url to download the file.\n\n4\n\nFinally, we use wget to download the contents at urlto directory. We append date to the file name so each file downloaded is identifiable by date.\n\n\n\n\nWe assumed that for all URLs, there was an available NetCDF file for download\nHowever, we realized that we downloaded either a NetCDF file or an HTML webpage. Using wget forcibly saved the contents at the URL into a NetCDF file.\nThis issue was not identified until after we visualized each hour of the data, and we noticed gaps and errors in our scripts to create visualizations. See Chapter 7 for further details on identifying these issues. For now, we show our modified approach to downloading the NetCDF files.\n\n\n5.1.2 Second Approach\nOur second approach is similar to the first, except we use requests, an HTTP client library that allows us to see the headers returned from the URL we query. The script is available in the side bar.\n\n\nCode\nimport requests\nimport pandas as pd\nfrom datetime import datetime\n\n1ids = [\"BSC18CA12-01\", \"BSC00CA12-01\", \"BSC06CA12-01\", \"BSC12CA12-01\"]\nstart_dates = [\"20210304\", \"20210304\", \"20210304\", \"20210303\"]\ntoday = datetime.now().strftime(\"%Y%m%d\")\ninit_times = [\"02\", \"08\", \"14\", \"20\"]\n\n2for i in zip(start_dates, ids, init_times):\n    start_date = i[0]\n    forecast_id = i[1]\n    init_time = i[2]\n\n    dates = pd.date_range(start=start_date, end=today)\n    dates = dates.strftime(\"%Y%m%d\").tolist()\n\n3    for date in dates:\n        url = (\n            \"https://firesmoke.ca/forecasts/\"\n            + forecast_id\n            + \"/\"\n            + date\n            + init_time\n            + \"/dispersion.nc\"\n        )\n4        directory = (\n            \"/usr/sci/scratch_nvme/arleth/basura_total/\"\n            + forecast_id\n            + \"/dispersion_\"\n            + date\n            + \".nc\"\n        )\n\n5        response = requests.get(url, stream=True)\n        header = response.headers\n        if (\n            \"Content-Type\" in header\n            and header[\"Content-Type\"] == \"application/octet-stream\"\n        ):\n6            with open(directory, mode=\"wb\") as file:\n                file.write(response.content)\n                print(f\"Downloaded file {directory}\")\n        else:\n            print(header[\"Content-Type\"])\n\n\n\n1\n\nFirst, create 3 lists containing forecast IDs, the start dates we wish to index on, and the smoke forecast initiation times . Notice we define a variable today, this allows us to run this script and query all URLs up to today’s date. Note we ran up to June 27, 2024 for now. We will loop through these sets of parameters.\n\n2\n\nIn a for loop, we use pandas to create a list of every date from the start date and end date of the current iteration. We will loop through these dates next.\n\n3\n\nFor each date in the list, we create the url to download the file.\n\n4\n\nDefine directory, the directory and file name to save the file to.\n\n5\n\nWe use requests to get the HTTP header at url. We inspect the Content-Type and if it is application/octet-stream, we download the file. We confirmed that a URL with a NetCDF file had this content type header.\n\n6\n\nWe write the content to directory, else we print the content header out to check what it is.\n\n\n\n\nThis approach yielded the results we expected, we downloaded only NetCDF files. We had failed downloads which appeared during conversion as described in Chapter 6. We assumed those files were unavailable from UBC which we later confirmed as described in Chapter 7.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Loading</span>"
    ]
  },
  {
    "objectID": "data_conversion.html",
    "href": "data_conversion.html",
    "title": "6  Data Conversion",
    "section": "",
    "text": "6.1 On Data Validation\nWe decided to perform data validation after conversion to the IDX file format. However, we realized that performing data validation both before and after conversion would be best. This is explored further in Chapter 7.\nFor now, the reader should understand that data validation of the NetCDF files is different from data validation of the IDX file. In this chapter, we use the assumption that the NetCDF files we can open have complete and uncorrupted data for conversion to IDX.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Conversion</span>"
    ]
  },
  {
    "objectID": "data_conversion.html#overview",
    "href": "data_conversion.html#overview",
    "title": "6  Data Conversion",
    "section": "6.2 Overview",
    "text": "6.2 Overview\nFor all our conversion script versions, the same general process is followed:\n\nCheck which NetCDF files were successfully downloaded from the data source by attempting to open each downloaded file with xarray.\nObtain a subset of data from the files to create a dataset of chronological, hour by hour, data.\nSave this time series data to an IDX file using the OpenVisus framework.\n\nWe will describe the latest version of our conversion, version 4. Throughout, we will explain how previous attempts were deficient.\nTo see previous attemps in their entirety, refer to the side bar. Please note that the previous scripts were working scripts, therefore they may be incomplete.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Conversion</span>"
    ]
  },
  {
    "objectID": "data_conversion.html#setting-system-directories",
    "href": "data_conversion.html#setting-system-directories",
    "title": "6  Data Conversion",
    "section": "6.3 Setting System Directories",
    "text": "6.3 Setting System Directories\nFirst we set the directory paths we want to use during the conversion process, which is to our 4 directories of NetCDF files for each forecast ID.\n\n\nCode\n1firesmoke_dir = \"/usr/sci/cedmav/data/firesmoke\"\nidx_dir = \"/usr/sci/scratch_nvme/arleth/idx/firesmoke\"\n\n2ids = [\"BSC18CA12-01\", \"BSC00CA12-01\", \"BSC06CA12-01\", \"BSC12CA12-01\"]\nstart_dates = [\"20210304\", \"20210304\", \"20210304\", \"20210303\"]\nend_dates = [\"20240627\", \"20240627\", \"20240627\", \"20240627\"]\n\n\n\n1\n\nEstablish the directory where all forecast ID NetCDFs are stored and where to save our IDX file on the ‘atlantis’ machine.\n\n2\n\nDefine the forecast IDs and dates we will loop over.\n\n\n\n\n\n6.3.1 Rationale and Future Improvements\n\n6.3.1.1 Data Usage\nIn versions 1 and 2 of our conversion attempt, we did not use all four sets of forecast ID files. We only used BSC12CA12-01 files to compile a single dataset. We learned that by not using all four sets of data, the dataset we created was less accurate. See Chapter 7 for further details.\nTherefore we decided to use all four datasets, see Section 6.6 for details. We elect to use dates up to June 27, 2024 as this was the last time we ran our scripts. We have yet to address the issue of how to keep the IDX file constantly up to date with data available up to the present day.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Conversion</span>"
    ]
  },
  {
    "objectID": "data_conversion.html#checking-the-netcdf-files",
    "href": "data_conversion.html#checking-the-netcdf-files",
    "title": "6  Data Conversion",
    "section": "6.4 Checking the NetCDF Files",
    "text": "6.4 Checking the NetCDF Files\nRecall we downloaded all NetCDF files available from UBC onto our machine in their respective directories as follows:\n/usr/sci/cedmav/data/firesmoke\n├── BSC00CA12-01\n├── BSC06CA12-01\n├── BSC12CA12-01\n├── BSC18CA12-01\nHere, we identify which NetCDF files for each forecast ID successfully open with xarray and store them in a dictionary.\nWe also confirm the following conditions we established in Chapter 4 by using dictionaries to track the max values and unique values of these attributes across all files:\n\nAll files across all four forecast IDs have the same NROWS, XORIG, YORIG, XCELL, YCELL values.\nSome files have either NCOLS = 1041 or NCOLS = 1081, but always NROWS = 381.\n\n\n\nCode\nimport os\nimport xarray as xr\nimport numpy as np\nimport tqdm\n\n1successful_files = {id_: [] for id_ in ids}\n\n2max_ncols = {id_: 0 for id_ in ids}\nmax_nrows = {id_: 0 for id_ in ids}\n3ncols = {id_: set() for id_ in ids}\nnrows = {id_: set() for id_ in ids}\n\n4max_grid_x = {id_: {\"xorig\": 0.0, \"xcell\": 0.0} for id_ in ids}\nmax_grid_y = {id_: {\"yorig\": 0.0, \"ycell\": 0.0} for id_ in ids}\n5xorigs = {id_: set() for id_ in ids}\nxcells = {id_: set() for id_ in ids}\nyorigs = {id_: set() for id_ in ids}\nycells = {id_: set() for id_ in ids}\n\n6for id_ in ids:\n7    file_names = os.listdir(f\"{firesmoke_dir}/{id_}/\")\n\n8    for file in tqdm(file_names):\n9        path = f\"{firesmoke_dir}/{id_}/{file}\"\n\n10        try:\n            ds = xr.open_dataset(path)\n\n11            successful_files[id_].append(file)\n\n12            max_ncols[id_] = max(max_ncols[id_], ds.NCOLS)\n            max_nrows[id_] = max(max_nrows[id_], ds.NROWS)\n            max_grid_x[id_][\"xorig\"] = max(max_grid_x[id_][\"xorig\"], ds.XORIG, key=abs)\n            max_grid_y[id_][\"yorig\"] = max(max_grid_y[id_][\"yorig\"], ds.YORIG, key=abs)\n            max_grid_x[id_][\"xcell\"] = max(max_grid_x[id_][\"xcell\"], ds.XCELL, key=abs)\n            max_grid_y[id_][\"ycell\"] = max(\n                max_grid_y[id_][\"ycell\"], ds.YCELL, key=abs\n            )\n\n13            ncols[id_].add(ds.NCOLS)\n            nrows[id_].add(ds.NROWS)\n            xorigs[id_].add(ds.XORIG)\n            yorigs[id_].add(ds.YORIG)\n            xcells[id_].add(ds.XCELL)\n            ycells[id_].add(ds.YCELL)\n\n14        except:\n            continue\n\n15for id_ in successful_files:\n    successful_files[id_] = np.sort(successful_files[id_]).tolist()\n\n\n\n1\n\nInitialize a dictionary to hold an empty list for each forecast ID. We update it with the file names that successfully open under the forecast ID directory.\n\n2\n\nInitialize dictionaries to hold an integer for each forecast ID. We update it to hold the maximum NCOLS/NROWS value available within forecast ID’s set of NetCDF files.\n\n3\n\nInitialize dictionaries to hold a set for each forecast ID. We update the set to hold all the unique NCOLS/NROWS values available within the forecast ID’s set of NetCDF files.\n\n4\n\nInitialize dictionaries to hold a dictionary of xorig/yorig and xcell/ycell values for each forecast ID. We update it to hold the maximum xorig/yorig and xcell/ycell pairs available within the forecast ID’s set of NetCDF files.\n\n5\n\nInitialize dictionaries to track unique xorig/yorig and xcell/ycell values.\n\n6\n\nFor each forecast ID, we populate the dictionaries above.\n\n7\n\nObtain a list of file names under the directory for id_. We loop through each file next.\n\n8\n\nBegin loop over each file. Note tqdm is just an accessory for generating a visible status bar in our Jupyter Notebook.\n\n9\n\nObtain absolute path name to current file.\n\n10\n\nHere we use a try statement since opening the file with xarray may lead to an error. except allows us to catch the exception accordingly and continue trying to open each file.\n\n11\n\nAt this line, the file opened without issue in xarray, so append this file name to the id_ list in the successful_files dictionary.\n\n12\n\nUse max to save the largest values in our dictionaries accordingly.\n\n13\n\nUpdate the dictionaries of sets with the file’s attributes, to ensure we catch all unique values.\n\n14\n\nIf the file did not open during the try continue to the next file.\n\n15\n\nSort the lists of successfully opened files by name, so they are in chronological order.\n\n\n\n\nThe following shows the information gathered:\n\nBSC18CA12-01BSC00CA12-01BSC06CA12-01BSC12CA12-01\n\n\ndataset: BSC18CA12-01\nNumber of successful files: 1010\nMax cell sizes: max_ncols = 1081 and max_nrows = 381\nMax xorig & xcell: {'xorig': -160.0, 'xcell': 0.10000000149011612}\nMax yorig & ycell: {'yorig': 32.0, 'ycell': 0.10000000149011612}\nncols: {1081, 1041}\nnrows: {381}\nxorigs: {-160.0, -156.0}\nyorigs: {32.0}\nxcells: {0.10000000149011612}\nycells: {0.10000000149011612}\n\n\ndataset: BSC00CA12-01\nNumber of successful files: 1067\nMax cell sizes: max_ncols = 1081 and max_nrows = 381\nMax xorig & xcell: {'xorig': -160.0, 'xcell': 0.10000000149011612}\nMax yorig & ycell: {'yorig': 32.0, 'ycell': 0.10000000149011612}\nncols: {1081, 1041}\nnrows: {381}\nxorigs: {-160.0, -156.0}\nyorigs: {32.0}\nxcells: {0.10000000149011612}\nycells: {0.10000000149011612}\n\n\ndataset: BSC06CA12-01\nNumber of successful files: 997\nMax cell sizes: max_ncols = 1081 and max_nrows = 381\nMax xorig & xcell: {'xorig': -160.0, 'xcell': 0.10000000149011612}\nMax yorig & ycell: {'yorig': 32.0, 'ycell': 0.10000000149011612}\nncols: {1081, 1041}\nnrows: {381}\nxorigs: {-160.0, -156.0}\nyorigs: {32.0}\nxcells: {0.10000000149011612}\nycells: {0.10000000149011612}\n\n\ndataset: BSC12CA12-01\nNumber of successful files: 1003\nMax cell sizes: max_ncols = 1081 and max_nrows = 381\nMax xorig & xcell: {'xorig': -160.0, 'xcell': 0.10000000149011612}\nMax yorig & ycell: {'yorig': 32.0, 'ycell': 0.10000000149011612}\nncols: {1081, 1041}\nnrows: {381}\nxorigs: {-160.0, -156.0}\nyorigs: {32.0}\nxcells: {0.10000000149011612}\nycells: {0.10000000149011612}\n\n\n\n\n6.4.1 Rationale and Future Improvements\n\n6.4.1.1 Unloadable Files\nOn our first attempt to convert the data we discovered that various files failed to open. Therefore, we used a dictionary to keep track of which files successfully open.\n\n\n6.4.1.2 Varying Grid Size\nIn this step we collect attribute information about the two different grids used in the dataset. We proceed to use these attributes to resample the grids accordingly, see Section 6.5.\n\n\n6.4.1.3 Optimization and Scaling\nOne improvement to this step is to stop tracking maxes and unique values seperately. Instead, we could just track unique values then get maxes from there. Furthermore, modularizing this process such that it handles the possibility of new grids being used in the dataset would make the conversion script more scalable.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Conversion</span>"
    ]
  },
  {
    "objectID": "data_conversion.html#sec-resampling",
    "href": "data_conversion.html#sec-resampling",
    "title": "6  Data Conversion",
    "section": "6.5 Preparing Resampling Grids",
    "text": "6.5 Preparing Resampling Grids\nNow that we have the sets of openable files, we begin to handle the need to resample data. To resample arrays of shape 381×1041 to 381×1081, we use the SciPy griddata function from the interpolate package. This function gives interpolated values on set of points xi from a set of points with corresponding values. We refer the reader to SciPy’s documentation for details.\nIn this step, we obtain the grids we wish to use as our points and xi. See Section 6.5.2 for how we use these grids with the griddata function.\n\n\n6.5.1 Generate Grids of Latitude and Longitude Points\nRecall we can generate a set of latitude and longitude coordinates by using the attributes given in each NetCDF file, see Section 4.3.3 for an example. Here we generate two sets of latitude and longitude coordinates for each grid size.\n\n\nCode\n1max_xorig = max_grid_x[ids[0]]['xorig']\nmax_xcell = max_grid_x[ids[0]]['xcell']\nmax_yorig = max_grid_y[ids[0]]['yorig']\nmax_ycell = max_grid_y[ids[0]]['ycell']\n\n2big_lon = np.linspace(max_xorig, max_xorig + max_xcell * (max_ncols[ids[0]] - 1), max_ncols[ids[0]])\nbig_lat = np.linspace(max_yorig, max_yorig + max_ycell * (max_nrows[ids[0]] - 1), max_nrows[ids[0]])\n\n3big_lon_pts, big_lat_pts = np.meshgrid(big_lon, big_lat)\nbig_tups = np.array([tup for tup in zip(big_lon_pts.flatten(), big_lat_pts.flatten())])\n\n4sml_ds = xr.open_dataset(firesmoke_dir + \"/BSC00CA12-01/dispersion_20210304.nc\")\nsml_lon = np.linspace(sml_ds.XORIG, sml_ds.XORIG + sml_ds.XCELL * (sml_ds.NCOLS - 1), sml_ds.NCOLS)\nsml_lat = np.linspace(sml_ds.YORIG, sml_ds.YORIG + sml_ds.YCELL * (sml_ds.NROWS - 1), sml_ds.NROWS)\n\n5sml_lon_pts, sml_lat_pts = np.meshgrid(sml_lon, sml_lat)\nsml_tups = np.array([tup for tup in zip(sml_lon_pts.flatten(), sml_lat_pts.flatten())])\n\n\n\n1\n\nGet the x/y origin and cell size parameters for the big 381×1081 grid.\n\n2\n\nGenerate one two lists, defining a grid of latitudes and longitudes.\n\n3\n\nUsing big_lon and big_lat, use meshgrid to generate our 381×1081 set of longitudes and latitudes.\n\n4\n\nOpen a file that uses the small 381×1041 grid. Then, use the attributes in that file to generate two lists defining a grid of latitudes and longitudes.\n\n5\n\nUsing sml_lon and sml_lat, use meshgrid to generate our 381×1041 set of longitudes and latitudes.\n\n\n\n\nSee below for an example of using these latitude and longitude grids to resample data on a 381×1041 grid to a 381×1081 grid.\n\n\n6.5.2 Example with griddata\nNow that we have the two sets of latitude and longitude points, we show an example of how these are used to resample an array of data from a 381×1041 grid to a 381×1081 grid.\n\nIn this example we use the latitude and longitude points generated from the attributes determine across all NetCDF files.\n\n\nCode\nimport numpy as np\nimport xarray as xr\n\n\n\n\nCode\n1max_xorig = -160.0\n2max_xcell = 0.10000000149011612\n3max_yorig = 32.0\n4max_ycell = 0.10000000149011612\n\n5big_lon = np.linspace(max_xorig, max_xorig + max_xcell * (1081 - 1), 1081)\n6big_lat = np.linspace(max_yorig, max_yorig + max_ycell * (381 - 1), 381)\n\n7big_lon_pts, big_lat_pts = np.meshgrid(big_lon, big_lat)\n8big_tups = np.array([tup for tup in zip(big_lon_pts.flatten(), big_lat_pts.flatten())])\n\n9sml_ds = xr.open_dataset(\"dispersion_20210304.nc\")\n10sml_lon = np.linspace(sml_ds.XORIG, sml_ds.XORIG + sml_ds.XCELL * (sml_ds.NCOLS - 1), sml_ds.NCOLS)\n11sml_lat = np.linspace(sml_ds.YORIG, sml_ds.YORIG + sml_ds.YCELL * (sml_ds.NROWS - 1), sml_ds.NROWS)\n\n12sml_lon_pts, sml_lat_pts = np.meshgrid(sml_lon, sml_lat)\n13sml_tups = np.array([tup for tup in zip(sml_lon_pts.flatten(), sml_lat_pts.flatten())])\n\n\n\n1\n\nDefine the maximum x-origin coordinate.\n\n2\n\nDefine the maximum x-cell size.\n\n3\n\nDefine the maximum y-origin coordinate.\n\n4\n\nDefine the maximum y-cell size.\n\n5\n\nCreate an array for the large longitude grid.\n\n6\n\nCreate an array for the large latitude grid.\n\n7\n\nCreate a meshgrid of points using the large longitude and latitude arrays.\n\n8\n\nCreate a flattened array of tuples representing the large grid points.\n\n9\n\nOpen the small dataset using xarray.\n\n10\n\nCreate an array for the small longitude grid.\n\n11\n\nCreate an array for the small latitude grid.\n\n12\n\nCreate a meshgrid of points using the small longitude and latitude arrays.\n\n13\n\nCreate a flattened array of tuples representing the small grid points.\n\n\n\n\n\n\nCode\nprint(f'Using the large grid, we have {np.shape(big_tups)[0]} lat/lon points to sample on.')\nprint(f'Using the small grid, we have {np.shape(sml_tups)[0]} lat/lon points to sample from.')\n\n\nUsing the large grid, we have 411861 lat/lon points to sample on.\nUsing the small grid, we have 396621 lat/lon points to sample from.\n\n\nLet’s get the data at timestep 0 inside dispersion_20210304.nc, which uses a grid of size 381×1041.\n\n\nCode\ntimestep = 0\n\nvals = np.squeeze(sml_ds['PM25'].values)\n\nprint(f'The shape of the PM25 array at timestep {timestep} is {np.shape(vals[timestep])}')\n\n\nThe shape of the PM25 array at timestep 0 is (381, 1041)\n\n\nWe use the following parameters with griddata to resample vals:\ngriddata(points, values, xi, method='cubic', fill_value)\n\npoints: Data point coordinates.\nvalues: Data values.\nxi: Points at which to interpolate data.\nmethod: cubic, 2D: Return the value determined from a piecewise cubic, continuously differentiable (C1), and approximately curvature-minimizing polynomial surface.\nfill_value: Value used to fill in for requested points outside of the convex hull of the input points.\n\n\n\nCode\nfrom scipy.interpolate import griddata\n\n1points = sml_tups\n2values = vals[timestep].flatten()\n3xi = big_tups\n4method = 'cubic'\n5fill_value = 0\n\n6arr = griddata(points, values, xi, method, fill_value)\n\n7print(f'We have interpolated 381×1081 = {np.shape(arr)[0]} points.')\n\n\n\n1\n\nThe points we want to sample from.\n\n2\n\nThe values for the grid we want to sample from, flattened into a 1D array.\n\n3\n\nThe points we want to sample to.\n\n4\n\nThe interpolation method used (‘cubic’ in this case).\n\n5\n\nThe fill value to use (0 instead of NaN).\n\n6\n\nPerform the interpolation.\n\n7\n\nPrint the number of points interpolated.\n\n\n\n\nWe have interpolated 381×1081 = 411861 points.\n\n\nNotice that we have interpolated values that are negative.\n\n\nCode\nprint(f'The minimum PM25 value in our interpolated values is {np.min(arr)}')\n\n\nThe minimum PM25 value in our interpolated values is -0.0004518490243624799\n\n\nWe change values less than our specified threshold to 0. We then reshape the values to be 381×1081 and make all the values of type float32, as this number type is used in the original NetCDF file for PM25 values.\n\n\nCode\n1arr[arr &lt; 1e-15] = 0\n\n2arr = arr.reshape((len(big_lat), len(big_lon)))\n\n3arr = arr.astype(np.float32)\n\n4print(f'The shape of the resampled PM25 array at timestep {timestep} is {np.shape(arr)}')\n\n\n\n1\n\nAny values that are less than a given threshold, make it 0.\n\n2\n\nReshape the result to match the new grid shape.\n\n3\n\nCast number to float32.\n\n4\n\nPrint the shape of the resampled PM25 array at the given timestep.\n\n\n\n\nThe shape of the resampled PM25 array at timestep 0 is (381, 1081)\n\n\nNow we can visualize the resampled values:\n\n\nCode\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\n\n1my_fig, my_plt = plt.subplots(figsize=(15, 6), subplot_kw=dict(projection=ccrs.PlateCarree()))\n\n2my_norm = \"log\"\n3my_extent = [np.min(big_lon), np.max(big_lon), np.min(big_lat), np.max(big_lat)]\n4my_aspect = 'auto'\n5my_origin = 'lower'\n6my_cmap = 'viridis'\n\nplot = my_plt.imshow(arr, norm=my_norm, extent=my_extent, \n7          aspect=my_aspect, origin=my_origin, cmap=my_cmap)\n\n8my_plt.coastlines()\n\n9my_plt.gridlines(draw_labels=True)\n\n10my_fig.colorbar(plot, location='right', label='ug/m^3')\n\n11my_fig.supxlabel('Longitude')\n12my_fig.supylabel('Latitude')\n\n13my_fig.suptitle('Ground level concentration of PM2.5 microns and smaller')\n\n14my_plt.set_title(f'Timestep {timestep}')\n\n15plt.show()\n\n\n\n1\n\nInitialize a figure and plot, so we can customize figure and plot of data.\n\n2\n\nColor PM25 values on a log scale, since values are small.\n\n3\n\nThis will number our x and y axes based on the longitude latitude range.\n\n4\n\nEnsure the aspect ratio of our plot fits all data, matplotlib can do this automatically.\n\n5\n\nTell matplotlib our origin is the lower-left corner.\n\n6\n\nSelect a colormap for our plot and the color bar on the right.\n\n7\n\nCreate our plot using imshow.\n\n8\n\nDraw coastlines.\n\n9\n\nDraw latitude longitude lines.\n\n10\n\nAdd a colorbar to our figure, based on the plot we just made above.\n\n11\n\nSet x axis label on our ax.\n\n12\n\nSet y axis label on our ax.\n\n13\n\nSet title of our figure.\n\n14\n\nSet title of our plot as the timestamp of our data.\n\n15\n\nShow the resulting visualization.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.5.3 Rationale and Future Improvements\n\n6.5.3.1 Discovering Varying Grids\nWe discovered the two grid shapes in version 1 of our conversion script. We found this after noticing the smoke data visualizations were nonsensical.\nFor example, the visualizations showed smoke eminating from the ocean, as shown in Figure 6.1.\n\n\n\n\n\n\n\n\n\n\n\n(a) Plotting Data from 381×1041 Grid on 381×1081 Grid\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Plotting Data Resampled from 381×1041 Grid onto 381×1081 Grid\n\n\n\n\n\n\n\nFigure 6.1: Visualization of Timestamp March 5, 2021 00:00:00 using IDX file Created in Conversion Script Version 1\n\n\n\nFor further details on how we investigated this issue, see Chapter 7.\n\n\n6.5.3.2 Handling Various Grids\nWe considered various approaches to handling the fact that the data was on a 381×1041 grid or 381×1081 grid.\n\n\n\nTable 6.1: The Weaknesses of Approaches to Handling Varying Array Sizes\n\n\n\n\n\n\n\n\n\nApproach\nWeakness\n\n\n\n\nExclude arrays with 1041 columns.\nThrowing away those data points would discard all the information they hold.\n\n\nForce data with 1041 columns into an array with 1081 columns without resampling.\nThis results in unused columns within the 1081-column array, leading to discontinuities and potential artifacts in the data representation.\n\n\nCrop arrays on 1081 columns to 1041 columns\nCropping the data would result in loss of information.\n\n\n\n\n\n\nThe approach we chose was to resample the data with 1041 columns to arrays with 1081 columns. This produced the most visually appealing result and preserved the most information possible.\n\n\n6.5.3.3 Scaling\nOne future improvement is to generalize precomputing additional grids if in the future the smoke forecasts change their grid size again. As of now we manually compute points for the 2 grid sizes.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Conversion</span>"
    ]
  },
  {
    "objectID": "data_conversion.html#sec-sequencing",
    "href": "data_conversion.html#sec-sequencing",
    "title": "6  Data Conversion",
    "section": "6.6 Sequencing of NetCDF Files",
    "text": "6.6 Sequencing of NetCDF Files\nAt this point we have the lists of openable files and our resampling grids. Now we will determine which files to use and in what order.\nThe unique challenge of UBC’s short term dataset is that the forecasts overlap, creating staggered predictions as shown in Section 4.4.2. Additionally, there are often missing files or data points. In the following sections we show how we use the data given these factors.\n\n6.6.1 Hourly Data per Forecast ID Dictionary\nRecall that within each set of Forecast ID files, some files failed to download or open. Therefore, we check exactly what set of hours are available in each collection of forecast ID NetCDF files and store that information in a dictionary.\nBelow you can see the first few entries of this dictionary:\nTo generate this dictionary, we did the following:\n\n\nCode\n1id_sets = {id_: {} for id_ in ids}\n\nfor id_ in ids:\n2    for file in tqdm(successful_files[id_]):\n3        path = f\"{firesmoke_dir}/{id_}/{file}\"\n\n4        ds = xr.open_dataset(path)\n\n5        for h in range(ds.sizes[\"TSTEP\"]):\n            id_sets[id_][(file, parse_tflag(ds[\"TFLAG\"].values[h][0]))] = h\n\n\n\n1\n\nInitialize a dictionary for each forecast ID.\n\n2\n\nFor all successfully opened files per forecast ID.\n\n3\n\nBuild path string to the file.\n\n4\n\nOpen the file with xarray.\n\n5\n\nFor every time step, get the dictionary at key id_. Then, add a key, value pair. The key is a tuple with the current file name and the TFLAG at h parsed with parse_tflag. The value is the index h.\n\n\n\n\nNow that we have an indexable set of all available hours for each forecast ID, we can generate the sequence to choose the best predictions for every time step.\n\n\n6.6.2 Utility Functions\nRecall that for each timestep, there are various predictions available as shown in Section 4.4.2.\nThe following utility functions encode the logic used for selecting the most accurate PM2.5 prediction per time step. Most accurate means, the prediction produced by the forecast run as close as possible to the time step of interest.\n\n\nCode\ndef prev_id(curr_id, verbose):\n    \"\"\"\n    Return the string of the previous dataset ID to use based on the current ID.\n    'Previous' means, last most recently updated forecast before curr_id.\n\n    Details on forecast update time can be found here: https://firesmoke.ca/forecasts/\n\n    Listed in order: [\"BSC18CA12-01\", \"BSC00CA12-01\", \"BSC06CA12-01\", \"BSC12CA12-01\"]\n\n    :param string curr_id: the ID used:\n    :param boolean verbose: whether to enable print statements for debugging:\n    \"\"\"\n    ret = \"\"\n\n    if curr_id == \"BSC18CA12-01\":\n        ret = \"BSC12CA12-01\"\n    if curr_id == \"BSC00CA12-01\":\n        ret = \"BSC18CA12-01\"\n    if curr_id == \"BSC06CA12-01\":\n        ret = \"BSC00CA12-01\"\n    if curr_id == \"BSC12CA12-01\":\n        ret = \"BSC06CA12-01\"\n\n    if verbose:\n        print(f\"prev_id({curr_id}) = {ret}\")\n\n    return ret\n\n\n\n\nCode\ndef get_id_from_date(date, verbose):\n    \"\"\"\n    Return the string of the dataset ID to use based on the date and hour given.\n\n    We aim to use the dataset that provides the latest forecast update available for the hour.\n\n    Details on forecast update time can be found here: https://firesmoke.ca/forecasts/\n\n    :param datetime date: pandas timestamp of the YYYYMMDD 00:00:00 date:\n    :param boolean verbose: whether to enable print statements for debugging:\n    \"\"\"\n    ret = \"\"\n\n1    if date &lt;= date.replace(hour=2):\n        ret = \"BSC12CA12-01\"\n    if date &gt;= date.replace(hour=3) and date &lt;= date.replace(hour=8):\n        ret = \"BSC18CA12-01\"\n    if date &gt;= date.replace(hour=9) and date &lt;= date.replace(hour=14):\n        ret = \"BSC00CA12-01\"\n    if date &gt;= date.replace(hour=15) and date &lt;= date.replace(hour=20):\n        ret = \"BSC06CA12-01\"\n    if date &gt;= date.replace(hour=21):\n        ret = \"BSC12CA12-01\"\n\n    if verbose:\n        print(f\"get_id_from_date({date}) = {ret}\")\n    return ret\n\n\n\n1\n\nBased on the given date, use the optimal forecast ID.\n\n\n\n\n\n\nCode\ndef dispersion_date_str(date, id_, verbose):\n    \"\"\"\n    For a given date object and forecats ID, generate the name for the dispersion file in which the optimal prediction is located in.\n    :param pd.Timestamp date: pandas timestamp of the date to make file name string out of:\n    :param string id_: string with the dataset id to use\n    :param boolean verbose: whether to enable print statements for debugging:\n    \"\"\"\n    ret = \"\"\n\n1    if id_ == \"BSC12CA12-01\":\n        new_date = date + datetime.timedelta(days=-1)\n        ret = f'dispersion_{new_date.strftime(\"%Y%m%d\")}.nc'\n2    else:\n        ret = f'dispersion_{date.strftime(\"%Y%m%d\")}.nc'\n\n    if verbose:\n        print(f\"dispersion_date_str({date}, {id_}) = {ret}\")\n\n    return ret\n\n\n\n1\n\nBSC00CA12-01 generates the first hours of the given date in ‘yesterday’s’ file. For example, the hours 12am-6am for January 2, 2023 are generated in dispersion_01012023.nc in the BSC00CA12-01 dataset.\n\n2\n\nFor all other forecast IDs, the optimal hour for all hours is in “today’s” file, where ‘today’ is the date given.\n\n\n\n\n\n\n6.6.3 Populating the idx_calls Array\nHere, we describe how we use our utility functions to parse all the NetCDF files we have to populate our idx_calls array. idx_calls is used in the final step by defining which predictions to load and in what order. See Section 6.7 for details on exact usage.\nFirst, we initialize our variables.\n\n\nCode\n1idx_calls = []\n\n2start_date = datetime.datetime.strptime(\"20210304\", \"%Y%m%d\")\nend_date = datetime.datetime.strptime(\"20240627\", \"%Y%m%d\")\n\n3current_date = start_date\ncurrent_hour = datetime.datetime(current_date.year, current_date.month, current_date.day)\n\n4file_str = ''\n\n5verbose = 1\n\n\n\n1\n\nArrays to hold the final order we will index files\n\n2\n\nDefine the start and end dates we will step through.\n\n3\n\nInitialize these variables to the start date and time. We use these to increment through to the end_date next.\n\n4\n\nInitialize a variable for holding the current file name in use.\n\n5\n\nTell utility functions to print for debugging.\n\n\n\n\nThen, we use a while loop to populate idx_calls with the optimal sequence.\n\n\nCode\n1while current_date &lt;= end_date:\n2    while current_hour &lt; current_date + datetime.timedelta(days=1):\n        prev_day_count = 0\n        found = 0\n\n3        while found == 0 and prev_day_count &lt;= 4:\n4            curr_date = current_hour + datetime.timedelta(days=-prev_day_count)\n\n5            curr_id = get_id_from_date(curr_date, verbose)\n\n            forecast_search_count = 0\n\n6            while found == 0 and forecast_search_count &lt; 4:\n7                file_str = dispersion_date_str(curr_date, curr_id, verbose)\n\n8                if (file_str, current_hour) in id_sets[curr_id]:\n                    update_idx_calls(\n9                        idx_calls, curr_id, (file_str, current_hour), id_sets\n                    )\n                    found = 1\n                else:\n                    forecast_search_count += 1\n\n10                curr_id = prev_id(curr_id, verbose)\n\n11            prev_day_count += 1\n\n12        current_hour += datetime.timedelta(hours=1)\n\n13    current_date += datetime.timedelta(days=1)\n\n\n\n1\n\nIterate from current_date by one hour increments until the end_date is reached.\n\n2\n\nIterate through each hour until the next day starts.\n\n3\n\nIf no optimal prediction is found and not more than 4 days have been searched.\n\n4\n\nSubtract prev_day_count from current_hour to test earlier dates.\n\n5\n\nRetrieve the forecast ID corresponding to the curr_date being tested.\n\n6\n\nLoop through potential forecasts until one is found or 4 different forecasts have been checked.\n\n7\n\nConstruct the filename string for the forecast dataset corresponding to curr_date and curr_id.\n\n8\n\nVerify if the file_str and current_hour combination exists in id_sets for the given curr_id.\n\n9\n\nIf available, record the forecast ID and time in idx_calls, and mark this combination as ‘found.’\n\n10\n\nIf the current ID doesn’t have the forecast, move to a previous forecast ID.\n\n11\n\nIncrement prev_day_count and repeat the search for an earlier day.\n\n12\n\nIncrement current_hour to move to the next hour in the current_date.\n\n13\n\nAfter completing all hours of a day, move to the next day.\n\n\n\n\nThe following shows the first 2 and final 2 entires of the idx_calls array.\n\n\nCode\n[['BSC12CA12-01',\n  'dispersion_20210303.nc',\n  datetime.datetime(2021, 3, 4, 0, 0),\n  3],\n ['BSC12CA12-01',\n  'dispersion_20210303.nc',\n  datetime.datetime(2021, 3, 4, 1, 0),\n  4],\n ...\n ['BSC12CA12-01',\n  'dispersion_20240626.nc',\n  datetime.datetime(2024, 6, 27, 22, 0),\n  25],\n ['BSC12CA12-01',\n  'dispersion_20240626.nc',\n  datetime.datetime(2024, 6, 27, 23, 0),\n  26]]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Conversion</span>"
    ]
  },
  {
    "objectID": "data_conversion.html#creating-the-idx-file",
    "href": "data_conversion.html#creating-the-idx-file",
    "title": "6  Data Conversion",
    "section": "6.7 Creating the IDX File",
    "text": "6.7 Creating the IDX File\nAt this point, we have precomputed the order in which we will load and write each array to our IDX file. We will now put everything together to generate our single dataset.\n\n\nCode\n1f = Field(\"PM25\", \"float32\")\n\n2db = CreateIdx(\n    url=idx_dir + \"/firesmoke.idx\",\n    fields=[f],\n    dims=[int(max_ncols[ids[0]]), int(max_nrows[ids[0]])],\n    time=[0, len(idx_calls) - 1, \"%00000000d/\"],\n)\n\n3tstep = 0\n\n4thresh = 1e-15\n\n5for call in tqdm(idx_calls):\n    curr_id = call[0]\n    curr_file = call[1]\n    tstep_index = call[3]\n\n6    ds = xr.open_dataset(f\"{firesmoke_dir}/{curr_id}/{curr_file}\")\n\n7    file_vals = np.squeeze(ds[\"PM25\"].values)\n\n8    resamp = ds.XORIG != max_xorig\n\n9    if resamp:\n        file_vals_resamp = griddata(\n            sml_tups,\n            file_vals[tstep_index].flatten(),\n            big_tups,\n            method=\"cubic\",\n            fill_value=0,\n        )\n\n10        file_vals_resamp[file_vals_resamp &lt; thresh] = 0\n\n11        file_vals_resamp = file_vals_resamp.reshape((len(big_lat), len(big_lon)))\n\n12        db.write(data=file_vals_resamp.astype(np.float32), field=f, time=tstep)\n13    else:\n        db.write(data=file_vals[tstep_index], field=f, time=tstep)\n\n14    tstep = tstep + 1\n\n\n\n1\n\nCreate an OpenVisus field to hold the PM25 variable data.\n\n2\n\nCreate the IDX file wherein url is the location to write the file, fields holds the data variables we will save, dims represents the shape of each array, and time defines how many time steps there are.\n\n3\n\nWe will usethis to keep track of which time step we are on as we step through our idx_calls.\n\n4\n\nThreshold to use to change small-enough resampled values to 0.\n\n5\n\nGet the information for current time step, in particular the [curr_id, file_str, parse_tflag(ds[‘TFLAG’].values[tstep_idx][0]), tstep_idx]\n\n6\n\nLoad the current file with xarray.\n\n7\n\nGet the full array of PM25 values in the file.\n\n8\n\nIf ds.XORIG is not already for the 381×1081 grid, we need to resample it to the larger grid.\n\n9\n\nUsing gridddata, interpolate the values on a 381×1081 grid using the precomputed lat/lon points.\n\n10\n\nAny values that are less than our threshold should have a value of 0. WHY\n\n11\n\nReshape the interpolated values to 381×1081.\n\n12\n\nWrite the resampled values for hour h to timestep t and field f of our IDX file.\n\n13\n\nThese values are already on a 381×1081 grid, so write the values at hour h to timestep t and field f of our IDX file.\n\n14\n\nIncrement to the next timestep for writing to IDX.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Conversion</span>"
    ]
  },
  {
    "objectID": "data_validation.html",
    "href": "data_validation.html",
    "title": "7  Data Validation",
    "section": "",
    "text": "7.1 Data Validation vs Data Exploration\nData exploration enlightens one on the contents of the data and metadata one presumes they have. We performed data exploration by loading and visualizing a few files. This allowed us to understand what data UBC aims to provide.\nWhat data exploration does not do is explain the origin of issues such missing or seemingly corrupt data. Data exploration uses the assumption that the data is perfect.\nData validation forces one to consider, where do issues that appear in the data come from, and are these issues with the data or with the systems used to access and manipulate the data?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Validation</span>"
    ]
  },
  {
    "objectID": "data_validation.html#data-loading",
    "href": "data_validation.html#data-loading",
    "title": "7  Data Validation",
    "section": "7.2 Data Loading",
    "text": "7.2 Data Loading\nMost of the issues we faced during data curation resulted from failing to validate our data loading system.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Validation</span>"
    ]
  },
  {
    "objectID": "data_validation.html#data-conversion",
    "href": "data_validation.html#data-conversion",
    "title": "7  Data Validation",
    "section": "7.3 Data Conversion",
    "text": "7.3 Data Conversion",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Validation</span>"
    ]
  },
  {
    "objectID": "data_conversion.html#sec-create-idx",
    "href": "data_conversion.html#sec-create-idx",
    "title": "6  Data Conversion",
    "section": "6.7 Creating the IDX File",
    "text": "6.7 Creating the IDX File\nAt this point, we have precomputed the order in which we will load and write each array to our IDX file. We will now use idx_calls to create the final IDX file containing our single dataset.\n\n\nCode\n1f = Field(\"PM25\", \"float32\")\n\n2db = CreateIdx(\n    url=idx_dir + \"/firesmoke.idx\",\n    fields=[f],\n    dims=[int(max_ncols[ids[0]]), int(max_nrows[ids[0]])],\n    time=[0, len(idx_calls) - 1, \"%00000000d/\"],\n)\n\n3tstep = 0\n\n4thresh = 1e-15\n\n5for call in tqdm(idx_calls):\n    curr_id = call[0]\n    curr_file = call[1]\n    tstep_index = call[3]\n\n6    ds = xr.open_dataset(f\"{firesmoke_dir}/{curr_id}/{curr_file}\")\n\n7    file_vals = np.squeeze(ds[\"PM25\"].values)\n\n8    resamp = ds.XORIG != max_xorig\n\n9    if resamp:\n        file_vals_resamp = griddata(\n            sml_tups,\n            file_vals[tstep_index].flatten(),\n            big_tups,\n            method=\"cubic\",\n            fill_value=0,\n        )\n\n10        file_vals_resamp[file_vals_resamp &lt; thresh] = 0\n\n11        file_vals_resamp = file_vals_resamp.reshape((len(big_lat), len(big_lon)))\n\n12        db.write(data=file_vals_resamp.astype(np.float32), field=f, time=tstep)\n13    else:\n        db.write(data=file_vals[tstep_index], field=f, time=tstep)\n\n14    tstep = tstep + 1\n\n\n\n1\n\nCreate an OpenVisus field to hold the PM25 variable data.\n\n2\n\nCreate the IDX file wherein url is the location to write the file, fields holds the data variables we will save, dims represents the shape of each array, and time defines how many time steps there are.\n\n3\n\nWe will usethis to keep track of which time step we are on as we step through our idx_calls.\n\n4\n\nThreshold to use to change small-enough resampled values to 0.\n\n5\n\nGet the information for current time step, in particular the [curr_id, file_str, parse_tflag(ds[‘TFLAG’].values[tstep_idx][0]), tstep_idx]\n\n6\n\nLoad the current file with xarray.\n\n7\n\nGet the full array of PM25 values in the file.\n\n8\n\nIf ds.XORIG is not already for the 381×1081 grid, we need to resample it to the larger grid.\n\n9\n\nUsing gridddata, interpolate the values on a 381×1081 grid using the precomputed lat/lon points.\n\n10\n\nAny values that are less than our threshold should have a value of 0. WHY\n\n11\n\nReshape the interpolated values to 381×1081.\n\n12\n\nWrite the resampled values for hour h to timestep t and field f of our IDX file.\n\n13\n\nThese values are already on a 381×1081 grid, so write the values at hour h to timestep t and field f of our IDX file.\n\n14\n\nIncrement to the next timestep for writing to IDX.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Conversion</span>"
    ]
  },
  {
    "objectID": "data_validation.html#data-validation-for-data-loading",
    "href": "data_validation.html#data-validation-for-data-loading",
    "title": "7  Data Validation",
    "section": "7.2 Data Validation for Data Loading",
    "text": "7.2 Data Validation for Data Loading\nHere, we will describe how we discovered the consequences of failing to validate that the files we downloaded were true NetCDF files instead of HTML webpages, as described in Chapter 5.\n\n7.2.1 The Problem\nWhen doing rudimentary visualizations we saw missing timesteps from our final dataset in the IDX file. We assumed that any missing time steps were due to the files being unavailable from the data source. However, we decided to validate which time steps were missing and why, in case the cause for apparently missing time steps was our fault.\n\n\n7.2.2 Visualizing all Timesteps\nTo determine exactly which time steps were unavailable, we decided to load and visualize every timestep from March 3, 2021 to June 27, 2024 as a video. We then identify which time steps fail to load or visualize and diagnose why. The scripts we proceed to describe can be found in the side bar or here.\n\n7.2.2.1 Creating the Images\nIn the following scripts, we generate PNG images for every time step in our IDX file and for every time step directly loaded from the downloaded NetCDF files. The time steps we load are the same ones specified in our idx_calls array from Chapter 6.\nThe purpose for loading from both our IDX file and NetCDF files to later check if any issues we encounter are present in the original files we downloaded as well.\n\nIDX File PNGsNetCDF Files PNGs\n\n\n\n\nCode\n# path to tiny NetCDF\nurl = (\"https://github.com/sci-visus/NSDF-WIRED/raw/main/data/firesmoke_metadata_recent.nc\")\n\n# Download the file using requests\nresponse = requests.get(url)\nlocal_netcdf = \"firesmoke_metadata.nc\"\nwith open(local_netcdf, \"wb\") as f:\n    f.write(response.content)\n\n# open tiny netcdf with xarray and OpenVisus backend\nds = xr.open_dataset(local_netcdf, engine=OpenVisusBackendEntrypoint)\n\n\n\n\n\n\nCode\nfizz_buzz &lt;- function(fbnums = 1:50) {\n  output &lt;- dplyr::case_when(\n    fbnums %% 15 == 0 ~ \"FizzBuzz\",\n    fbnums %% 3 == 0 ~ \"Fizz\",\n    fbnums %% 5 == 0 ~ \"Buzz\",\n    TRUE ~ as.character(fbnums)\n  )\n  print(output)\n}",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Validation</span>"
    ]
  }
]