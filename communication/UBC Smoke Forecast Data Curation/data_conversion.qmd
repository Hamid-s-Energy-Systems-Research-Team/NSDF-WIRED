---
format:
  html:
    code-links:
      - text: Conversion Script Version 1
        icon: file-code
        href: https://github.com/sci-visus/NSDF-WIRED/blob/main/conversion/firesmoke_to_idx_v1.ipynb
      - text: Conversion Script Version 2
        icon: file-code
        href: https://github.com/sci-visus/NSDF-WIRED/blob/main/conversion/firesmoke_to_idx_v2.ipynb
      - text: Conversion Script Version 3
        icon: file-code
        href: https://github.com/sci-visus/NSDF-WIRED/blob/main/conversion/firesmoke_to_idx_v3.ipynb
      - text: Conversion Script Version 4
        icon: file-code
        href: https://github.com/sci-visus/NSDF-WIRED/blob/main/conversion/firesmoke_to_idx_v4.ipynb
      - text: Example with griddata
        icon: file-code
        href: https://github.com/sci-visus/NSDF-WIRED/blob/main/conversion/firesmoke_to_idx_v4.ipynb
    code-tools: true
---

# Data Conversion {#sec-data-conversion}

So far, we have established what the data from UBC is and how to download it to our machine. Now we describe how to compile the data on our machine into an IDX file using the [OpenViSUS libary](https://github.com/sci-visus/OpenVisus/tree/master) and its access to the [PIDX library](https://github.com/sci-visus/PIDX?tab=readme-ov-file).

## On Data Validation

We decided to perform data validation *after* conversion to the IDX file format. However, we realized that performing data validation *both before and after* conversion would be best. This is explored further in @sec-data-validation. 

For now, the reader should understand that data validation of the NetCDF files is different from data validation of the IDX file. In this chapter, we use the assumption that the NetCDF files *we can open* have complete and uncorrupted data for conversion to IDX.

## Overview

For all our conversion script versions, the same general process is followed:

1. Check which NetCDF files were successfully downloaded from the data source by attempting to open each downloaded file with `xarray`.
2. Obtain a subset of data from the files to create a dataset of chronological, hour by hour, data. 
3. Save this time series data to an `IDX` file using the [OpenVisus](https://github.com/sci-visus/OpenVisus/tree/master) framework.

We will describe the latest version of our conversion, **version 4**. Throughout, we will explain how previous attempts were deficient. 

To see previous attemps in their entirety, refer to the side bar. Please note that the previous scripts were working scripts, therefore they may be incomplete.

## Setting System Directories
First we set the directory paths we want to use during the conversion process, which is to our 4 directories of NetCDF files for each forecast ID.

```{python}
# | eval: false
firesmoke_dir = "/usr/sci/cedmav/data/firesmoke"  # <1>
idx_dir = "/usr/sci/scratch_nvme/arleth/idx/firesmoke"  # <1>

ids = ["BSC18CA12-01", "BSC00CA12-01", "BSC06CA12-01", "BSC12CA12-01"]  # <2>
start_dates = ["20210304", "20210304", "20210304", "20210303"]  # <2>
end_dates = ["20240627", "20240627", "20240627", "20240627"]  # <2>
```
1. Establish the directory where all forecast ID NetCDFs are stored and where to save our IDX file on the 'atlantis' machine.
2. Define the forecast IDs and dates we will loop over.

### Rationale and Future Improvements
#### Data Usage
In versions 1 and 2 of our conversion attempt, we did not use all four sets of forecast ID files. We only used BSC12CA12-01 files to compile a single dataset. We learned that by not using all four sets of data, the dataset we created was less accurate. See @sec-data-validation for further details.

Therefore we decided to use all four datasets, see @sec-sequencing for details. We elect to use dates up to June 27, 2024 as this was the last time we ran our scripts. We have yet to address the issue of how to keep the IDX file constantly up to date with data available up to the present day.

## Checking the NetCDF Files
Recall we downloaded all NetCDF files available from UBC onto our machine in their respective directories as follows:
```
/usr/sci/cedmav/data/firesmoke
├── BSC00CA12-01
├── BSC06CA12-01
├── BSC12CA12-01
├── BSC18CA12-01
```

Here, we identify which NetCDF files for each forecast ID successfully open with `xarray` and store them in a dictionary.

We also confirm the following conditions we established in @sec-data-source by using dictionaries to track the max values and unique values of these attributes across all files:

1. All files across all four forecast IDs have the same `NROWS`, `XORIG`, `YORIG`, `XCELL`, `YCELL` values.
2. Some files have either `NCOLS = 1041` or `NCOLS = 1081`, but always `NROWS = 381`.

```{python}
# | eval: false
import os
import xarray as xr
import numpy as np
import tqdm

successful_files = {id_: [] for id_ in ids}  # <1>

max_ncols = {id_: 0 for id_ in ids}  # <2>
max_nrows = {id_: 0 for id_ in ids}  # <2>
ncols = {id_: set() for id_ in ids}  # <3>
nrows = {id_: set() for id_ in ids}  # <3>

max_grid_x = {id_: {"xorig": 0.0, "xcell": 0.0} for id_ in ids}  # <4>
max_grid_y = {id_: {"yorig": 0.0, "ycell": 0.0} for id_ in ids}  # <4>
xorigs = {id_: set() for id_ in ids}  # <5>
xcells = {id_: set() for id_ in ids}
yorigs = {id_: set() for id_ in ids}
ycells = {id_: set() for id_ in ids}  # <5>

for id_ in ids:  # <6>
    file_names = os.listdir(f"{firesmoke_dir}/{id_}/")  # <7>

    for file in tqdm(file_names):  # <8>
        path = f"{firesmoke_dir}/{id_}/{file}"  # <9>

        try:  # <10>
            ds = xr.open_dataset(path)  # <10>

            successful_files[id_].append(file)  # <11>

            max_ncols[id_] = max(max_ncols[id_], ds.NCOLS)  # <12>
            max_nrows[id_] = max(max_nrows[id_], ds.NROWS)
            max_grid_x[id_]["xorig"] = max(max_grid_x[id_]["xorig"], ds.XORIG, key=abs)
            max_grid_y[id_]["yorig"] = max(max_grid_y[id_]["yorig"], ds.YORIG, key=abs)
            max_grid_x[id_]["xcell"] = max(max_grid_x[id_]["xcell"], ds.XCELL, key=abs)
            max_grid_y[id_]["ycell"] = max(
                max_grid_y[id_]["ycell"], ds.YCELL, key=abs
            )  # <12>

            ncols[id_].add(ds.NCOLS)  # <13>
            nrows[id_].add(ds.NROWS)
            xorigs[id_].add(ds.XORIG)
            yorigs[id_].add(ds.YORIG)
            xcells[id_].add(ds.XCELL)
            ycells[id_].add(ds.YCELL)  # <13>

        except:  # <14>
            continue

for id_ in successful_files:  # <15>
    successful_files[id_] = np.sort(successful_files[id_]).tolist()  # <15>
```
1. Initialize a dictionary to hold an empty list for each forecast ID. We update it with the file names that successfully open under the forecast ID directory.
2. Initialize dictionaries to hold an integer for each forecast ID. We update it to hold the maximum `NCOLS`/`NROWS` value available within forecast ID's set of NetCDF files.
3. Initialize dictionaries to hold a set for each forecast ID. We update the set to hold all the unique `NCOLS`/`NROWS` values available within the forecast ID's set of NetCDF files.
4. Initialize dictionaries to hold a dictionary of `xorig`/`yorig` and `xcell`/`ycell` values for each forecast ID. We update it to hold the maximum `xorig`/`yorig` and `xcell`/`ycell` pairs available within the forecast ID's set of NetCDF files.
5. Initialize dictionaries to track unique `xorig`/`yorig` and `xcell`/`ycell` values.
6. For each forecast ID, we populate the dictionaries above.
7. Obtain a list of file names under the directory for `id_`. We loop through each file next.
8. Begin loop over each file. Note `tqdm` is just an accessory for generating a visible status bar in our Jupyter Notebook.
9. Obtain absolute path name to current file.
10. Here we use a `try` statement since opening the file with `xarray` may lead to an error. `except` allows us to catch the exception accordingly and continue trying to open each file.
11. At this line, the file opened without issue in `xarray`, so append this file name to the `id_` list in the `successful_files` dictionary.
12. Use `max` to save the largest values in our dictionaries accordingly.
13. Update the dictionaries of sets with the file's attributes, to ensure we catch all unique values.
14. If the file did not open during the `try` continue to the next file.
15. Sort the lists of successfully opened files by name, so they are in chronological order.

The following shows the information gathered:

::: {.panel-tabset}
## BSC18CA12-01
```
dataset: BSC18CA12-01
Number of successful files: 1010
Max cell sizes: max_ncols = 1081 and max_nrows = 381
Max xorig & xcell: {'xorig': -160.0, 'xcell': 0.10000000149011612}
Max yorig & ycell: {'yorig': 32.0, 'ycell': 0.10000000149011612}
ncols: {1081, 1041}
nrows: {381}
xorigs: {-160.0, -156.0}
yorigs: {32.0}
xcells: {0.10000000149011612}
ycells: {0.10000000149011612}
```
## BSC00CA12-01
```
dataset: BSC00CA12-01
Number of successful files: 1067
Max cell sizes: max_ncols = 1081 and max_nrows = 381
Max xorig & xcell: {'xorig': -160.0, 'xcell': 0.10000000149011612}
Max yorig & ycell: {'yorig': 32.0, 'ycell': 0.10000000149011612}
ncols: {1081, 1041}
nrows: {381}
xorigs: {-160.0, -156.0}
yorigs: {32.0}
xcells: {0.10000000149011612}
ycells: {0.10000000149011612}
```
## BSC06CA12-01
```
dataset: BSC06CA12-01
Number of successful files: 997
Max cell sizes: max_ncols = 1081 and max_nrows = 381
Max xorig & xcell: {'xorig': -160.0, 'xcell': 0.10000000149011612}
Max yorig & ycell: {'yorig': 32.0, 'ycell': 0.10000000149011612}
ncols: {1081, 1041}
nrows: {381}
xorigs: {-160.0, -156.0}
yorigs: {32.0}
xcells: {0.10000000149011612}
ycells: {0.10000000149011612}
```
## BSC12CA12-01
```
dataset: BSC12CA12-01
Number of successful files: 1003
Max cell sizes: max_ncols = 1081 and max_nrows = 381
Max xorig & xcell: {'xorig': -160.0, 'xcell': 0.10000000149011612}
Max yorig & ycell: {'yorig': 32.0, 'ycell': 0.10000000149011612}
ncols: {1081, 1041}
nrows: {381}
xorigs: {-160.0, -156.0}
yorigs: {32.0}
xcells: {0.10000000149011612}
ycells: {0.10000000149011612}
```
:::

### Rationale and Future Improvements
#### Unloadable Files
On our first attempt to convert the data we discovered that various files failed to open. Therefore, we used a dictionary to keep track of which files successfully open.

#### Varying Grid Size
In this step we collect attribute information about the two different grids used in the dataset. We proceed to use these attributes to resample the grids accordingly, see @sec-resampling.

#### Optimization and Scaling
One improvement to this step is to stop tracking maxes and unique values seperately. Instead, we could just track unique values then get maxes from there. Furthermore, modularizing this process such that it handles the possibility of new grids being used in the dataset would make the conversion script more scalable.

## Preparing Resampling Grids {#sec-resampling}
Now that we have the sets of openable files, we begin to handle the need to resample data. To resample arrays of shape 381×1041 to 381×1081, we use the SciPy `griddata` function from the `interpolate` package. This function gives interpolated values on set of points `xi` from a set of `points` with corresponding `values`. We refer the reader to SciPy's [documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.griddata.html) for details.

In this step, we obtain the grids we wish to use as our `points` and `xi`. See @sec-resample-example for how we use these grids with the `griddata` function.

<!-- Thus, we can sample a set of corresponding `PM25` values on a 381×1081 grid by using using a 381×1041 array of latitude and longitudes as `points` and 381×1041 array of PM25 values as `values` with the `griddata` function. We will generate these grids of lat/lon points using the attribute information we collected in the previous step. -->

### Generate Grids of Latitude and Longitude Points
Recall we can generate a set of latitude and longitude coordinates by using the attributes given in each NetCDF file, see @sec-netcdf-demo for an example. Here we generate two sets of latitude and longitude coordinates for each grid size.
```{python}
# | eval: false
max_xorig = max_grid_x[ids[0]]['xorig'] # <1>
max_xcell = max_grid_x[ids[0]]['xcell']
max_yorig = max_grid_y[ids[0]]['yorig']
max_ycell = max_grid_y[ids[0]]['ycell'] # <1>

big_lon = np.linspace(max_xorig, max_xorig + max_xcell * (max_ncols[ids[0]] - 1), max_ncols[ids[0]]) # <2>
big_lat = np.linspace(max_yorig, max_yorig + max_ycell * (max_nrows[ids[0]] - 1), max_nrows[ids[0]]) # <2>

big_lon_pts, big_lat_pts = np.meshgrid(big_lon, big_lat) # <3>
big_tups = np.array([tup for tup in zip(big_lon_pts.flatten(), big_lat_pts.flatten())]) # <3>

sml_ds = xr.open_dataset(firesmoke_dir + "/BSC00CA12-01/dispersion_20210304.nc") # <4>
sml_lon = np.linspace(sml_ds.XORIG, sml_ds.XORIG + sml_ds.XCELL * (sml_ds.NCOLS - 1), sml_ds.NCOLS)
sml_lat = np.linspace(sml_ds.YORIG, sml_ds.YORIG + sml_ds.YCELL * (sml_ds.NROWS - 1), sml_ds.NROWS) # <4>

sml_lon_pts, sml_lat_pts = np.meshgrid(sml_lon, sml_lat) # <5>
sml_tups = np.array([tup for tup in zip(sml_lon_pts.flatten(), sml_lat_pts.flatten())]) # <5>
```
1. Get the x/y origin and cell size parameters for the big 381×1081 grid.
2. Generate one two lists, defining a grid of latitudes and longitudes.
3. Using `big_lon` and `big_lat`, use meshgrid to generate our 381×1081 set of longitudes and latitudes.
4. Open a file that uses the small 381×1041 grid. Then, use the attributes in that file to generate two lists defining a grid of latitudes and longitudes.
5. Using `sml_lon` and `sml_lat`, use meshgrid to generate our 381×1041 set of longitudes and latitudes.

See below for an example of using these latitude and longitude grids to resample data on a 381×1041 grid to a 381×1081 grid.

### Example with `griddata` {#sec-resample-example}
Now that we have the two sets of latitude and longitude points, we show an example of how these are used to resample an array of data from a 381×1041 grid to a 381×1081 grid.

{{< embed data_notebooks/data_conversion/resample_example.ipynb echo=true >}}

### Rationale and Future Improvements
#### Discovering Varying Grids
We discovered the two grid shapes in version 1 of our conversion script. We found this after noticing the smoke data visualizations were nonsensical.

For example, the visualizations showed smoke eminating from the ocean, as shown in @fig-resampling.

::: {#fig-resampling layout-nrow=2}

![Plotting Data from 381×1041 Grid on 381×1081 Grid](images/small-on-big.jpeg){#fig-small-on-big}

![Plotting Data Resampled from 381×1041 Grid onto 381×1081 Grid](images/resamped-on-big.jpeg){#fig-resamped-on-big}

Visualization of Timestamp March 5, 2021 00:00:00 using IDX file Created in Conversion Script Version 1 
:::

For further details on how we investigated this issue, see @sec-data-validation.

#### Handling Various Grids
We considered various approaches to handling the fact that the data was on a 381×1041 grid or 381×1081 grid.

| Approach                                                                                     | Weakness                                                                                                                           |
|----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|
| Exclude arrays with 1041 columns.                                                               | Throwing away those data points would discard all the information they hold.                                                     |
| Force data with 1041 columns into an array with 1081 columns without resampling.              | This results in unused columns within the 1081-column array, leading to discontinuities and potential artifacts in the data representation. |
| Crop arrays on  1081 columns to 1041 columns                                              | Cropping the data would result in loss of information.                                                                           |
: The Weaknesses of Approaches to Handling Varying Array Sizes {#tbl-sizes}

The approach we chose was to **resample the data with 1041 columns to arrays with 1081 columns**. This produced the most visually appealing result and preserved the most information possible.

#### Scaling
One future improvement is to generalize precomputing additional grids if in the future the smoke forecasts change their grid size again. As of now we manually compute points for the 2 grid sizes.

## Sequencing of NetCDF Files {#sec-sequencing}

At this point we have the lists of openable files and our resampling grids. Now we will determine which files to use and in what order.

### Understanding Staggered Forecasts

The unique challenge of UBC's short term dataset is that the forecasts overlap, creating staggered predictions.

Let's explore these staggered forecasts to understand, how to select the optimal predictions to create a single dataset consisting of the most optimal predictions. For details on each forecast's time information, please see @sec-data-source.




Across all the NetCDF files we have downloaded, recall for every hour there exists 1-4 different forecasts to represent that hour. We aim to choose the forecast which best represents each hour. In order to understand the best representation for each hour, we need to know what hours are represented in each forecast.



### Finding Hours per Forecast ID

Recall that within each set of Forecast ID files, some files failed to download or otherwise open. Therefore, we must check exactly what set of hours are available in each collection of forecast ID NetCDF files.

In order to make loading an hour from a specified forecast ID dataset as easy as possible, we create a dictionary for each forecast ID set. Below you can see the first few entries of this dictionary:
```python
#TODO
```

To generate this dictionary, we did the following:
```python
id_sets = {id_: {} for id_ in ids} # <1>

for id_ in ids:
    # get successful files to add all successful hours to set
    for file in tqdm(successful_files[id_]): # <2>
        path = f'{firesmoke_dir}/{id_}/{file}' # <3>
        
        ds = xr.open_dataset(path) # <3>

        for h in range(ds.sizes["TSTEP"]): # <4>
            id_sets[id_][(file, parse_tflag(ds['TFLAG'].values[h][0]))] = h # <4>
```
1. Initialize a dictionary to hold an empty dictionary for each forecast ID.
2. We step through all the successfully opened files found for `id_`.
3. Open the file and open it using `xarray`.
4. Create a tuple with the file name and the parsed TFLAG for current hour `h`. Add index `h` to our dictionary assigning the tuple as its key.

### Creating the Sequence

Now that we have an indexable set of all available hours for each forecast ID, we can generate the sequence to extract the time series dataset we would like to create.

## Creating the IDX File

At this point, we have precomputed the order in which we will load and write each array to our IDX file. We will now put everything together to generate our single dataset.

```python
f = Field("PM25", "float32") # <1>

db = CreateIdx( # <2>
    url=idx_dir + "/firesmoke.idx",
    fields=[f],
    dims=[int(max_ncols[ids[0]]), int(max_nrows[ids[0]])],
    time=[0, len(idx_calls) - 1, "%00000000d/"],
) # <2>

tstep = 0 # <3>

thresh = 1e-15 # <4>

for call in tqdm(idx_calls): # <5>
    curr_id = call[0]
    curr_file = call[1]
    tstep_index = call[3] # <5>

    ds = xr.open_dataset(f"{firesmoke_dir}/{curr_id}/{curr_file}") # <6>

    file_vals = np.squeeze(ds["PM25"].values) # <7>

    resamp = ds.XORIG != max_xorig # <8>

    if resamp: # <9>
        file_vals_resamp = griddata(
            sml_tups,
            file_vals[tstep_index].flatten(),
            big_tups,
            method="cubic",
            fill_value=0,
        ) # <9>

        file_vals_resamp[file_vals_resamp < thresh] = 0 # <10>

        file_vals_resamp = file_vals_resamp.reshape((len(big_lat), len(big_lon))) # <11>

        db.write(data=file_vals_resamp.astype(np.float32), field=f, time=tstep) # <12>
    else: # <13>
        db.write(data=file_vals[tstep_index], field=f, time=tstep) # <13>

    tstep = tstep + 1 # <14>
```
1. Create an OpenVisus field to hold the PM25 variable data.
2. Create the IDX file wherein `url` is the location to write the file, `fields` holds the data variables we will save, `dims` represents the shape of each array, and `time` defines how many time steps there are.
3. We will usethis to keep track of which time step we are on as we step through our `idx_calls`.
4. Threshold to use to change small-enough resampled values to 0.
5. Get the information for current time step, in particular the [curr_id, file_str, parse_tflag(ds['TFLAG'].values[tstep_idx][0]), tstep_idx]
6. Load the current file with `xarray`.
7. Get the full array of PM25 values in the file.
8. If `ds.XORIG` is not already for the 381×1081 grid, we need to resample it to the larger grid.
9. Using `gridddata`, interpolate the values on a 381×1081 grid using the precomputed lat/lon points.
10. Any values that are less than our threshold should have a value of 0. WHY
11. Reshape the interpolated values to 381×1081.
12. Write the resampled values for hour `h` to timestep `t` and field `f` of our IDX file.
13. These values are already on a 381×1081 grid, so write the values at hour `h` to timestep `t` and field `f` of our IDX file.
14. Increment to the next timestep for writing to IDX.