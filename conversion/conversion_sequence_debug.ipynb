{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a904c38c-c059-4cd3-ae3f-7a4b4e5e8d13",
   "metadata": {},
   "source": [
    "# Here we debug the sequence produced by our firesmoke conversion scripts\n",
    "\n",
    "## We need to make sure that, the missing dates are truly because the netCDF files are unavailable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41619410-ce20-43f3-9282-c77889e679d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used to read/manipulate netCDF data\n",
    "import xarray as xr\n",
    "\n",
    "# Used for numerical work\n",
    "import numpy as np\n",
    "\n",
    "# Used for processing netCDF time data\n",
    "import datetime\n",
    "\n",
    "# To load/save final sequence array to file\n",
    "import pickle\n",
    "\n",
    "# for checking and using timestamps\n",
    "import pandas as pd\n",
    "\n",
    "# for downloading from internet, we use wget here to forcibly download whatever is available at URL...\n",
    "import wget\n",
    "\n",
    "# Accessory, used to generate progress bar for running for loops\n",
    "# from tqdm.notebook import tqdm\n",
    "# import ipywidgets\n",
    "# import jupyterlab_widgets\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c71d51c-9084-45cf-9388-a710127af996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load idx_calls from a file\n",
    "with open('idx_calls_v4.pkl', 'rb') as f:\n",
    "    idx_calls = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57c567d-75b3-4155-a17b-61756caf82f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the start and end dates\n",
    "start_date = pd.Timestamp(datetime.datetime.strptime(\"20210303\", \"%Y%m%d\"))\n",
    "end_date = pd.Timestamp(datetime.datetime.strptime(\"20240627\", \"%Y%m%d\"))\n",
    "\n",
    "# Get all hours between the start and end dates\n",
    "desired_tflag_set = {start_date + pd.Timedelta(hours=x) for x in range(int((end_date - start_date).total_seconds() // 3600) + 1)}\n",
    "\n",
    "print(f'There are {len(desired_tflag_set)} hours between 3/3/21 and 6/27/24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de765de-6418-42e7-8c77-c23614c45d8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all hours in idx conversion\n",
    "idx_hours = {call[2] for call in idx_calls}\n",
    "\n",
    "# Get set of missing hours\n",
    "hours_missing_set = desired_tflag_set.difference(idx_hours)\n",
    "\n",
    "print(f'There are {len(hours_missing_set)} missing hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4961f70d-19aa-4e7f-91df-5aed967ed044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hours_missing_sorted = np.sort([i for i in hours_missing_set])\n",
    "\n",
    "# check out first 1000 missing hours\n",
    "print(hours_missing_sorted[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627e8d41-4ba9-4e90-a812-efba41e82625",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Let's see if the hours missing aren't available from firesmoke.ca...\n",
    "For each missing date, let's just grab that day and the 4 that **precede** it, at each dataset. Then we'll check if any of those downloaded files have the missing hours available in them...\n",
    "\n",
    "We may have somehow failed to download all available netCDF files in `data_download` workflow..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71db263e-60cf-4e7a-98a0-081fc221f08c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# directory to hold the downloaded files\n",
    "garbage = '/usr/sci/scratch_nvme/arleth/total_garbage'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3a3701-314f-484e-80da-db3180dd33bc",
   "metadata": {},
   "source": [
    "From all the failed hours, get the DDMMYYYY dates, this is what we use to query firesmoke.ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f662bef-1e59-4b2e-a97a-228ab8f07c23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "missing_dates = set()\n",
    "\n",
    "for t in hours_missing_sorted:\n",
    "    missing_dates.add(t.normalize())\n",
    "\n",
    "missing_dates\n",
    "\n",
    "print(f'{len(missing_dates)} total missing dates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a3ed5c-a553-4554-853f-0c25797b9f11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make sequence of queries to download, using missing_tflags and dataset metadata\n",
    "ids = [\"BSC18CA12-01\", \"BSC00CA12-01\", \"BSC06CA12-01\", \"BSC12CA12-01\"]\n",
    "init_times = [\"02\", \"08\", \"14\", \"20\"]\n",
    "to_download = set()\n",
    "\n",
    "# for all missing dates\n",
    "for date in missing_dates:\n",
    "    # for 4 days\n",
    "    for i in range(4):\n",
    "        curr_date = date + pd.Timedelta(days=-i)\n",
    "        \n",
    "        # build date string for query\n",
    "        date_str = curr_date.strftime('%Y%m%d')\n",
    "        \n",
    "        # for each dataset\n",
    "        for id_, init_time in zip(ids, init_times):\n",
    "            # build URL string to download from and directory & filename to download to\n",
    "            url = f'https://firesmoke.ca/forecasts/{id_}/{date_str}{init_time}/dispersion.nc'\n",
    "            directory = f'{garbage}/{id_}/dispersion_{date_str}.nc'\n",
    "            to_download.add((url, directory))\n",
    "\n",
    "print(f'there are {len(to_download)} files to try downloading...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17f5366-6ead-4a6f-80f1-123a95c0e4d5",
   "metadata": {},
   "source": [
    "The following code block is for downloading the files, I commented it out to avoid rerunning the download script, overwriting files.. etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999f0f3c-0fca-4871-b2ec-f1d9a7c6c8c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # download files, do this as a python script by doing `nbconvert --to script...`, \n",
    "# # it's faster somehow\n",
    "# for q in tqdm(to_download):\n",
    "#     print(f'downloading {q[0]} to {q[1]}')\n",
    "#     wget.download(q[0], out=q[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1edf3af-c976-4758-9185-562f183d60a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d85c8dd-6789-45be-9091-84f54b77d204",
   "metadata": {},
   "source": [
    "#### See all hours available from files we downloaded. Check intersection with our missing tflags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c8a4c0-f7ea-4503-ac0c-c17d7f2c0c6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Track files that successfully opened\n",
    "successful_files = {id_: [] for id_ in ids}\n",
    "\n",
    "# Set of all hours available from the files newly downloaded\n",
    "new_hours = set()\n",
    "\n",
    "# Try opening all downloaded files\n",
    "for query in to_download:\n",
    "    # get file path from current query from to_download\n",
    "    path = query[1]\n",
    "    # get file name from query\n",
    "    file_name = path[-len('dispersion_20210228.nc'):]\n",
    "    # get id from path string\n",
    "    id_ = path[-len('BSC00CA12-01/dispersion_20210228.nc'):-len('/dispersion_20210228.nc')]\n",
    "\n",
    "    # keep track of which files successfully open\n",
    "    try:\n",
    "        # open the file with xarray\n",
    "        ds = xr.open_dataset(path)\n",
    "\n",
    "        # append file name to successful_files\n",
    "        successful_files[id_].append(path)\n",
    "        # add each available hour to new_hours\n",
    "        for h in range(ds.sizes[\"TSTEP\"]):\n",
    "            vals = ds['TFLAG'].values[h]\n",
    "            curr_tflag = parse_tflag(ds['TFLAG'].values[h][0])\n",
    "            panda_tflag = pd.Timestamp(curr_tflag)\n",
    "            new_hours.add((id_, panda_tflag))\n",
    "    except:\n",
    "        # netcdf file does not exist\n",
    "        print(f'FAILED, {id_}, {file_name}')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb846dd-36f3-4124-89bd-f0a0af2e8be3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for id_ in ids:\n",
    "    print(f'For {id_}:')\n",
    "    print(f'Of files downloaded, there are {len(successful_files[id_])} that open successfully.')\n",
    "    print('---')\n",
    "print(f'Of files downloaded, there are {len(new_hours)} hours available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8e84b7-aefe-455a-ba40-d6a6a47734f5",
   "metadata": {},
   "source": [
    "#### Determine how many new hours were downloaded that we could add to the final IDX conversion, if any..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debeb75c-03a0-471e-b038-13a18eb74d46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get all timestamps downloaded\n",
    "hours_set = set()\n",
    "\n",
    "# get each timestamp, add it to a set\n",
    "for hour in new_hours:\n",
    "    hours_set.add(hour[1])\n",
    "\n",
    "print(f'there are {len(hours_set.intersection(hours_missing_set))} hours available from downloaded data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000b773b-341c-4112-bc6b-da81248ad0e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hours_set.intersection(hours_missing_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3356abab-8781-4f2e-ad6e-14417b2157f4",
   "metadata": {},
   "source": [
    "## Turns out all of the hours we see as missing are truly unavailable.\n",
    "We will proceed to share."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c1f510-6fce-4dbf-b5e5-1dc915797e07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.shape(hours_missing_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418ab4a9-16c8-4d0d-9f20-d821f47c2aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hours_missing_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520742ce-0d7b-436c-a970-6adca007f81f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csv_arr = [['hour', 'day', 'month', 'year']]\n",
    "\n",
    "for h in hours_missing_sorted:\n",
    "    csv_arr.append([h.hour, h.day, h.month, h.year])\n",
    "    \n",
    "np.shape(csv_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69d611f-02cc-46fb-b002-41d8d4b5b2b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ref: https://stackoverflow.com/questions/6081008/dump-a-numpy-array-into-a-csv-file\n",
    "# ref: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html\n",
    "import pandas as pd \n",
    "df = pd.DataFrame(csv_arr)\n",
    "df.to_csv(\"missing_hours.csv\", header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
