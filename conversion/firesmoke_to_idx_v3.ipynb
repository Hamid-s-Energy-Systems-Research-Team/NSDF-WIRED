{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ca8ac1b-4eaa-478a-ad7e-1364e6c1791e",
   "metadata": {},
   "source": [
    "# Firesmoke Data Conversion to IDX using OpenVisus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6f4506-47b9-452e-908e-15e532e1b801",
   "metadata": {},
   "source": [
    "## Import necessary libraries, install them if you do not have them. This was developed in Python 3.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f499004e-bebd-4abf-8885-f16ce6ad9531",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used to read/manipulate netCDF data\n",
    "import xarray as xr\n",
    "\n",
    "# Used to convert to .idx\n",
    "from OpenVisus import *\n",
    "\n",
    "# Used for numerical work\n",
    "import numpy as np\n",
    "\n",
    "# Used for processing netCDF time data\n",
    "import datetime\n",
    "\n",
    "# Used for interacting with OS file system (to get directory file names)\n",
    "import os\n",
    "\n",
    "# To load/save final sequence array to file\n",
    "import pickle\n",
    "\n",
    "# Used for resampling arrays to fit the same lat/lon grid\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# Accessory, used to generate progress bar for running for loops\n",
    "# from tqdm.notebook import tqdm\n",
    "# import ipywidgets\n",
    "# import jupyterlab_widgets\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce3169e-d1dd-4050-811a-6fd661442c81",
   "metadata": {},
   "source": [
    "## Get relevant directory paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0c7861-34fb-464b-95b1-4a1aa4a8ba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******* THIS IS WHEN RUNNING FROM ATLANTIS.SCI **************\n",
    "firesmoke_dir = \"/usr/sci/cedmav/data/firesmoke\"\n",
    "\n",
    "# path to save idx file and data\n",
    "idx_dir = \"/usr/sci/scratch_nvme/arleth/idx/firesmoke\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d88f90-0370-481c-93be-e3c1e1a10ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata of datasets, had to be obtained manually\n",
    "ids = [\"BSC18CA12-01\", \"BSC00CA12-01\", \"BSC06CA12-01\", \"BSC12CA12-01\"]\n",
    "start_dates = [\"20210304\", \"20210304\", \"20210304\", \"20210303\"]\n",
    "end_dates = [\"20231016\", \"20240210\", \"20231016\", \"20231015\"]\n",
    "\n",
    "id_dates = {ids[i]: {\"start_date\": start_dates[i], \"end_date\": end_dates[i]} for i in range(len(ids))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32528eee-cd60-44f6-9e14-cae79c5ce478",
   "metadata": {},
   "source": [
    "## Gather information about the metadata of our files, since it is inconsistent file to file. We need to know what to normalize across all files.\n",
    "\n",
    "### In particular:\n",
    "#### 1. Count number of files there are per firesmoke directory.\n",
    "#### 2. Determine maximum row,col dimension sizes for pm25 array.\n",
    "#### 3. Determine maximum latitude longitude grid parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c92053f-9e91-4e6d-b46b-a9efa04c6d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all files that are available from UBC\n",
    "successful_files = {id_: [] for id_ in ids}\n",
    "\n",
    "# Variables to hold maxes, also to track the unique max values\n",
    "max_ncols = {id_: 0 for id_ in ids}\n",
    "max_nrows = {id_: 0 for id_ in ids}\n",
    "ncols = {id_: set() for id_ in ids}\n",
    "nrows = {id_: set() for id_ in ids}\n",
    "\n",
    "# Max grid dimensions\n",
    "max_grid_x = {id_: {\"xorig\": 0.0, \"xcell\": 0.0} for id_ in ids}\n",
    "max_grid_y = {id_: {\"yorig\": 0.0, \"ycell\": 0.0} for id_ in ids}\n",
    "xorigs = {id_: set() for id_ in ids}\n",
    "xcells = {id_: set() for id_ in ids}\n",
    "yorigs = {id_: set() for id_ in ids}\n",
    "ycells = {id_: set() for id_ in ids}\n",
    "\n",
    "for id_ in ids:\n",
    "    # get list of netcdf file names for each dataset\n",
    "    file_names = os.listdir(f'{firesmoke_dir}/{id_}/')\n",
    "    \n",
    "    # try opening each file, process only if it successfully opens\n",
    "    for file in tqdm(file_names):\n",
    "        # get file's path\n",
    "        path = f'{firesmoke_dir}/{id_}/{file}'\n",
    "        \n",
    "        # keep track of which files successfully open\n",
    "        try:\n",
    "            # open the file with xarray\n",
    "            ds = xr.open_dataset(path)\n",
    "    \n",
    "            # append file name to successful_files\n",
    "            successful_files[id_].append(file)\n",
    "    \n",
    "            # update maxes accordingly\n",
    "            # these *are* allowed to get mixed up between files right? in this case don't need to worry bout it\n",
    "            max_ncols[id_] = max(max_ncols[id_], ds.NCOLS)\n",
    "            max_nrows[id_] = max(max_nrows[id_], ds.NROWS)\n",
    "    \n",
    "            # these should not get mixed up between files right? or can they?\n",
    "            # if they do get mixed up, wouldn't it be a ill-defined grid?\n",
    "            # ref: https://stackoverflow.com/questions/18296755/python-max-function-using-key-and-lambda-expression\n",
    "            max_grid_x[id_][\"xorig\"] = max(max_grid_x[id_][\"xorig\"], ds.XORIG, key=abs)\n",
    "            max_grid_y[id_][\"yorig\"] = max(max_grid_y[id_][\"yorig\"], ds.YORIG, key=abs)\n",
    "            max_grid_x[id_][\"xcell\"] = max(max_grid_x[id_][\"xcell\"], ds.XCELL, key=abs)\n",
    "            max_grid_y[id_][\"ycell\"] = max(max_grid_y[id_][\"ycell\"], ds.YCELL, key=abs)\n",
    "    \n",
    "            # update sets\n",
    "            ncols[id_].add(ds.NCOLS)\n",
    "            nrows[id_].add(ds.NROWS)\n",
    "            xorigs[id_].add(ds.XORIG)\n",
    "            yorigs[id_].add(ds.YORIG)\n",
    "            xcells[id_].add(ds.XCELL)\n",
    "            ycells[id_].add(ds.YCELL)\n",
    "            \n",
    "        except:\n",
    "            # netcdf file does not exist\n",
    "            continue\n",
    "\n",
    "# Sort datasets' lists of successful files so they're in order of date\n",
    "for id_ in successful_files:\n",
    "    successful_files[id_] = np.sort(successful_files[id_]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fba2c72-e070-4b77-8060-f004bceb779b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the information for all ids\n",
    "for id_ in ids:\n",
    "    print(f'dataset: {id_}')\n",
    "    print(f'Number of successful files: {len(successful_files[id_])}')\n",
    "    print(f'Max cell sizes: max_ncols = {max_ncols[id_]} and max_nrows = {max_nrows[id_]}')\n",
    "    print(f'Max xorig & xcell: {max_grid_x[id_]}')\n",
    "    print(f'Max yorig & ycell: {max_grid_y[id_]}')\n",
    "    print(f'ncols: {ncols[id_]}')\n",
    "    print(f'nrows: {nrows[id_]}')\n",
    "    print(f'xorigs: {xorigs[id_]}')\n",
    "    print(f'yorigs: {yorigs[id_]}')\n",
    "    print(f'xcells: {xcells[id_]}')\n",
    "    print(f'ycells: {ycells[id_]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005f312f-f319-46a5-a7d2-1de4c49a9ed1",
   "metadata": {},
   "source": [
    "### Get latitude/longitude coordinates using the max values and non-max values AMONG ALL DATASETS, this is used for resampling during conversion\n",
    "\n",
    "Luckily, all datasets have the same 'smaller' and 'larger' lat/lon grid parameters :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9fe211-ccb8-46df-93e9-92f7d18e702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get parameters for bigger lat/lon\n",
    "max_xorig = max_grid_x[ids[0]]['xorig']\n",
    "max_xcell = max_grid_x[ids[0]]['xcell']\n",
    "max_yorig = max_grid_y[ids[0]]['yorig']\n",
    "max_ycell = max_grid_y[ids[0]]['ycell']\n",
    "\n",
    "# get arrays of bigger lat/lon grid\n",
    "big_lon = np.linspace(max_xorig, max_xorig + max_xcell * (max_ncols[ids[0]] - 1), max_ncols[ids[0]])\n",
    "big_lat = np.linspace(max_yorig, max_yorig + max_ycell * (max_nrows[ids[0]] - 1), max_nrows[ids[0]])\n",
    "\n",
    "# get coordinates made of new lat/lon arrays\n",
    "big_lon_pts, big_lat_pts = np.meshgrid(big_lon, big_lat)\n",
    "big_tups = np.array([tup for tup in zip(big_lon_pts.flatten(), big_lat_pts.flatten())])\n",
    "\n",
    "# get arrays of smaller lat/lon grid\n",
    "sml_ds = xr.open_dataset(firesmoke_dir + \"/BSC00CA12-01/dispersion_20210304.nc\")\n",
    "sml_lon = np.linspace(sml_ds.XORIG, sml_ds.XORIG + sml_ds.XCELL * (sml_ds.NCOLS - 1), sml_ds.NCOLS)\n",
    "sml_lat = np.linspace(sml_ds.YORIG, sml_ds.YORIG + sml_ds.YCELL * (sml_ds.NROWS - 1), sml_ds.NROWS)\n",
    "\n",
    "# get coordinates made of small lat/lon arrays\n",
    "sml_lon_pts, sml_lat_pts = np.meshgrid(sml_lon, sml_lat)\n",
    "sml_tups = np.array([tup for tup in zip(sml_lon_pts.flatten(), sml_lat_pts.flatten())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b68ee49-dabf-4258-95b9-707814280028",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## TESTING `resample_array` AND SCRIBBLES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc424296-d880-462f-921b-d4966025e1d2",
   "metadata": {},
   "source": [
    "### This is plotting the oiginal 381x1041 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30986a19-aa82-4fd3-91fb-b76ed6811a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the PM25 values, squeeze out empty axis\n",
    "# vals = np.squeeze(sml_ds['PM25'].values)\n",
    "\n",
    "# # Perform the interpolation\n",
    "# arr = griddata(sml_tups, vals[15].flatten(), big_tups, method='cubic', fill_value=0)\n",
    "\n",
    "# # Any values that are less than a given threshold, make it 0\n",
    "# arr[arr < 1e-15] = 0\n",
    "\n",
    "# # Reshape the result to match the new grid shape\n",
    "# arr = arr.reshape((len(big_lat), len(big_lon)))\n",
    "\n",
    "# arr = arr.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f64beb-b62d-46b5-b7e2-1cf1d7796449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.min(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed0975b-23c0-40f1-8666-ea9579b7d767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(arr[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb1f2db-95a9-467b-af10-6f671f4f48d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's use matplotlib's imshow, since our data is on a grid\n",
    "# # ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n",
    "\n",
    "# # Initialize a figure and plot, so we can customize figure and plot of data\n",
    "# # ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html\n",
    "# # ref: https://scitools.org.uk/cartopy/docs/latest/getting_started/index.html\n",
    "# my_fig, my_plt = plt.subplots(figsize=(15, 6), subplot_kw=dict(projection=ccrs.PlateCarree()))\n",
    "\n",
    "# # Let's set some parameters to get the visualization we want\n",
    "# # ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n",
    "\n",
    "# # color PM25 values on a log scale, since values are small\n",
    "# my_norm = \"log\" \n",
    "# # this will number our x and y axes based on the longitude latitude range\n",
    "# my_extent = [np.min(sml_lon), np.max(sml_lon), np.min(sml_lat), np.max(sml_lat)]\n",
    "# # ensure the aspect ratio of our plot fits all data, matplotlib can does this automatically\n",
    "# my_aspect = 'auto'\n",
    "# # tell matplotlib, our origin is the lower-left corner\n",
    "# my_origin = 'lower'\n",
    "# # select a colormap for our plot and the color bar on the right\n",
    "# my_cmap = 'viridis'\n",
    "\n",
    "# # create our plot using imshow\n",
    "# plot = my_plt.imshow(arr, norm=my_norm, extent=my_extent, \n",
    "#           aspect=my_aspect, origin=my_origin, cmap=my_cmap)\n",
    "\n",
    "# # draw coastlines\n",
    "# my_plt.coastlines()\n",
    "\n",
    "# # draw latitude longitude lines\n",
    "# # ref: https://scitools.org.uk/cartopy/docs/latest/gallery/gridlines_and_labels/gridliner.html\n",
    "# my_plt.gridlines(draw_labels=True)\n",
    "\n",
    "# # add a colorbar to our figure, based on the plot we just made above\n",
    "# my_fig.colorbar(plot,location='right', label='ug/m^3')\n",
    "\n",
    "# # # Set x and y axis labels on our ax\n",
    "# # my_plt.set_xlabel('Longitude')\n",
    "# # my_plt.set_ylabel('Latitude')\n",
    "\n",
    "# # Set title of our figure\n",
    "# my_fig.suptitle('Ground level concentration of PM2.5 microns and smaller')\n",
    "\n",
    "# # # Set title of our plot as the timestamp of our data\n",
    "# # my_plt.set_title(f'{my_timestamp}')\n",
    "\n",
    "# # Show the resulting visualization\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d18dc69-01cc-4fa9-91a5-1377bcdc6bea",
   "metadata": {},
   "source": [
    "### This is visualizing the resampled version of array above, from 381x1041 -> 381x1081 grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c59925a-f689-4c9f-a9a9-a897f45372e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's use matplotlib's imshow, since our data is on a grid\n",
    "# # ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n",
    "\n",
    "# # Initialize a figure and plot, so we can customize figure and plot of data\n",
    "# # ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html\n",
    "# # ref: https://scitools.org.uk/cartopy/docs/latest/getting_started/index.html\n",
    "# my_fig, my_plt = plt.subplots(figsize=(15, 6), subplot_kw=dict(projection=ccrs.PlateCarree()))\n",
    "\n",
    "# # Let's set some parameters to get the visualization we want\n",
    "# # ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n",
    "\n",
    "# # color PM25 values on a log scale, since values are small\n",
    "# my_norm = \"log\" \n",
    "# # this will number our x and y axes based on the longitude latitude range\n",
    "# my_extent = [np.min(big_lon), np.max(big_lon), np.min(big_lat), np.max(big_lat)]\n",
    "# # ensure the aspect ratio of our plot fits all data, matplotlib can does this automatically\n",
    "# my_aspect = 'auto'\n",
    "# # tell matplotlib, our origin is the lower-left corner\n",
    "# my_origin = 'lower'\n",
    "# # select a colormap for our plot and the color bar on the right\n",
    "# my_cmap = 'viridis'\n",
    "\n",
    "# # create our plot using imshow\n",
    "# plot = my_plt.imshow(arr_resamp, norm=my_norm, extent=my_extent, \n",
    "#           aspect=my_aspect, origin=my_origin, cmap=my_cmap, vmin=.00001, vmax=1)\n",
    "\n",
    "# # draw coastlines\n",
    "# my_plt.coastlines()\n",
    "\n",
    "# # draw latitude longitude lines\n",
    "# # ref: https://scitools.org.uk/cartopy/docs/latest/gallery/gridlines_and_labels/gridliner.html\n",
    "# my_plt.gridlines(draw_labels=True)\n",
    "\n",
    "# # add a colorbar to our figure, based on the plot we just made above\n",
    "# my_fig.colorbar(plot,location='right', label='ug/m^3')\n",
    "\n",
    "# # # Set x and y axis labels on our ax\n",
    "# # my_plt.set_xlabel('Longitude')\n",
    "# # my_plt.set_ylabel('Latitude')\n",
    "\n",
    "# # Set title of our figure\n",
    "# my_fig.suptitle('Ground level concentration of PM2.5 microns and smaller')\n",
    "\n",
    "# # # Set title of our plot as the timestamp of our data\n",
    "# # my_plt.set_title(f'{my_timestamp}')\n",
    "\n",
    "# # Show the resulting visualization\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f939c8-a3f9-4ec5-ab38-6c6cae22d486",
   "metadata": {},
   "source": [
    "## Determine sequence of files to load later for IDX conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb53b83-197f-4af8-ba35-118c50d5b699",
   "metadata": {},
   "source": [
    "### First determine what hours are available in all datasets, from there we construct final sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7e05fb-8237-41cf-9b35-e7c25aed27c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for parsing time flags (TFLAG) from netcdf files\n",
    "def parse_tflag(tflag):\n",
    "    year = int(tflag[0] // 1000)\n",
    "    day_of_year = int(tflag[0] % 1000)\n",
    "    date = datetime.datetime(year, 1, 1) + datetime.timedelta(days=day_of_year - 1)\n",
    "\n",
    "    time_in_day = int(tflag[1])\n",
    "    hours = time_in_day // 10000\n",
    "    minutes = (time_in_day % 10000) // 100\n",
    "    seconds = time_in_day % 100\n",
    "\n",
    "    full_datetime = datetime.datetime(year, date.month, date.day, hours, minutes, seconds)\n",
    "    return full_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f92213-d818-425b-a39c-e94a7b89e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get set of all available hours for each dataset using successful_files\n",
    "id_sets = {id_: {} for id_ in ids}\n",
    "\n",
    "for id_ in ids:    \n",
    "    # get successful files to add all successful hours to set\n",
    "    for file in tqdm(successful_files[id_]):\n",
    "        # get file's path\n",
    "        path = f'{firesmoke_dir}/{id_}/{file}'\n",
    "        \n",
    "        # open the file with xarray\n",
    "        ds = xr.open_dataset(path)\n",
    "\n",
    "        # add each available hour to successful_seq, store the index h, needed for idx conversion\n",
    "        for h in range(ds.sizes[\"TSTEP\"]):\n",
    "            id_sets[id_][(file, parse_tflag(ds['TFLAG'].values[h][0]))] = h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c91637a-b19b-44b0-8f4d-0023a0cb2334",
   "metadata": {},
   "source": [
    "### Ideally we use all dates, so step through all hours and grab from datasets accordingly.\n",
    "**Importantly, we should ideally use first six hours of each dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e6d2d0-10b7-4a5f-b720-40961f5132fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(ids[1])\n",
    "# id_sets[ids[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25976a7e-aaba-415f-99b1-47c2c2760298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # IT'S THERE\n",
    "# my_key = ('dispersion_20240209.nc', datetime.datetime(2024, 2, 10, 0, 0))\n",
    "# id_sets[ids[1]][my_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61b4235-7d40-49f6-b6d2-aeaff5e0cdab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Array to hold the final order we will search files, tuples of dataset ID and file name\n",
    "# conversion_seq = np.empty((24000, 3), dtype=object)\n",
    "# # Initialize each row to ['','',0]\n",
    "# conversion_seq[:] = ['', '', 0]\n",
    "\n",
    "conversion_seq = []\n",
    "idx_calls = []\n",
    "\n",
    "# Define the start and end dates we will step through\n",
    "start_date = datetime.datetime.strptime(\"20210304\", \"%Y%m%d\")\n",
    "end_date = datetime.datetime.strptime(\"20240210\", \"%Y%m%d\")\n",
    "\n",
    "# the dataset we wanna use\n",
    "curr_id_idx = 0\n",
    "\n",
    "# iterate over each day\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    # get dispersion file for current_date\n",
    "    today_file_str = f'dispersion_{current_date.strftime(\"%Y%m%d\")}.nc'\n",
    "    # get dispersion file for previous date, may be needed:\n",
    "    yesterday =  current_date + datetime.timedelta(days=-1)\n",
    "    yest_file_str = f'dispersion_{yesterday.strftime(\"%Y%m%d\")}.nc'\n",
    "\n",
    "    # file string to use\n",
    "    file_str = ''\n",
    "    \n",
    "    # if we need to use yesterday dispersion file or not\n",
    "    yes_yesterday = False\n",
    "    ids = [\"BSC18CA12-01\", \"BSC00CA12-01\", \"BSC06CA12-01\", \"BSC12CA12-01\"]\n",
    "    # iterate over each hour of the current day\n",
    "    current_hour = datetime.datetime(current_date.year, current_date.month, current_date.day)\n",
    "    while current_hour < current_date + datetime.timedelta(days=1):\n",
    "        # based on the hour, grab from corresponding dataset id\n",
    "        if current_hour <= current_date.replace(hour=2):\n",
    "            # HERE WE NEED TO USE PRIOR DATE, UPDATE FILE STR\n",
    "            yes_yesterday = True\n",
    "            curr_id_idx = 3 # BSC12CA12-01\n",
    "        if current_hour >= current_date.replace(hour=3) and current_hour <= current_date.replace(hour=8):\n",
    "            yes_yesterday = False\n",
    "            curr_id_idx = 0 # BSC18CA12-01\n",
    "        if current_hour >= current_date.replace(hour=9) and current_hour <= current_date.replace(hour=14):\n",
    "            yes_yesterday = False\n",
    "            curr_id_idx = 1 # BSC00CA12-01\n",
    "        if (current_hour >= current_date.replace(hour=15) and current_hour <= current_date.replace(hour=20)):\n",
    "            yes_yesterday = False\n",
    "            curr_id_idx = 2 # BSC06CA12-01\n",
    "        if current_hour >= current_date.replace(hour=21):\n",
    "            yes_yesterday = False\n",
    "            curr_id_idx = 3 # BSC12CA12-01\n",
    "\n",
    "        # choose desired file name\n",
    "        if yes_yesterday:\n",
    "            file_str = yest_file_str\n",
    "        else:\n",
    "            file_str = today_file_str\n",
    "        \n",
    "        # if desired timestamp at desired dataset id is available, use it\n",
    "        if (file_str, current_hour) in id_sets[ids[curr_id_idx]]:\n",
    "            # append dataset id, file name, and index of time step for idx conversion\n",
    "            tstep_idx = id_sets[ids[curr_id_idx]][(file_str, current_hour)]\n",
    "\n",
    "            # set as next in conversion sequence\n",
    "            conversion_seq.append([ids[curr_id_idx], file_str, tstep_idx])\n",
    "\n",
    "            # get file's path\n",
    "            path = f'{firesmoke_dir}/{ids[curr_id_idx]}/{file_str}'\n",
    "            # open the file with xarray\n",
    "            ds = xr.open_dataset(path)\n",
    "            idx_calls.append([ids[curr_id_idx], file_str, parse_tflag(ds['TFLAG'].values[tstep_idx][0])])\n",
    "        else: # search the others in order of most recent to least\n",
    "            found = 0\n",
    "            search_count = 0\n",
    "\n",
    "            print(f'NOT avail in {ids[curr_id_idx]}')\n",
    "            \n",
    "            # ensure curr_id_idx is big enough to subract from\n",
    "            curr_id_idx = curr_id_idx + 4\n",
    "\n",
    "            # if we are using yesterday file, we now use today file\n",
    "            if yes_yesterday:\n",
    "                file_str = today_file_str\n",
    "            \n",
    "            while found == 0 and search_count < 3:\n",
    "                # try previous id in sequence\n",
    "                curr_id_idx = (curr_id_idx - 1) % 4\n",
    "                # if timestamp is available, use it\n",
    "                if (file_str, current_hour) in id_sets[ids[curr_id_idx]]:\n",
    "                    # append dataset id, file name, and index of time step for idx conversion\n",
    "                    tstep_idx = id_sets[ids[curr_id_idx]][(file_str, current_hour)]\n",
    "                    \n",
    "                    # append dataset id, file name, and index of time step for idx conversion\n",
    "                    conversion_seq.append([ids[curr_id_idx], file_str, tstep_idx])\n",
    "\n",
    "                    # get file's path\n",
    "                    path = f'{firesmoke_dir}/{ids[curr_id_idx]}/{file_str}'\n",
    "                    # open the file with xarray\n",
    "                    ds = xr.open_dataset(path)\n",
    "                    idx_calls.append([ids[curr_id_idx], file_str, parse_tflag(ds['TFLAG'].values[tstep_idx][0])])\n",
    "\n",
    "                    found = 1\n",
    "                else:  \n",
    "                    search_count += 1\n",
    "                print(f'findingggg, found = {found}, search_count = {search_count}')\n",
    "                print(f'ids[curr_id_idx] = {ids[curr_id_idx]}')\n",
    "\n",
    "            # if we *still* haven't found file, try a final check, check ALL datasets again but for yesterday str\n",
    "            if not found:\n",
    "                # use yesterday file\n",
    "                file_str = yest_file_str\n",
    "\n",
    "                # reset search counters and conditions\n",
    "                found = 0\n",
    "                search_count = 0\n",
    "\n",
    "                while found == 0 and search_count < 4:\n",
    "                    # try previous id in sequence\n",
    "                    curr_id_idx = (curr_id_idx - 1) % 4\n",
    "                    # if timestamp is available, use it\n",
    "                    if (file_str, current_hour) in id_sets[ids[curr_id_idx]]:\n",
    "                        # append dataset id, file name, and index of time step for idx conversion\n",
    "                        tstep_idx = id_sets[ids[curr_id_idx]][(file_str, current_hour)]\n",
    "                        \n",
    "                        # append dataset id, file name, and index of time step for idx conversion\n",
    "                        conversion_seq.append([ids[curr_id_idx], file_str, tstep_idx])\n",
    "    \n",
    "                        # get file's path\n",
    "                        path = f'{firesmoke_dir}/{ids[curr_id_idx]}/{file_str}'\n",
    "                        # open the file with xarray\n",
    "                        ds = xr.open_dataset(path)\n",
    "                        idx_calls.append([ids[curr_id_idx], file_str, parse_tflag(ds['TFLAG'].values[tstep_idx][0])])\n",
    "    \n",
    "                        found = 1\n",
    "                    else:  \n",
    "                        search_count += 1\n",
    "\n",
    "                    print(f'FAILED TO FIND, LAST RESORT:')\n",
    "                    print(f'found = {found}, search_count = {search_count}')\n",
    "                    print(f'ids[curr_id_idx] = {ids[curr_id_idx]}')\n",
    "                if not found:\n",
    "                    print('FAILed TO FIND ANYTHING')\n",
    "                    \n",
    "        # move to next hour\n",
    "        current_hour += datetime.timedelta(hours=1)\n",
    "\n",
    "    # move to the next day\n",
    "    current_date += datetime.timedelta(days=1)\n",
    "    print(f'{current_date}, ids[curr_id_idx] = {ids[curr_id_idx]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe5512b-2bae-4fbf-9792-f798ebefa2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture captured_output\n",
    "# for c in idx_calls:\n",
    "#     print(c)\n",
    "\n",
    "# with open('idx_calls_v3.txt', 'w') as f:\n",
    "#     f.write(captured_output.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bad0d93-02c2-4795-ae4a-0966225cfab5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save conversion_seq to file\n",
    "with open('conversion_seq_v3.pkl', 'wb') as f:\n",
    "    pickle.dump(conversion_seq, f)\n",
    "\n",
    "# save idx_calls to file\n",
    "with open('idx_calls_v3.pkl', 'wb') as f:\n",
    "    pickle.dump(idx_calls, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a6cb5a-acc6-4b6d-95b8-40c426a55650",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load conversion_seq from a file\n",
    "with open('conversion_seq_v3.pkl', 'rb') as f:\n",
    "    conversion_seq = pickle.load(f)\n",
    "\n",
    "# Load idx_calls from a file\n",
    "with open('idx_calls_v3.pkl', 'rb') as f:\n",
    "    idx_calls = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4910833d-b97b-40f2-ae09-e792fb724333",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversion_seq[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b95b67-a9e1-499b-bc23-d6313fff9254",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx_calls[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bd9ca5-ca24-4f90-afc5-b61d0c3b97e3",
   "metadata": {},
   "source": [
    "## Do conversion from netCDF files to IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a778f8a-6019-4568-8dd5-d7412c39b442",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx_calls[22200:22200+145]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618e390a-bff6-46a3-8c5a-7139b56536c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#********** FOR TESTING\n",
    "# my_seq = conversion_seq[22200:22200+145]\n",
    "my_seq = conversion_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8bf6a5-0d2c-4b39-a2be-fbfb80a13dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'len(conversion_seq) = {len(conversion_seq)}')\n",
    "print(f'len(idx_calls) = {len(idx_calls)}')\n",
    "print(f'len(my_seq) = {len(my_seq)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96e63f9-b881-488b-a7cb-bfa7ccf9872e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Create idx file of i'th dataset\n",
    "# # useful for dealing with fields that are not all the same size:\n",
    "# # https://github.com/sci-visus/OpenVisus/blob/master/Samples/jupyter/nasa_conversion_example.ipynb\n",
    "   \n",
    "# # create OpenVisus field for the pm25 variable\n",
    "# f = Field('PM25', 'float32')\n",
    "\n",
    "# # create the idx file for this dataset using field f\n",
    "# # dims is maximum array size, we will resample data accordingly to fit this\n",
    "# # time is number of files * 24 (hours)\n",
    "# db = CreateIdx(url=idx_dir + '/firesmoke.idx', fields=[f], \n",
    "#                dims=[int(max_ncols[ids[0]]), int(max_nrows[ids[0]])], time=[0, len(my_seq) - 1, '%00000000d/'])\n",
    "\n",
    "# # to track what timestep we are on in idx\n",
    "# tstep = 0\n",
    "\n",
    "# # threshold to use to change small-enough resampled values to 0\n",
    "# thresh = 1e-15\n",
    "\n",
    "# for call in tqdm(my_seq):\n",
    "#     # get instructions from call\n",
    "#     curr_id = call[0]\n",
    "#     curr_file = call[1]\n",
    "#     tstep_index = call[2]\n",
    "#     # open the file with xarray\n",
    "#     ds = xr.open_dataset(f'{firesmoke_dir}/{curr_id}/{curr_file}')\n",
    "    \n",
    "#     # Get the PM25 values, squeeze out empty axis\n",
    "#     file_vals = np.squeeze(ds['PM25'].values)\n",
    "    \n",
    "#     # to decide if we need to resample or not\n",
    "#     resamp = ds.XORIG != max_xorig\n",
    "    \n",
    "#     # resample data if not already on max lat/lon grid\n",
    "#     if resamp:\n",
    "#         # Perform the interpolation\n",
    "#         file_vals_resamp = griddata(sml_tups, file_vals[tstep_index].flatten(), big_tups, method='cubic', fill_value=0)\n",
    "        \n",
    "#         # Any values that are less than a given threshold, make it 0\n",
    "#         file_vals_resamp[file_vals_resamp < thresh] = 0\n",
    "        \n",
    "#         # Reshape the result to match the new grid shape\n",
    "#         file_vals_resamp = file_vals_resamp.reshape((len(big_lat), len(big_lon)))\n",
    "#         # Write resampled values at hour h to timestep t and field f\n",
    "#         db.write(data=file_vals_resamp.astype(np.float32),field=f,time=tstep)\n",
    "#     else:\n",
    "#         # Write original values at hour h to timestep t and field f\n",
    "#         db.write(data=file_vals[tstep_index], field=f, time=tstep)\n",
    "\n",
    "#     # move to next timestep in IDX\n",
    "#     tstep = tstep + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b908337-8454-4f24-9cec-4325ad74f2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to idx data directory\n",
    "os.chdir(idx_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3af4da-d469-44b9-9c90-20216304c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compress dataset\n",
    "db.compressDataset(['zip'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
