{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ca8ac1b-4eaa-478a-ad7e-1364e6c1791e",
   "metadata": {},
   "source": [
    "# Firesmoke Data Conversion to IDX using OpenVisus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6f4506-47b9-452e-908e-15e532e1b801",
   "metadata": {},
   "source": [
    "## Import necessary libraries, install them if you do not have them. This was developed in Python 3.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f499004e-bebd-4abf-8885-f16ce6ad9531",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used to read/manipulate netCDF data\n",
    "import xarray as xr\n",
    "\n",
    "# Used to convert to .idx\n",
    "from OpenVisus import *\n",
    "\n",
    "# Used for numerical work\n",
    "import numpy as np\n",
    "\n",
    "# Used for processing netCDF time data\n",
    "import datetime\n",
    "\n",
    "# Used for interacting with OS file system (to get directory file names)\n",
    "import os\n",
    "\n",
    "# Used for resampling arrays to fit the same lat/lon grid\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# Accessory, used to generate progress bar for running for loops\n",
    "# from tqdm.notebook import tqdm\n",
    "# import ipywidgets\n",
    "# import jupyterlab_widgets\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce3169e-d1dd-4050-811a-6fd661442c81",
   "metadata": {},
   "source": [
    "## Get relevant directory paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0c7861-34fb-464b-95b1-4a1aa4a8ba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******* THIS IS WHEN RUNNING FROM ATLANTIS.SCI **************\n",
    "# directory to all firesmoke data, mounted on my personal machine, change accordingly\n",
    "firesmoke_dir = \"/usr/sci/cedmav/data/firesmoke\"\n",
    "\n",
    "# path to save idx file and data\n",
    "idx_dir = \"/usr/sci/scratch_nvme/arleth/idx/firesmoke2d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092f38b1-5202-40bf-a41d-ac113b47ffa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ******* THIS IS WHEN RUNNING FROM MY MACBOOK **************\n",
    "# # directory to all firesmoke data, mounted on my personal machine, change accordingly\n",
    "# firesmoke_dir = \"/Users/arleth/Mount/firesmoke\"\n",
    "\n",
    "# # path to save idx file and data\n",
    "# idx_dir = \"/Users/arleth/Mount/idx/firesmoke2d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d88f90-0370-481c-93be-e3c1e1a10ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory name for dataset\n",
    "dataset_name = \"BSC00CA12-01\"\n",
    "\n",
    "# Inside dataset_dir are the netcdf files\n",
    "dataset_dir = firesmoke_dir + \"/\" + dataset_name\n",
    "\n",
    "# Inside idx_dir is where to save the final idx file\n",
    "idx_dir = idx_dir + \"/\" + dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32528eee-cd60-44f6-9e14-cae79c5ce478",
   "metadata": {},
   "source": [
    "## Gather information about the metadata of our files, since it is inconsistent file to file. We need to know what to normalize across all files.\n",
    "\n",
    "### In particular:\n",
    "#### 1. Count number of files there are per firesmoke directory.\n",
    "#### 2. Determine maximum row,col dimension sizes for pm25 array.\n",
    "#### 3. Determine maximum latitude longitude grid parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c92053f-9e91-4e6d-b46b-a9efa04c6d58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list of all files that are available from UBC\n",
    "successful_files = []\n",
    "\n",
    "# variables to hold maxes, also to track the unique max values\n",
    "max_ncols = 0.0\n",
    "max_nrows = 0.0\n",
    "ncols = set()\n",
    "nrows = set()\n",
    "\n",
    "# useful:\n",
    "# longitude = np.linspace(xorig, xorig + xcell * (ncols - 1), ncols)\n",
    "# latitude = np.linspace(yorig, yorig + ycell * (nrows - 1), nrows)\n",
    "max_grid_x = {\"xorig\" : 0.0, \"xcell\" : 0.0}\n",
    "max_grid_y = {\"yorig\" : 0.0, \"ycell\" : 0.0}\n",
    "xorigs = set()\n",
    "xcells = set()\n",
    "yorigs = set()\n",
    "ycells = set()\n",
    "\n",
    "# get list of netcdf file names for BSC00CA12-01 dataset\n",
    "file_names = os.listdir(dataset_dir)\n",
    "\n",
    "# try opening each file, process only if it successfully opens\n",
    "for file in tqdm(file_names):\n",
    "    # get file's path\n",
    "    path = dataset_dir + \"/\" + file\n",
    "    \n",
    "    # keep track of which files successfully open\n",
    "    try:\n",
    "        # open the file with xarray\n",
    "        ds = xr.open_dataset(path)\n",
    "\n",
    "        # append file name to successful_files\n",
    "        successful_files.append(file)\n",
    "\n",
    "        # update maxes accordingly\n",
    "        # these *are* allowed to get mixed up between files right? in this case don't need to worry bout it\n",
    "        max_ncols = max(max_ncols, ds.NCOLS)\n",
    "        max_nrows = max(max_nrows, ds.NROWS)\n",
    "\n",
    "        # these should not get mixed up between files right? or can they?\n",
    "        # if they do get mixed up, wouldn't it be a ill-defined grid?\n",
    "        # ref: https://stackoverflow.com/questions/18296755/python-max-function-using-key-and-lambda-expression\n",
    "        max_grid_x[\"xorig\"] = max(max_grid_x[\"xorig\"], ds.XORIG, key=abs)\n",
    "        max_grid_y[\"yorig\"] = max(max_grid_y[\"yorig\"], ds.YORIG, key=abs)\n",
    "        max_grid_x[\"xcell\"] = max(max_grid_x[\"xcell\"], ds.XCELL, key=abs)\n",
    "        max_grid_y[\"ycell\"] = max(max_grid_y[\"ycell\"], ds.YCELL, key=abs)\n",
    "\n",
    "        # update sets\n",
    "        ncols.add(ds.NCOLS)\n",
    "        nrows.add(ds.NROWS)\n",
    "        xorigs.add(ds.XORIG)\n",
    "        yorigs.add(ds.YORIG)\n",
    "        xcells.add(ds.XCELL)\n",
    "        ycells.add(ds.YCELL)\n",
    "        \n",
    "    except:\n",
    "        # netcdf file does not exist\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fba2c72-e070-4b77-8060-f004bceb779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of successful files is {np.size(successful_files)}')\n",
    "print(f'Max cell sizes are max_ncols = {max_ncols} and max_nrows = {max_nrows}')\n",
    "print(f'Max xorig & xcell: {max_grid_x}')\n",
    "print(f'Max yorig & ycell: {max_grid_y}')\n",
    "print(f'ncols: {ncols}')\n",
    "print(f'nrows: {nrows}')\n",
    "print(f'xorigs: {xorigs}')\n",
    "print(f'yorigs: {yorigs}')\n",
    "print(f'xcells: {xcells}')\n",
    "print(f'ycells: {ycells}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005f312f-f319-46a5-a7d2-1de4c49a9ed1",
   "metadata": {},
   "source": [
    "### Get latitude/longitude coordinates using the max values and non-max values, this is used for resampling during conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9fe211-ccb8-46df-93e9-92f7d18e702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get parameters for bigger lat/lon\n",
    "max_xorig = max_grid_x['xorig']\n",
    "max_xcell = max_grid_x['xcell']\n",
    "max_yorig = max_grid_y['yorig']\n",
    "max_ycell = max_grid_y['ycell']\n",
    "\n",
    "# get arrays of bigger lat/lon grid\n",
    "big_lon = np.linspace(max_xorig, max_xorig + max_xcell * (max_ncols - 1), max_ncols)\n",
    "big_lat = np.linspace(max_yorig, max_yorig + max_ycell * (max_nrows - 1), max_nrows)\n",
    "\n",
    "# get coordinates made of new lat/lon arrays\n",
    "big_lon_pts, big_lat_pts = np.meshgrid(big_lon, big_lat)\n",
    "big_tups = np.array([tup for tup in zip(big_lon_pts.flatten(), big_lat_pts.flatten())])\n",
    "\n",
    "# get arrays of smaller lat/lon grid\n",
    "sml_ds = xr.open_dataset(firesmoke_dir + \"/BSC00CA12-01/dispersion_20210304.nc\")\n",
    "sml_lon = np.linspace(sml_ds.XORIG, sml_ds.XORIG + sml_ds.XCELL * (sml_ds.NCOLS - 1), sml_ds.NCOLS)\n",
    "sml_lat = np.linspace(sml_ds.YORIG, sml_ds.YORIG + sml_ds.YCELL * (sml_ds.NROWS - 1), sml_ds.NROWS)\n",
    "\n",
    "# get coordinates made of small lat/lon arrays\n",
    "sml_lon_pts, sml_lat_pts = np.meshgrid(sml_lon, sml_lat)\n",
    "sml_tups = np.array([tup for tup in zip(sml_lon_pts.flatten(), sml_lat_pts.flatten())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b68ee49-dabf-4258-95b9-707814280028",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## TESTING `resample_array` AND SCRIBBLES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc424296-d880-462f-921b-d4966025e1d2",
   "metadata": {},
   "source": [
    "### This is plotting the oiginal 381x1041 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30986a19-aa82-4fd3-91fb-b76ed6811a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the PM25 values, squeeze out empty axis\n",
    "# vals = np.squeeze(sml_ds['PM25'].values)\n",
    "\n",
    "# # Perform the interpolation\n",
    "# arr = griddata(sml_tups, vals[15].flatten(), big_tups, method='cubic', fill_value=0)\n",
    "\n",
    "# # Any values that are less than a given threshold, make it 0\n",
    "# arr[arr < 1e-15] = 0\n",
    "\n",
    "# # Reshape the result to match the new grid shape\n",
    "# arr = arr.reshape((len(big_lat), len(big_lon)))\n",
    "\n",
    "# arr = arr.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f64beb-b62d-46b5-b7e2-1cf1d7796449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.min(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed0975b-23c0-40f1-8666-ea9579b7d767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(arr[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb1f2db-95a9-467b-af10-6f671f4f48d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's use matplotlib's imshow, since our data is on a grid\n",
    "# # ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n",
    "\n",
    "# # Initialize a figure and plot, so we can customize figure and plot of data\n",
    "# # ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html\n",
    "# # ref: https://scitools.org.uk/cartopy/docs/latest/getting_started/index.html\n",
    "# my_fig, my_plt = plt.subplots(figsize=(15, 6), subplot_kw=dict(projection=ccrs.PlateCarree()))\n",
    "\n",
    "# # Let's set some parameters to get the visualization we want\n",
    "# # ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n",
    "\n",
    "# # color PM25 values on a log scale, since values are small\n",
    "# my_norm = \"log\" \n",
    "# # this will number our x and y axes based on the longitude latitude range\n",
    "# my_extent = [np.min(sml_lon), np.max(sml_lon), np.min(sml_lat), np.max(sml_lat)]\n",
    "# # ensure the aspect ratio of our plot fits all data, matplotlib can does this automatically\n",
    "# my_aspect = 'auto'\n",
    "# # tell matplotlib, our origin is the lower-left corner\n",
    "# my_origin = 'lower'\n",
    "# # select a colormap for our plot and the color bar on the right\n",
    "# my_cmap = 'viridis'\n",
    "\n",
    "# # create our plot using imshow\n",
    "# plot = my_plt.imshow(arr, norm=my_norm, extent=my_extent, \n",
    "#           aspect=my_aspect, origin=my_origin, cmap=my_cmap)\n",
    "\n",
    "# # draw coastlines\n",
    "# my_plt.coastlines()\n",
    "\n",
    "# # draw latitude longitude lines\n",
    "# # ref: https://scitools.org.uk/cartopy/docs/latest/gallery/gridlines_and_labels/gridliner.html\n",
    "# my_plt.gridlines(draw_labels=True)\n",
    "\n",
    "# # add a colorbar to our figure, based on the plot we just made above\n",
    "# my_fig.colorbar(plot,location='right', label='ug/m^3')\n",
    "\n",
    "# # # Set x and y axis labels on our ax\n",
    "# # my_plt.set_xlabel('Longitude')\n",
    "# # my_plt.set_ylabel('Latitude')\n",
    "\n",
    "# # Set title of our figure\n",
    "# my_fig.suptitle('Ground level concentration of PM2.5 microns and smaller')\n",
    "\n",
    "# # # Set title of our plot as the timestamp of our data\n",
    "# # my_plt.set_title(f'{my_timestamp}')\n",
    "\n",
    "# # Show the resulting visualization\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d18dc69-01cc-4fa9-91a5-1377bcdc6bea",
   "metadata": {},
   "source": [
    "### This is visualizing the resampled version of array above, from 381x1041 -> 381x1081 grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c59925a-f689-4c9f-a9a9-a897f45372e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's use matplotlib's imshow, since our data is on a grid\n",
    "# # ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n",
    "\n",
    "# # Initialize a figure and plot, so we can customize figure and plot of data\n",
    "# # ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html\n",
    "# # ref: https://scitools.org.uk/cartopy/docs/latest/getting_started/index.html\n",
    "# my_fig, my_plt = plt.subplots(figsize=(15, 6), subplot_kw=dict(projection=ccrs.PlateCarree()))\n",
    "\n",
    "# # Let's set some parameters to get the visualization we want\n",
    "# # ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n",
    "\n",
    "# # color PM25 values on a log scale, since values are small\n",
    "# my_norm = \"log\" \n",
    "# # this will number our x and y axes based on the longitude latitude range\n",
    "# my_extent = [np.min(big_lon), np.max(big_lon), np.min(big_lat), np.max(big_lat)]\n",
    "# # ensure the aspect ratio of our plot fits all data, matplotlib can does this automatically\n",
    "# my_aspect = 'auto'\n",
    "# # tell matplotlib, our origin is the lower-left corner\n",
    "# my_origin = 'lower'\n",
    "# # select a colormap for our plot and the color bar on the right\n",
    "# my_cmap = 'viridis'\n",
    "\n",
    "# # create our plot using imshow\n",
    "# plot = my_plt.imshow(arr_resamp, norm=my_norm, extent=my_extent, \n",
    "#           aspect=my_aspect, origin=my_origin, cmap=my_cmap, vmin=.00001, vmax=1)\n",
    "\n",
    "# # draw coastlines\n",
    "# my_plt.coastlines()\n",
    "\n",
    "# # draw latitude longitude lines\n",
    "# # ref: https://scitools.org.uk/cartopy/docs/latest/gallery/gridlines_and_labels/gridliner.html\n",
    "# my_plt.gridlines(draw_labels=True)\n",
    "\n",
    "# # add a colorbar to our figure, based on the plot we just made above\n",
    "# my_fig.colorbar(plot,location='right', label='ug/m^3')\n",
    "\n",
    "# # # Set x and y axis labels on our ax\n",
    "# # my_plt.set_xlabel('Longitude')\n",
    "# # my_plt.set_ylabel('Latitude')\n",
    "\n",
    "# # Set title of our figure\n",
    "# my_fig.suptitle('Ground level concentration of PM2.5 microns and smaller')\n",
    "\n",
    "# # # Set title of our plot as the timestamp of our data\n",
    "# # my_plt.set_title(f'{my_timestamp}')\n",
    "\n",
    "# # Show the resulting visualization\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bd9ca5-ca24-4f90-afc5-b61d0c3b97e3",
   "metadata": {},
   "source": [
    "## Do conversion from netCDF files to IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96e63f9-b881-488b-a7cb-bfa7ccf9872e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create idx file of i'th dataset\n",
    "# useful for dealing with fields that are not all the same size:\n",
    "# https://github.com/sci-visus/OpenVisus/blob/master/Samples/jupyter/nasa_conversion_example.ipynb\n",
    "   \n",
    "# create OpenVisus field for the pm25 variable\n",
    "f = Field('PM25', 'float32')\n",
    "\n",
    "# create the idx file for this dataset using field f\n",
    "# dims is maximum array size, we will resample data accordingly to fit this\n",
    "# time is number of files * 24 (hours)\n",
    "db = CreateIdx(url=idx_dir + '/' + dataset_name + '.idx', fields=[f], \n",
    "               dims=[int(max_ncols), int(max_nrows)], time=[0, (len(successful_files) * 24) - 1, '%00000000d/'])\n",
    "\n",
    "# put file names *in order* so we store data chronologically\n",
    "# perhaps do this in a less *fragile* way?\n",
    "successful_files = np.sort(successful_files)\n",
    "\n",
    "# to track what timestep we are on in idx\n",
    "tstep = 0\n",
    "\n",
    "# threshold to use to change small-enough resampled values to 0\n",
    "thresh = 1e-15\n",
    "\n",
    "# for all netcdf files we downloaded for i'th dataset\n",
    "for file in tqdm(successful_files):\n",
    "    # open the file with xarray\n",
    "    ds = xr.open_dataset(dataset_dir + \"/\" + file)\n",
    "    \n",
    "    # Get the PM25 values, squeeze out empty axis\n",
    "    file_vals = np.squeeze(ds['PM25'].values)\n",
    "    \n",
    "    # to decide if we need to resample or not\n",
    "    resamp = ds.XORIG != max_xorig\n",
    "    \n",
    "    # For all 24 hours in current file, a full day is time slices 15 through 15+23\n",
    "    for h in tqdm(np.arange(15, 15+24)):\n",
    "        # resample data if not already on max lat/lon grid\n",
    "        if resamp:\n",
    "            print(f'resampling... {ds.XORIG} != {max_xorig}')\n",
    "            # Perform the interpolation\n",
    "            file_vals_resamp = griddata(sml_tups, file_vals[h].flatten(), big_tups, method='cubic', fill_value=0)\n",
    "            \n",
    "            # Any values that are less than a given threshold, make it 0\n",
    "            file_vals_resamp[file_vals_resamp < thresh] = 0\n",
    "            \n",
    "            # Reshape the result to match the new grid shape\n",
    "            file_vals_resamp = file_vals_resamp.reshape((len(big_lat), len(big_lon)))\n",
    "            print(f'np.min(file_vals_resamp) = {np.min(file_vals_resamp)}')\n",
    "            # Write resampled values at hour h to timestep t and field f\n",
    "            db.write(data=file_vals_resamp.astype(np.float32),field=f,time=tstep)\n",
    "        else:\n",
    "            # Write original values at hour h to timestep t and field f\n",
    "            db.write(data=file_vals[h], field=f, time=tstep)\n",
    "    \n",
    "        # move to next timestep in IDX\n",
    "        tstep = tstep + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b908337-8454-4f24-9cec-4325ad74f2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to idx data directory\n",
    "os.chdir('/usr/sci/scratch_nvme/arleth/idx/firesmoke2d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3af4da-d469-44b9-9c90-20216304c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compress dataset\n",
    "db.compressDataset(['zip'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
